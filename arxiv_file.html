<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "https://www.w3.org/TR/html4/strict.dtd">
<!-- saved from url=(0014)about:internet -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><style type="text/css" nonce="">
body,td,div,p,a,input {font-family: arial, sans-serif;}
</style><meta http-equiv="X-UA-Compatible" content="IE=edge"><link rel="shortcut icon" href="https://www.google.com/a/cpanel/virginia.edu/images/favicon.ico" type="image/x-icon"><title>UVa Mail - cs daily Subj-class mailing 80100081 1</title><style type="text/css" nonce="">
body, td {font-size:13px} a:link, a:active {color:#1155CC; text-decoration:none} a:hover {text-decoration:underline; cursor: pointer} a:visited{color:##6611CC} img{border:0px} pre { white-space: pre; white-space: -moz-pre-wrap; white-space: -o-pre-wrap; white-space: pre-wrap; word-wrap: break-word; max-width: 800px; overflow: auto;} .logo { left: -7px; position: relative; }
</style></head><body><div class="bodycontainer"><table width="100%" cellpadding="0" cellspacing="0" border="0"><tbody><tr height="14px"><td width="143"><img src="./UVa Mail - cs daily Subj-class mailing 80100081 1_files/unnamed.png" width="143" height="59" alt="UVa Mail" class="logo"></td><td align="right"><font size="-1" color="#777"><b>Dane Williamson &lt;dw3zn@virginia.edu&gt;</b></font></td></tr></tbody></table><hr><div class="maincontent"><table width="100%" cellpadding="0" cellspacing="0" border="0"><tbody><tr><td><font size="+1"><b>cs daily Subj-class mailing 80100081 1</b></font><br></td></tr></tbody></table><hr><table width="100%" cellpadding="0" cellspacing="0" border="0" class="message"><tbody><tr><td><font size="-1"><b>send mail ONLY to cs </b>&lt;no-reply@arxiv.org&gt;</font></td><td align="right"><font size="-1">Wed, Mar 9, 2022 at 4:11 AM</font></td></tr><tr><td colspan="2" style="padding-bottom: 4px;"><font size="-1" class="recipient"><div class="replyto">Reply-To: cs@arxiv.org</div><div>To: cs daily title/abstract distribution &lt;rabble@arxiv.org&gt;</div></font></td></tr><tr><td colspan="2"><table width="100%" cellpadding="12" cellspacing="0" border="0"><tbody><tr><td><div style="overflow: hidden;"><font size="-1">------------------------------<wbr>------------------------------<wbr>------------------<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
Send any comments regarding submissions directly to submitter.<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
Archives at <a href="http://arxiv.org/" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=http://arxiv.org/&amp;source=gmail&amp;ust=1646963617821000&amp;usg=AOvVaw14KIOAgN79y9J-rLYqX87s" rel="noreferrer" target="_blank">http://arxiv.org/</a><br>
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
&nbsp;Submissions to:<br>
Artificial Intelligence<br>
Computer Vision and Pattern Recognition<br>
Machine Learning<br>
Software Engineering<br>
&nbsp;received from&nbsp; Mon&nbsp; 7 Mar 22 19:00:00 GMT&nbsp; to&nbsp; Tue&nbsp; 8 Mar 22 19:00:00 GMT<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04172<br>
Date: Tue, 8 Mar 2022 16:03:35 GMT&nbsp; &nbsp;(315kb,D)<br>
<br>
Title: Distributed Control using Reinforcement Learning with<br>
&nbsp; Temporal-Logic-Based Reward Shaping<br>
Authors: Ningyuan Zhang, Wenliang Liu, Calin Belta<br>
Categories: cs.AI cs.SY eess.SY<br>
Comments: 12 pages, 4 figures, accepted by L4DC 2022<br>
\\<br>
&nbsp; We present a computational framework for synthesis of distributed control<br>
strategies for a heterogeneous team of robots in a partially observable<br>
environment. The goal is to cooperatively satisfy specifications given as<br>
Truncated Linear Temporal Logic (TLTL) formulas. Our approach formulates the<br>
synthesis problem as a stochastic game and employs a policy graph method to<br>
find a control strategy with memory for each agent. We construct the stochastic<br>
game on the product between the team transition system and a finite state<br>
automaton (FSA) that tracks the satisfaction of the TLTL formula. We use the<br>
quantitative semantics of TLTL as the reward of the game, and further reshape<br>
it using the FSA to guide and accelerate the learning process. Simulation<br>
results demonstrate the efficacy of the proposed solution under demanding task<br>
specifications and the effectiveness of reward shaping in significantly<br>
accelerating the speed of learning.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04172" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04172&amp;source=gmail&amp;ust=1646963617821000&amp;usg=AOvVaw1WRnxpZ7qqQ_yI5LGYDdzw" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04172</a> ,&nbsp; 315kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03664<br>
Date: Mon, 7 Mar 2022 19:02:26 GMT&nbsp; &nbsp;(7708kb,D)<br>
<br>
Title: Unsupervised Domain Adaptation with Contrastive Learning for OCT<br>
&nbsp; Segmentation<br>
Authors: Alvaro Gomariz, Huanxiang Lu, Yun Yvonna Li, Thomas Albrecht, Andreas<br>
&nbsp; Maunz, Fethallah Benmansour, Alessandra M.Valcarcel, Jennifer Luu, Daniela<br>
&nbsp; Ferrara, Orcun Goksel<br>
Categories: cs.CV cs.LG eess.IV<br>
Comments: Main: 10 pages, 4 figures, 1 table. Supplementary: 2 pages, 1 figure,<br>
&nbsp; 4 tables<br>
\\<br>
&nbsp; Accurate segmentation of retinal fluids in 3D Optical Coherence Tomography<br>
images is key for diagnosis and personalized treatment of eye diseases. While<br>
deep learning has been successful at this task, trained supervised models often<br>
fail for images that do not resemble labeled examples, e.g. for images acquired<br>
using different devices. We hereby propose a novel semi-supervised learning<br>
framework for segmentation of volumetric images from new unlabeled domains. We<br>
jointly use supervised and contrastive learning, also introducing a contrastive<br>
pairing scheme that leverages similarity between nearby slices in 3D. In<br>
addition, we propose channel-wise aggregation as an alternative to conventional<br>
spatial-pooling aggregation for contrastive feature map projection. We evaluate<br>
our methods for domain adaptation from a (labeled) source domain to an<br>
(unlabeled) target domain, each containing images acquired with different<br>
acquisition devices. In the target domain, our method achieves a Dice<br>
coefficient 13.8% higher than SimCLR (a state-of-the-art contrastive<br>
framework), and leads to results comparable to an upper bound with supervised<br>
training in that domain. In the source domain, our model also improves the<br>
results by 5.4% Dice, by successfully leveraging information from many<br>
unlabeled images.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03664" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03664&amp;source=gmail&amp;ust=1646963617822000&amp;usg=AOvVaw3-5G6Zq7rQW4NF4V41z0yJ" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03664</a> ,&nbsp; 7708kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03677<br>
Date: Mon, 7 Mar 2022 19:28:39 GMT&nbsp; &nbsp;(136kb,D)<br>
<br>
Title: Object-centric and memory-guided normality reconstruction for video<br>
&nbsp; anomaly detection<br>
Authors: Khalil Bergaoui, Yassine Naji, Aleksandr Setkov, Ang\'elique Loesch,<br>
&nbsp; Mich\`ele Gouiff\`es and Romaric Audigier<br>
Categories: cs.CV<br>
\\<br>
&nbsp; This paper addresses video anomaly detection problem for videosurveillance.<br>
Due to the inherent rarity and heterogeneity of abnormal events, the problem is<br>
viewed as a normality modeling strategy, in which our model learns<br>
object-centric normal patterns without seeing anomalous samples during<br>
training. The main contributions consist in coupling pretrained object-level<br>
action features prototypes with a cosine distance-based anomaly estimation<br>
function, therefore extending previous methods by introducing additional<br>
constraints to the mainstream reconstruction-based strategy. Our framework<br>
leverages both appearance and motion information to learn object-level behavior<br>
and captures prototypical patterns within a memory module. Experiments on<br>
several well-known datasets demonstrate the effectiveness of our method as it<br>
outperforms current state-of-the-art on most relevant spatio-temporal<br>
evaluation metrics.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03677" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03677&amp;source=gmail&amp;ust=1646963617822000&amp;usg=AOvVaw100XlHzoKVe3VEqSB-3DFV" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03677</a> ,&nbsp; 136kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03689<br>
Date: Mon, 7 Mar 2022 20:15:17 GMT&nbsp; &nbsp;(310kb,D)<br>
<br>
Title: WaveMix: Resource-efficient Token Mixing for Images<br>
Authors: Pranav Jeevan and Amit Sethi<br>
Categories: cs.CV cs.AI cs.LG<br>
Comments: 12 pages, 2 figures<br>
ACM-class: I.4.0; I.4.1; I.4.7; I.4.8; I.4.9; I.4.10; I.2.10; I.5.1; I.5.2;<br>
&nbsp; I.5.4<br>
\\<br>
&nbsp; Although certain vision transformer (ViT) and CNN architectures generalize<br>
well on vision tasks, it is often impractical to use them on green, edge, or<br>
desktop computing due to their computational requirements for training and even<br>
testing. We present WaveMix as an alternative neural architecture that uses a<br>
multi-scale 2D discrete wavelet transform (DWT) for spatial token mixing.<br>
Unlike ViTs, WaveMix neither unrolls the image nor requires self-attention of<br>
quadratic complexity. Additionally, DWT introduces another inductive bias --<br>
besides convolutional filtering -- to utilize the 2D structure of an image to<br>
improve generalization. The multi-scale nature of the DWT also reduces the<br>
requirement for a deeper architecture compared to the CNNs, as the latter<br>
relies on pooling for partial spatial mixing. WaveMix models show<br>
generalization that is competitive with ViTs, CNNs, and token mixers on several<br>
datasets while requiring lower GPU RAM (training and testing), number of<br>
computations, and storage. WaveMix have achieved State-of-the-art (SOTA)<br>
results in EMNIST Byclass and EMNIST Balanced datasets.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03689" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03689&amp;source=gmail&amp;ust=1646963617822000&amp;usg=AOvVaw0lMNp0LfCkalTGIBZDX8d2" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03689</a> ,&nbsp; 310kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03727<br>
Date: Mon, 7 Mar 2022 21:27:40 GMT&nbsp; &nbsp;(13802kb,D)<br>
<br>
Title: Barlow constrained optimization for Visual Question Answering<br>
Authors: Abhishek Jha, Badri N. Patro, Luc Van Gool, Tinne Tuytelaars<br>
Categories: cs.CV<br>
\\<br>
&nbsp; Visual question answering is a vision-and-language multimodal task, that aims<br>
at predicting answers given samples from the question and image modalities.<br>
Most recent methods focus on learning a good joint embedding space of images<br>
and questions, either by improving the interaction between these two<br>
modalities, or by making it a more discriminant space. However, how informative<br>
this joint space is, has not been well explored. In this paper, we propose a<br>
novel regularization for VQA models, Constrained Optimization using Barlow's<br>
theory (COB), that improves the information content of the joint space by<br>
minimizing the redundancy. It reduces the correlation between the learned<br>
feature components and thereby disentangles semantic concepts. Our model also<br>
aligns the joint space with the answer embedding space, where we consider the<br>
answer and image+question as two different `views' of what in essence is the<br>
same semantic information. We propose a constrained optimization policy to<br>
balance the categorical and redundancy minimization forces. When built on the<br>
state-of-the-art GGE model, the resulting model improves VQA accuracy by 1.4%<br>
and 4% on the VQA-CP v2 and VQA v2 datasets respectively. The model also<br>
exhibits better interpretability.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03727" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03727&amp;source=gmail&amp;ust=1646963617822000&amp;usg=AOvVaw2xzTymQc34LwqyH4-kC-JQ" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03727</a> ,&nbsp; 13802kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03768<br>
Date: Mon, 7 Mar 2022 23:10:40 GMT&nbsp; &nbsp;(342kb,D)<br>
<br>
Title: CrowdFormer: Weakly-supervised Crowd counting with Improved<br>
&nbsp; Generalizability<br>
Authors: Siddharth Singh Savner, Vivek Kanhangad<br>
Categories: cs.CV<br>
\\<br>
&nbsp; Convolutional neural networks (CNNs) have dominated the field of computer<br>
vision for nearly a decade due to their strong ability to learn local features.<br>
However, due to their limited receptive field, CNNs fail to model the global<br>
context. On the other hand, transformer, an attention-based architecture can<br>
model the global context easily. Despite this, there are limited studies that<br>
investigate the effectiveness of transformers in crowd counting. In addition,<br>
the majority of the existing crowd counting methods are based on the regression<br>
of density maps which requires point-level annotation of each person present in<br>
the scene. This annotation task is laborious and also error-prone. This has led<br>
to increased focus on weakly-supervised crowd counting methods which require<br>
only the count-level annotations. In this paper, we propose a weakly-supervised<br>
method for crowd counting using a pyramid vision transformer. We have conducted<br>
extensive evaluations to validate the effectiveness of the proposed method. Our<br>
method is comparable to the state-of-the-art on the benchmark crowd datasets.<br>
More importantly, it shows remarkable generalizability.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03768" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03768&amp;source=gmail&amp;ust=1646963617822000&amp;usg=AOvVaw2MLyXypD_NB_1I7eOuf7Ho" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03768</a> ,&nbsp; 342kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03796<br>
Date: Tue, 8 Mar 2022 01:36:26 GMT&nbsp; &nbsp;(2440kb,D)<br>
<br>
Title: PAMI-AD: An Activity Detector Exploiting Part-attention and Motion<br>
&nbsp; Information in Surveillance Videos<br>
Authors: Yunhao Du, Zhihang Tong, Junfeng Wan, Binyu Zhang, and Yanyun Zhao<br>
Categories: cs.CV<br>
Comments: 4 pages, 4 figures<br>
\\<br>
&nbsp; Activity detection in surveillance videos is a challenging task caused by<br>
small objects, complex activity categories, its untrimmed nature, etc. In this<br>
work, we propose an effective activity detection system for person-only and<br>
vehicle-only activities in untrimmed surveillance videos, named PAMI-AD. It<br>
consists of four modules, i.e., multi-object tracking, background modeling,<br>
activity classifier and post-processing. In particular, we propose a novel<br>
part-attention mechanism for person-only activities and a simple but strong<br>
motion information encoding method for vehicle-only activities. Our proposed<br>
system achieves the best results on the VIRAT dataset. Furthermore, our team<br>
won the 1st place in the TRECVID 2021 ActEV challenge.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03796" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03796&amp;source=gmail&amp;ust=1646963617823000&amp;usg=AOvVaw1-zN32qrZeW5xBsqzCyxPP" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03796</a> ,&nbsp; 2440kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03800<br>
Date: Tue, 8 Mar 2022 01:44:03 GMT&nbsp; &nbsp;(13838kb,D)<br>
<br>
Title: Unknown-Aware Object Detection: Learning What You Don't Know from Videos<br>
&nbsp; in the Wild<br>
Authors: Xuefeng Du, Xin Wang, Gabriel Gozum, Yixuan Li<br>
Categories: cs.CV<br>
Comments: CVPR2022<br>
\\<br>
&nbsp; Building reliable object detectors that can detect out-of-distribution (OOD)<br>
objects is critical yet underexplored. One of the key challenges is that models<br>
lack supervision signals from unknown data, producing overconfident predictions<br>
on OOD objects. We propose a new unknown-aware object detection framework<br>
through Spatial-Temporal Unknown Distillation (STUD), which distills unknown<br>
objects from videos in the wild and meaningfully regularizes the model's<br>
decision boundary. STUD first identifies the unknown candidate object proposals<br>
in the spatial dimension, and then aggregates the candidates across multiple<br>
video frames to form a diverse set of unknown objects near the decision<br>
boundary. Alongside, we employ an energy-based uncertainty regularization loss,<br>
which contrastively shapes the uncertainty space between the in-distribution<br>
and distilled unknown objects. STUD establishes the state-of-the-art<br>
performance on OOD detection tasks for object detection, reducing the FPR95<br>
score by over 10% compared to the previous best method. Code is available at<br>
<a href="https://github.com/deeplearning-wisc/stud" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/deeplearning-wisc/stud&amp;source=gmail&amp;ust=1646963617823000&amp;usg=AOvVaw3lBFK4hcRWL7A4J6TFVzWU" rel="noreferrer" target="_blank">https://github.com/<wbr>deeplearning-wisc/stud</a>.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03800" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03800&amp;source=gmail&amp;ust=1646963617823000&amp;usg=AOvVaw067ulicQ9-RqQ4ox48ygPF" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03800</a> ,&nbsp; 13838kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03806<br>
Date: Tue, 8 Mar 2022 02:00:17 GMT&nbsp; &nbsp;(1416kb,D)<br>
<br>
Title: Panoramic Human Activity Recognition<br>
Authors: Ruize Han, Haomin Yan, Jiacheng Li, Songmiao Wang, Wei Feng, Song Wang<br>
Categories: cs.CV cs.AI<br>
Comments: 17 pages<br>
\\<br>
&nbsp; To obtain a more comprehensive activity understanding for a crowded scene, in<br>
this paper, we propose a new problem of panoramic human activity recognition<br>
(PAR), which aims to simultaneous achieve the individual action, social group<br>
activity, and global activity recognition. This is a challenging yet practical<br>
problem in real-world applications. For this problem, we develop a novel<br>
hierarchical graph neural network to progressively represent and model the<br>
multi-granularity human activities and mutual social relations for a crowd of<br>
people. We further build a benchmark to evaluate the proposed method and other<br>
existing related methods. Experimental results verify the rationality of the<br>
proposed PAR problem, the effectiveness of our method and the usefulness of the<br>
benchmark. We will release the source code and benchmark to the public for<br>
promoting the study on this problem.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03806" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03806&amp;source=gmail&amp;ust=1646963617823000&amp;usg=AOvVaw1_lOhVhrB7sp02LHPuH-4t" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03806</a> ,&nbsp; 1416kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03809<br>
Date: Tue, 8 Mar 2022 02:03:49 GMT&nbsp; &nbsp;(9626kb,D)<br>
<br>
Title: Image Search with Text Feedback by Additive Attention Compositional<br>
&nbsp; Learning<br>
Authors: Yuxin Tian, Shawn Newsam, Kofi Boakye<br>
Categories: cs.CV cs.CL<br>
\\<br>
&nbsp; Effective image retrieval with text feedback stands to impact a range of<br>
real-world applications, such as e-commerce. Given a source image and text<br>
feedback that describes the desired modifications to that image, the goal is to<br>
retrieve the target images that resemble the source yet satisfy the given<br>
modifications by composing a multi-modal (image-text) query. We propose a novel<br>
solution to this problem, Additive Attention Compositional Learning (AACL),<br>
that uses a multi-modal transformer-based architecture and effectively models<br>
the image-text contexts. Specifically, we propose a novel image-text<br>
composition module based on additive attention that can be seamlessly plugged<br>
into deep neural networks. We also introduce a new challenging benchmark<br>
derived from the Shopping100k dataset. AACL is evaluated on three large-scale<br>
datasets (FashionIQ, Fashion200k, and Shopping100k), each with strong<br>
baselines. Extensive experiments show that AACL achieves new state-of-the-art<br>
results on all three datasets.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03809" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03809&amp;source=gmail&amp;ust=1646963617823000&amp;usg=AOvVaw2Q2gTB0VR0A5KN2z-xOE7O" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03809</a> ,&nbsp; 9626kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03818<br>
Date: Tue, 8 Mar 2022 02:40:18 GMT&nbsp; &nbsp;(17765kb)<br>
<br>
Title: Shadows can be Dangerous: Stealthy and Effective Physical-world<br>
&nbsp; Adversarial Attack by Natural Phenomenon<br>
Authors: Yiqi Zhong, Xianming Liu, Deming Zhai, Junjun Jiang, Xiangyang Ji<br>
Categories: cs.CV<br>
Comments: This paper has been accepted by CVPR2022<br>
\\<br>
&nbsp; Estimating the risk level of adversarial examples is essential for safely<br>
deploying machine learning models in the real world. One popular approach for<br>
physical-world attacks is to adopt the "sticker-pasting" strategy, which<br>
however suffers from some limitations, including difficulties in access to the<br>
target or printing by valid colors. A new type of non-invasive attacks emerged<br>
recently, which attempt to cast perturbation onto the target by optics based<br>
tools, such as laser beam and projector. However, the added optical patterns<br>
are artificial but not natural. Thus, they are still conspicuous and<br>
attention-grabbed, and can be easily noticed by humans. In this paper, we study<br>
a new type of optical adversarial examples, in which the perturbations are<br>
generated by a very common natural phenomenon, shadow, to achieve naturalistic<br>
and stealthy physical-world adversarial attack under the black-box setting. We<br>
extensively evaluate the effectiveness of this new attack on both simulated and<br>
real-world environments. Experimental results on traffic sign recognition<br>
demonstrate that our algorithm can generate adversarial examples effectively,<br>
reaching 98.23% and 90.47% success rates on LISA and GTSRB test sets<br>
respectively, while continuously misleading a moving camera over 95% of the<br>
time in real-world scenarios. We also offer discussions about the limitations<br>
and the defense mechanism of this attack.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03818" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03818&amp;source=gmail&amp;ust=1646963617824000&amp;usg=AOvVaw2AMevqqilrdgVPBEYpfI5D" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03818</a> ,&nbsp; 17765kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03819<br>
Date: Tue, 8 Mar 2022 02:44:58 GMT&nbsp; &nbsp;(14008kb,D)<br>
<br>
Title: Table Structure Recognition with Conditional Attention<br>
Authors: Bin Xiao, Murat Simsek, Burak Kantarci and Ala Abu Alkheir<br>
Categories: cs.CV cs.IR<br>
Comments: IJDAR under review<br>
\\<br>
&nbsp; Tabular data in digital documents is widely used to express compact and<br>
important information for readers. However, it is challenging to parse tables<br>
from unstructured digital documents, such as PDFs and images, into<br>
machine-readable format because of the complexity of table structures and the<br>
missing of meta-information. Table Structure Recognition (TSR) problem aims to<br>
recognize the structure of a table and transform the unstructured tables into a<br>
structured and machine-readable format so that the tabular data can be further<br>
analysed by the down-stream tasks, such as semantic modeling and information<br>
retrieval. In this study, we hypothesize that a complicated table structure can<br>
be represented by a graph whose vertices and edges represent the cells and<br>
association between cells, respectively. Then we define the table structure<br>
recognition problem as a cell association classification problem and propose a<br>
conditional attention network (CATT-Net). The experimental results demonstrate<br>
the superiority of our proposed method over the state-of-the-art methods on<br>
various datasets. Besides, we investigate whether the alignment of a cell<br>
bounding box or a text-focused approach has more impact on the model<br>
performance. Due to the lack of public dataset annotations based on these two<br>
approaches, we further annotate the ICDAR2013 dataset providing both types of<br>
bounding boxes, which can be a new benchmark dataset for evaluating the methods<br>
in this field. Experimental results show that the alignment of a cell bounding<br>
box can help improve the Micro-averaged F1 score from 0.915 to 0.963, and the<br>
Macro-average F1 score from 0.787 to 0.923.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03819" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03819&amp;source=gmail&amp;ust=1646963617824000&amp;usg=AOvVaw2EtKab483OCxf3qmim8Jdn" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03819</a> ,&nbsp; 14008kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03821<br>
Date: Tue, 8 Mar 2022 02:57:49 GMT&nbsp; &nbsp;(1095kb,D)<br>
<br>
Title: Coarse-to-Fine Vision Transformer<br>
Authors: Mengzhao Chen, Mingbao Lin, Ke Li, Yunhang Shen, Yongjian Wu, Fei<br>
&nbsp; Chao, Rongrong Ji<br>
Categories: cs.CV<br>
\\<br>
&nbsp; Vision Transformers (ViT) have made many breakthroughs in computer vision<br>
tasks. However, considerable redundancy arises in the spatial dimension of an<br>
input image, leading to massive computational costs. Therefore, We propose a<br>
coarse-to-fine vision transformer (CF-ViT) to relieve computational burden<br>
while retaining performance in this paper. Our proposed CF-ViT is motivated by<br>
two important observations in modern ViT models: (1) The coarse-grained patch<br>
splitting can locate informative regions of an input image. (2) Most images can<br>
be well recognized by a ViT model in a small-length token sequence. Therefore,<br>
our CF-ViT implements network inference in a two-stage manner. At coarse<br>
inference stage, an input image is split into a small-length patch sequence for<br>
a computationally economical classification. If not well recognized, the<br>
informative patches are identified and further re-split in a fine-grained<br>
granularity. Extensive experiments demonstrate the efficacy of our CF-ViT. For<br>
example, without any compromise on performance, CF-ViT reduces 53% FLOPs of<br>
LV-ViT, and also achieves 2.01x throughput.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03821" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03821&amp;source=gmail&amp;ust=1646963617824000&amp;usg=AOvVaw0woKuEbsIuMhOT5vKMf9-6" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03821</a> ,&nbsp; 1095kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03831<br>
Date: Tue, 8 Mar 2022 03:34:10 GMT&nbsp; &nbsp;(80059kb,D)<br>
<br>
Title: Deep Rectangling for Image Stitching: A Learning Baseline<br>
Authors: Lang Nie, Chunyu Lin, Kang Liao, Shuaicheng Liu, Yao Zhao<br>
Categories: cs.CV<br>
Comments: Accepted by CVPR2022; Codes and dataset:<br>
&nbsp; <a href="https://github.com/nie-lang/DeepRectangling" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/nie-lang/DeepRectangling&amp;source=gmail&amp;ust=1646963617824000&amp;usg=AOvVaw2Y7OgPya4nrSm4k9Vs0h1f" rel="noreferrer" target="_blank">https://github.com/nie-lang/<wbr>DeepRectangling</a><br>
\\<br>
&nbsp; Stitched images provide a wide field-of-view (FoV) but suffer from unpleasant<br>
irregular boundaries. To deal with this problem, existing image rectangling<br>
methods devote to searching an initial mesh and optimizing a target mesh to<br>
form the mesh deformation in two stages. Then rectangular images can be<br>
generated by warping stitched images. However, these solutions only work for<br>
images with rich linear structures, leading to noticeable distortions for<br>
portraits and landscapes with non-linear objects. In this paper, we address<br>
these issues by proposing the first deep learning solution to image<br>
rectangling. Concretely, we predefine a rigid target mesh and only estimate an<br>
initial mesh to form the mesh deformation, contributing to a compact one-stage<br>
solution. The initial mesh is predicted using a fully convolutional network<br>
with a residual progressive regression strategy. To obtain results with high<br>
content fidelity, a comprehensive objective function is proposed to<br>
simultaneously encourage the boundary rectangular, mesh shape-preserving, and<br>
content perceptually natural. Besides, we build the first image stitching<br>
rectangling dataset with a large diversity in irregular boundaries and scenes.<br>
Experiments demonstrate our superiority over traditional methods both<br>
quantitatively and qualitatively.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03831" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03831&amp;source=gmail&amp;ust=1646963617824000&amp;usg=AOvVaw2iWynLcyy7RZi3hyt76YeK" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03831</a> ,&nbsp; 80059kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03833<br>
Date: Tue, 8 Mar 2022 03:44:49 GMT&nbsp; &nbsp;(15574kb,D)<br>
<br>
Title: Quasi-Balanced Self-Training on Noise-Aware Synthesis of Object Point<br>
&nbsp; Clouds for Closing Domain Gap<br>
Authors: Yongwei Chen, Zihao Wang, Longkun Zou, Ke Chen, Kui Jia<br>
Categories: cs.CV cs.AI<br>
\\<br>
&nbsp; Semantic analyses of object point clouds are largely driven by releasing of<br>
benchmarking datasets, including synthetic ones whose instances are sampled<br>
from object CAD models. However, learning from synthetic data may not<br>
generalize to practical scenarios, where point clouds are typically incomplete,<br>
non-uniformly distributed, and noisy. Such a challenge of Simulation-to-Real<br>
(Sim2Real) domain gap could be mitigated via learning algorithms of domain<br>
adaptation; however, we argue that generation of synthetic point clouds via<br>
more physically realistic rendering is a powerful alternative, as systematic<br>
non-uniform noise patterns can be captured. To this end, we propose an<br>
integrated scheme consisting of physically realistic synthesis of object point<br>
clouds via rendering stereo images via projection of speckle patterns onto CAD<br>
models and a novel quasi-balanced self-training designed for more balanced data<br>
distribution by sparsity-driven selection of pseudo labeled samples for long<br>
tailed classes. Experiment results can verify the effectiveness of our method<br>
as well as both of its modules for unsupervised domain adaptation on point<br>
cloud classification, achieving the state-of-the-art performance.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03833" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03833&amp;source=gmail&amp;ust=1646963617825000&amp;usg=AOvVaw3sgnaIdYc9x4luOjiOqVPN" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03833</a> ,&nbsp; 15574kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03838<br>
Date: Tue, 8 Mar 2022 04:01:08 GMT&nbsp; &nbsp;(2844kb,D)<br>
<br>
Title: Multi-Scale Self-Contrastive Learning with Hard Negative Mining for<br>
&nbsp; Weakly-Supervised Query-based Video Grounding<br>
Authors: Shentong Mo, Daizong Liu, Wei Hu<br>
Categories: cs.CV cs.AI cs.LG cs.MM<br>
\\<br>
&nbsp; Query-based video grounding is an important yet challenging task in video<br>
understanding, which aims to localize the target segment in an untrimmed video<br>
according to a sentence query. Most previous works achieve significant progress<br>
by addressing this task in a fully-supervised manner with segment-level labels,<br>
which require high labeling cost. Although some recent efforts develop<br>
weakly-supervised methods that only need the video-level knowledge, they<br>
generally match multiple pre-defined segment proposals with query and select<br>
the best one, which lacks fine-grained frame-level details for distinguishing<br>
frames with high repeatability and similarity within the entire video. To<br>
alleviate the above limitations, we propose a self-contrastive learning<br>
framework to address the query-based video grounding task under a<br>
weakly-supervised setting. Firstly, instead of utilizing redundant segment<br>
proposals, we propose a new grounding scheme that learns frame-wise matching<br>
scores referring to the query semantic to predict the possible foreground<br>
frames by only using the video-level annotations. Secondly, since some<br>
predicted frames (i.e., boundary frames) are relatively coarse and exhibit<br>
similar appearance to their adjacent frames, we propose a coarse-to-fine<br>
contrastive learning paradigm to learn more discriminative frame-wise<br>
representations for distinguishing the false positive frames. In particular, we<br>
iteratively explore multi-scale hard negative samples that are close to<br>
positive samples in the representation space for distinguishing fine-grained<br>
frame-wise details, thus enforcing more accurate segment grounding. Extensive<br>
experiments on two challenging benchmarks demonstrate the superiority of our<br>
proposed method compared with the state-of-the-art methods.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03838" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03838&amp;source=gmail&amp;ust=1646963617825000&amp;usg=AOvVaw0maJxjpGWEDiY0Hq7qR9e6" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03838</a> ,&nbsp; 2844kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03843<br>
Date: Tue, 8 Mar 2022 04:26:07 GMT&nbsp; &nbsp;(1602kb,D)<br>
<br>
Title: Self-supervised Social Relation Representation for Human Group Detection<br>
Authors: Jiacheng Li, Ruize Han, Haomin Yan, Zekun Qian, Wei Feng, Song Wang<br>
Categories: cs.CV<br>
Comments: 17 pages<br>
\\<br>
&nbsp; Human group detection, which splits crowd of people into groups, is an<br>
important step for video-based human social activity analysis. The core of<br>
human group detection is the human social relation representation and<br>
division.In this paper, we propose a new two-stage multi-head framework for<br>
human group detection. In the first stage, we propose a human behavior<br>
simulator head to learn the social relation feature embedding, which is<br>
self-supervisely trained by leveraging the socially grounded multi-person<br>
behavior relationship. In the second stage, based on the social relation<br>
embedding, we develop a self-attention inspired network for human group<br>
detection. Remarkable performance on two state-of-the-art large-scale<br>
benchmarks, i.e., PANDA and JRDB-Group, verifies the effectiveness of the<br>
proposed framework. Benefiting from the self-supervised social relation<br>
embedding, our method can provide promising results with very few (labeled)<br>
training data. We will release the source code to the public.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03843" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03843&amp;source=gmail&amp;ust=1646963617826000&amp;usg=AOvVaw3Y_tTZkfynTkCNJc6py3tj" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03843</a> ,&nbsp; 1602kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03859<br>
Date: Tue, 8 Mar 2022 05:30:03 GMT&nbsp; &nbsp;(2133kb)<br>
<br>
Title: A New 27 Class Sign Language Dataset Collected from 173 Individuals<br>
Authors: Arda Mavi and Zeynep Dikle<br>
Categories: cs.CV<br>
Comments: 5 pages, 1 figure<br>
\\<br>
&nbsp; After the interviews, it has been comprehended that speech-impaired<br>
individuals who use sign languages have difficulty communicating with other<br>
people who do not know sign language. Due to the communication problems, the<br>
sense of independence of speech-impaired individuals could be damaged and lead<br>
them to socialize less with society. To contribute to the development of<br>
technologies, that can reduce the communication problems of speech-impaired<br>
persons, a new dataset was presented with this paper. The dataset was created<br>
by processing American Sign Language-based photographs collected from 173<br>
volunteers, published as 27 Class Sign Language Dataset on the Kaggle Datasets<br>
web page.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03859" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03859&amp;source=gmail&amp;ust=1646963617826000&amp;usg=AOvVaw3ULk_WdTAzsbXeHPXDiPmH" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03859</a> ,&nbsp; 2133kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03860<br>
Date: Tue, 8 Mar 2022 05:33:35 GMT&nbsp; &nbsp;(6933kb,D)<br>
<br>
Title: Weakly Supervised Semantic Segmentation using Out-of-Distribution Data<br>
Authors: Jungbeom Lee, Seong Joon Oh, Sangdoo Yun, Junsuk Choe, Eunji Kim,<br>
&nbsp; Sungroh Yoon<br>
Categories: cs.CV<br>
Comments: CVPR 2022<br>
\\<br>
&nbsp; Weakly supervised semantic segmentation (WSSS) methods are often built on<br>
pixel-level localization maps obtained from a classifier. However, training on<br>
class labels only, classifiers suffer from the spurious correlation between<br>
foreground and background cues (e.g. train and rail), fundamentally bounding<br>
the performance of WSSS. There have been previous endeavors to address this<br>
issue with additional supervision. We propose a novel source of information to<br>
distinguish foreground from the background: Out-of-Distribution (OoD) data, or<br>
images devoid of foreground object classes. In particular, we utilize the hard<br>
OoDs that the classifier is likely to make false-positive predictions. These<br>
samples typically carry key visual features on the background (e.g. rail) that<br>
the classifiers often confuse as foreground (e.g. train), so these cues let<br>
classifiers correctly suppress spurious background cues. Acquiring such hard<br>
OoDs does not require an extensive amount of annotation efforts; it only incurs<br>
a few additional image-level labeling costs on top of the original efforts to<br>
collect class labels. We propose a method, W-OoD, for utilizing the hard OoDs.<br>
W-OoD achieves state-of-the-art performance on Pascal VOC 2012.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03860" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03860&amp;source=gmail&amp;ust=1646963617826000&amp;usg=AOvVaw1H6TuDIfFkntfNlY4Y5Pvg" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03860</a> ,&nbsp; 6933kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03871<br>
Date: Tue, 8 Mar 2022 06:16:33 GMT&nbsp; &nbsp;(8146kb,D)<br>
<br>
Title: Discriminability-<wbr>Transferability Trade-Off: An Information-Theoretic<br>
&nbsp; Perspective<br>
Authors: Quan Cui, Bingchen Zhao, Zhao-Min Chen, Borui Zhao, Renjie Song,<br>
&nbsp; Jiajun Liang, Boyan Zhou, Osamu Yoshie<br>
Categories: cs.CV cs.AI<br>
Comments: First two authors contributed equally<br>
\\<br>
&nbsp; This work simultaneously considers the discriminability and transferability<br>
properties of deep representations in the typical supervised learning task,<br>
i.e., image classification. By a comprehensive temporal analysis, we observe a<br>
trade-off between these two properties. The discriminability keeps increasing<br>
with the training progressing while the transferability intensely diminishes in<br>
the later training period.<br>
&nbsp; From the perspective of information-bottleneck theory, we reveal that the<br>
incompatibility between discriminability and transferability is attributed to<br>
the over-compression of input information. More importantly, we investigate why<br>
and how the InfoNCE loss can alleviate the over-compression, and further<br>
present a learning framework, named contrastive temporal coding~(CTC), to<br>
counteract the over-compression and alleviate the incompatibility. Extensive<br>
experiments validate that CTC successfully mitigates the incompatibility,<br>
yielding discriminative and transferable representations. Noticeable<br>
improvements are achieved on the image classification task and challenging<br>
transfer learning tasks. We hope that this work will raise the significance of<br>
the transferability property in the conventional supervised learning setting.<br>
Code will be publicly available.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03871" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03871&amp;source=gmail&amp;ust=1646963617826000&amp;usg=AOvVaw3seaNXxvgkuZmBmtJrEN80" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03871</a> ,&nbsp; 8146kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03872<br>
Date: Tue, 8 Mar 2022 06:22:04 GMT&nbsp; &nbsp;(2648kb)<br>
<br>
Title: Visual anomaly detection in video by variational autoencoder<br>
Authors: Faraz Waseem (yahoo), Rafael Perez Martinez (Stanford University),<br>
&nbsp; Chris Wu (Stanford University)<br>
Categories: cs.CV cs.AI<br>
\\<br>
&nbsp; Video anomalies detection is the intersection of anomaly detection and visual<br>
intelligence. It has commercial applications in surveillance, security,<br>
self-driving cars and crop monitoring. Videos can capture a variety of<br>
anomalies. Due to efforts needed to label training data, unsupervised<br>
approaches to train anomaly detection models for videos is more practical An<br>
autoencoder is a neural network that is trained to recreate its input using<br>
latent representation of input also called a bottleneck layer. Variational<br>
autoencoder uses distribution (mean and variance) as compared to latent vector<br>
as bottleneck layer and can have better regularization effect. In this paper we<br>
have demonstrated comparison between performance of convolutional LSTM versus a<br>
variation convolutional LSTM autoencoder<br>
\\ ( <a href="https://arxiv.org/abs/2203.03872" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03872&amp;source=gmail&amp;ust=1646963617827000&amp;usg=AOvVaw0EPC9TJAY-Wf1_1GYSXM5k" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03872</a> ,&nbsp; 2648kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03884<br>
Date: Tue, 8 Mar 2022 07:16:23 GMT&nbsp; &nbsp;(4852kb,D)<br>
<br>
Title: Semi-Supervised Semantic Segmentation Using Unreliable Pseudo-Labels<br>
Authors: Yuchao Wang, Haochen Wang, Yujun Shen, Jingjing Fei, Wei Li, Guoqiang<br>
&nbsp; Jin, Liwei Wu, Rui Zhao, Xinyi Le<br>
Categories: cs.CV<br>
Comments: Accepted to CVPR 2022<br>
\\<br>
&nbsp; The crux of semi-supervised semantic segmentation is to assign adequate<br>
pseudo-labels to the pixels of unlabeled images. A common practice is to select<br>
the highly confident predictions as the pseudo ground-truth, but it leads to a<br>
problem that most pixels may be left unused due to their unreliability. We<br>
argue that every pixel matters to the model training, even its prediction is<br>
ambiguous. Intuitively, an unreliable prediction may get confused among the top<br>
classes (i.e., those with the highest probabilities), however, it should be<br>
confident about the pixel not belonging to the remaining classes. Hence, such a<br>
pixel can be convincingly treated as a negative sample to those most unlikely<br>
categories. Based on this insight, we develop an effective pipeline to make<br>
sufficient use of unlabeled data. Concretely, we separate reliable and<br>
unreliable pixels via the entropy of predictions, push each unreliable pixel to<br>
a category-wise queue that consists of negative samples, and manage to train<br>
the model with all candidate pixels. Considering the training evolution, where<br>
the prediction becomes more and more accurate, we adaptively adjust the<br>
threshold for the reliable-unreliable partition. Experimental results on<br>
various benchmarks and training settings demonstrate the superiority of our<br>
approach over the state-of-the-art alternatives.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03884" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03884&amp;source=gmail&amp;ust=1646963617827000&amp;usg=AOvVaw3Y5J8GKVNojuTW22Ehy2aC" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03884</a> ,&nbsp; 4852kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03886<br>
Date: Tue, 8 Mar 2022 07:18:16 GMT&nbsp; &nbsp;(4454kb)<br>
<br>
Title: Boosting Mask R-CNN Performance for Long, Thin Forensic Traces with<br>
&nbsp; Pre-Segmentation and IoU Region Merging<br>
Authors: Moritz Zink, Martin Schiele, Pengcheng Fan, Stephan Gasterst\"adt<br>
Categories: cs.CV cs.AI cs.RO<br>
Comments: 9 Pages, 19 Figures<br>
\\<br>
&nbsp; Mask R-CNN has recently achieved great success in the field of instance<br>
segmentation. However, weaknesses of the algorithm have been repeatedly pointed<br>
out as well, especially in the segmentation of long, sparse objects whose<br>
orientation is not exclusively horizontal or vertical. We present here an<br>
approach that significantly improves the performance of the algorithm by first<br>
pre-segmenting the images with a PSPNet algorithm. To further improve its<br>
prediction, we have developed our own cost functions and heuristics in the form<br>
of training strategies, which can prevent so-called (early) overfitting and<br>
achieve a more targeted convergence. Furthermore, due to the high variance of<br>
the images, especially for PSPNet, we aimed to develop strategies for a high<br>
robustness and generalization, which are also presented here.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03886" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03886&amp;source=gmail&amp;ust=1646963617827000&amp;usg=AOvVaw0ABg4gj6wbzAlh9sWkWHIP" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03886</a> ,&nbsp; 4454kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03888<br>
Date: Tue, 8 Mar 2022 07:20:16 GMT&nbsp; &nbsp;(2449kb,D)<br>
<br>
Title: ART-Point: Improving Rotation Robustness of Point Cloud Classifiers via<br>
&nbsp; Adversarial Rotation<br>
Authors: Robin Wang, Yibo Yang, Dacheng Tao<br>
Categories: cs.CV<br>
Comments: CVPR 2022<br>
\\<br>
&nbsp; Point cloud classifiers with rotation robustness have been widely discussed<br>
in the 3D deep learning community. Most proposed methods either use rotation<br>
invariant descriptors as inputs or try to design rotation equivariant networks.<br>
However, robust models generated by these methods have limited performance<br>
under clean aligned datasets due to modifications on the original classifiers<br>
or input space. In this study, for the first time, we show that the rotation<br>
robustness of point cloud classifiers can also be acquired via adversarial<br>
training with better performance on both rotated and clean datasets.<br>
Specifically, our proposed framework named ART-Point regards the rotation of<br>
the point cloud as an attack and improves rotation robustness by training the<br>
classifier on inputs with Adversarial RoTations. We contribute an axis-wise<br>
rotation attack that uses back-propagated gradients of the pre-trained model to<br>
effectively find the adversarial rotations. To avoid model over-fitting on<br>
adversarial inputs, we construct rotation pools that leverage the<br>
transferability of adversarial rotations among samples to increase the<br>
diversity of training data. Moreover, we propose a fast one-step optimization<br>
to efficiently reach the final robust model. Experiments show that our proposed<br>
rotation attack achieves a high success rate and ART-Point can be used on most<br>
existing classifiers to improve the rotation robustness while showing better<br>
performance on clean datasets than state-of-the-art methods.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03888" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03888&amp;source=gmail&amp;ust=1646963617827000&amp;usg=AOvVaw14H2NdsWeazgHxQp2osmrC" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03888</a> ,&nbsp; 2449kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03890<br>
Date: Tue, 8 Mar 2022 07:29:31 GMT&nbsp; &nbsp;(38748kb,D)<br>
<br>
Title: ClearPose: Large-scale Transparent Object Dataset and Benchmark<br>
Authors: Xiaotong Chen, Huijie Zhang, Zeren Yu, Anthony Opipari, Odest<br>
&nbsp; Chadwicke Jenkins<br>
Categories: cs.CV cs.RO<br>
\\<br>
&nbsp; Transparent objects are ubiquitous in household settings and pose distinct<br>
challenges for visual sensing and perception systems. The optical properties of<br>
transparent objects leave conventional 3D sensors alone unreliable for object<br>
depth and pose estimation. These challenges are highlighted by the shortage of<br>
large-scale RGB-Depth datasets focusing on transparent objects in real-world<br>
settings. In this work, we contribute a large-scale real-world RGB-Depth<br>
transparent object dataset named ClearPose to serve as a benchmark dataset for<br>
segmentation, scene-level depth completion and object-centric pose estimation<br>
tasks. The ClearPose dataset contains over 350K labeled real-world RGB-Depth<br>
frames and 4M instance annotations covering 63 household objects. The dataset<br>
includes object categories commonly used in daily life under various lighting<br>
and occluding conditions as well as challenging test scenarios such as cases of<br>
occlusion by opaque or translucent objects, non-planar orientations, presence<br>
of liquids, etc. We benchmark several state-of-the-art depth completion and<br>
object pose estimation deep neural networks on ClearPose.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03890" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03890&amp;source=gmail&amp;ust=1646963617827000&amp;usg=AOvVaw2PzvBBcBrKtPeCjlljrH5N" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03890</a> ,&nbsp; 38748kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03897<br>
Date: Tue, 8 Mar 2022 07:34:52 GMT&nbsp; &nbsp;(6058kb,D)<br>
<br>
Title: Multi-Modal Mixup for Robust Fine-tuning<br>
Authors: Junhyuk So, Changdae Oh, Minchul Shin, Kyungwoo Song<br>
Categories: cs.CV cs.CL cs.IR cs.LG<br>
\\<br>
&nbsp; Pre-trained large-scale models provide a transferable embedding, and they<br>
show comparable performance on the diverse downstream task. However, the<br>
transferability of multi-modal learning is restricted, and the analysis of<br>
learned embedding has not been explored well. This paper provides a perspective<br>
to understand the multi-modal embedding in terms of uniformity and alignment.<br>
We newly find that the representation learned by multi-modal learning models<br>
such as CLIP has a two separated representation space for each heterogeneous<br>
dataset with less alignment. Besides, there are unexplored large intermediate<br>
areas between two modalities with less uniformity. Less robust embedding might<br>
restrict the transferability of the representation for the downstream task.<br>
This paper provides a new end-to-end fine-tuning method for robust<br>
representation that encourages better uniformity and alignment score. First, we<br>
propose a multi-modal Mixup, $m^{2}$-Mix that mixes the representation of image<br>
and text to generate the hard negative samples. Second, we fine-tune the<br>
multi-modal model on a hard negative sample as well as normal negative and<br>
positive samples with contrastive learning. Our multi-modal Mixup provides a<br>
robust representation, and we validate our methods on classification,<br>
retrieval, and structure-awareness task.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03897" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03897&amp;source=gmail&amp;ust=1646963617828000&amp;usg=AOvVaw34fwRzG3Od4QNb834E6PiQ" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03897</a> ,&nbsp; 6058kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03905<br>
Date: Tue, 8 Mar 2022 08:02:33 GMT&nbsp; &nbsp;(1101kb,D)<br>
<br>
Title: End-to-end system for object detection from sub-sampled radar data<br>
Authors: Madhumitha Sakthi, Ahmed Tewfik, Marius Arvinte, Haris Vikalo<br>
Categories: cs.CV<br>
Comments: Submitted to EUSIPCO 2022<br>
\\<br>
&nbsp; Robust and accurate sensing is of critical importance for advancing<br>
autonomous automotive systems. The need to acquire situational awareness in<br>
complex urban conditions using sensors such as radar has motivated research on<br>
power and latency-efficient signal acquisition methods. In this paper, we<br>
present an end-to-end signal processing pipeline, capable of operating in<br>
extreme weather conditions, that relies on sub-sampled radar data to perform<br>
object detection in vehicular settings. The results of the object detection are<br>
further utilized to sub-sample forthcoming radar data, which stands in contrast<br>
to prior work where the sub-sampling relies on image information. We show<br>
robust detection based on radar data reconstructed using 20% of samples under<br>
extreme weather conditions such as snow or fog, and on low-illuminated nights.<br>
Additionally, we generate 20% sampled radar data in a fine-tuning set and show<br>
1.1% gain in AP50 across scenes and 3% AP50 gain in motorway condition.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03905" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03905&amp;source=gmail&amp;ust=1646963617828000&amp;usg=AOvVaw0EM8Vn811r8GSxmLp8XP8T" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03905</a> ,&nbsp; 1101kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03911<br>
Date: Tue, 8 Mar 2022 08:10:45 GMT&nbsp; &nbsp;(1234kb,D)<br>
<br>
Title: Language Matters: A Weakly Supervised Pre-training Approach for Scene<br>
&nbsp; Text Detection and Spotting<br>
Authors: Chuhui Xue, Yu Hao, Shijian Lu, Philip Torr, Song Bai<br>
Categories: cs.CV<br>
\\<br>
&nbsp; Recently, Vision-Language Pre-training (VLP) techniques have greatly<br>
benefited various vision-language tasks by jointly learning visual and textual<br>
representations, which intuitively helps in Optical Character Recognition (OCR)<br>
tasks due to the rich visual and textual information in scene text images.<br>
However, these methods cannot well cope with OCR tasks because of the<br>
difficulty in both instance-level text encoding and image-text pair acquisition<br>
(i.e. images and captured texts in them). This paper presents a weakly<br>
supervised pre-training method that can acquire effective scene text<br>
representations by jointly learning and aligning visual and textual<br>
information. Our network consists of an image encoder and a character-aware<br>
text encoder that extract visual and textual features, respectively, as well as<br>
a visual-textual decoder that models the interaction among textual and visual<br>
features for learning effective scene text representations. With the learning<br>
of textual features, the pre-trained model can attend texts in images well with<br>
character awareness. Besides, these designs enable the learning from weakly<br>
annotated texts (i.e. partial texts in images without text bounding boxes)<br>
which mitigates the data annotation constraint greatly. Experiments over the<br>
weakly annotated images in ICDAR2019-LSVT show that our pre-trained model<br>
improves F-score by +2.5% and +4.8% while transferring its weights to other<br>
text detection and spotting networks, respectively. In addition, the proposed<br>
method outperforms existing pre-training techniques consistently across<br>
multiple public datasets (e.g., +3.2% and +1.3% for Total-Text and CTW1500).<br>
\\ ( <a href="https://arxiv.org/abs/2203.03911" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03911&amp;source=gmail&amp;ust=1646963617828000&amp;usg=AOvVaw3h__Z91SnNl40km7d0gUxq" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03911</a> ,&nbsp; 1234kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03914<br>
Date: Tue, 8 Mar 2022 08:24:22 GMT&nbsp; &nbsp;(3645kb,D)<br>
<br>
Title: Globally-Optimal Event Camera Motion Estimation<br>
Authors: Xin Peng, Yifu Wang, Ling Gao, Laurent Kneip<br>
Categories: cs.CV<br>
Journal-ref: European Conference on Computer Vision (ECCV), 2020<br>
DOI: 10.1007/978-3-030-58574-7_4<br>
\\<br>
&nbsp; Event cameras are bio-inspired sensors that perform well in HDR conditions<br>
and have high temporal resolution. However, different from traditional<br>
frame-based cameras, event cameras measure asynchronous pixel-level brightness<br>
changes and return them in a highly discretised format, hence new algorithms<br>
are needed. The present paper looks at fronto-parallel motion estimation of an<br>
event camera. The flow of the events is modeled by a general homographic<br>
warping in a space-time volume, and the objective is formulated as a<br>
maximisation of contrast within the image of unwarped events. However, in stark<br>
contrast to prior art, we derive a globally optimal solution to this generally<br>
non-convex problem, and thus remove the dependency on a good initial guess. Our<br>
algorithm relies on branch-and-bound optimisation for which we derive novel,<br>
recursive upper and lower bounds for six different contrast estimation<br>
functions. The practical validity of our approach is supported by a highly<br>
successful application to AGV motion estimation with a downward facing event<br>
camera, a challenging scenario in which the sensor experiences fronto-parallel<br>
motion in front of noisy, fast moving textures.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03914" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03914&amp;source=gmail&amp;ust=1646963617828000&amp;usg=AOvVaw1zBOVMWVOfap2-_t5Wul4x" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03914</a> ,&nbsp; 3645kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03931<br>
Date: Tue, 8 Mar 2022 08:54:00 GMT&nbsp; &nbsp;(4227kb,D)<br>
<br>
Title: Part-Aware Self-Supervised Pre-Training for Person Re-Identification<br>
Authors: Kuan Zhu, Haiyun Guo, Tianyi Yan, Yousong Zhu, Jinqiao Wang, Ming Tang<br>
Categories: cs.CV<br>
\\<br>
&nbsp; In person re-identification (ReID), very recent researches have validated<br>
pre-training the models on unlabelled person images is much better than on<br>
ImageNet. However, these researches directly apply the existing self-supervised<br>
learning (SSL) methods designed for image classification to ReID without any<br>
adaption in the framework. These SSL methods match the outputs of local views<br>
(e.g., red T-shirt, blue shorts) to those of the global views at the same time,<br>
losing lots of details. In this paper, we propose a ReID-specific pre-training<br>
method, Part-Aware Self-Supervised pre-training (PASS), which can generate<br>
part-level features to offer fine-grained information and is more suitable for<br>
ReID. PASS divides the images into several local areas, and the local views<br>
randomly cropped from each area are assigned with a specific learnable [PART]<br>
token. On the other hand, the [PART]s of all local areas are also appended to<br>
the global views. PASS learns to match the output of the local views and global<br>
views on the same [PART]. That is, the learned [PART] of the local views from a<br>
local area is only matched with the corresponding [PART] learned from the<br>
global views. As a result, each [PART] can focus on a specific local area of<br>
the image and extracts fine-grained information of this area. Experiments show<br>
PASS sets the new state-of-the-art performances on Market1501 and MSMT17 on<br>
various ReID tasks, e.g., vanilla ViT-S/16 pre-trained by PASS achieves<br>
92.2\%/90.2\%/88.5\% mAP accuracy on Market1501 for supervised/UDA/USL ReID.<br>
Our codes are available at <a href="https://github.com/CASIA-IVA-Lab/PASS-reID" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/CASIA-IVA-Lab/PASS-reID&amp;source=gmail&amp;ust=1646963617829000&amp;usg=AOvVaw0ztKVUIX07xO3mayFnyxFY" rel="noreferrer" target="_blank">https://github.com/CASIA-IVA-<wbr>Lab/PASS-reID</a>.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03931" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03931&amp;source=gmail&amp;ust=1646963617829000&amp;usg=AOvVaw0NQ1Do4Ku34INvevav6VW9" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03931</a> ,&nbsp; 4227kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03937<br>
Date: Tue, 8 Mar 2022 09:01:41 GMT&nbsp; &nbsp;(2604kb,D)<br>
<br>
Title: Dynamic Group Transformer: A General Vision Transformer Backbone with<br>
&nbsp; Dynamic Group Attention<br>
Authors: Kai Liu, Tianyi Wu, Cong Liu, Guodong Guo<br>
Categories: cs.CV<br>
Comments: 8 pages, 3 figures<br>
ACM-class: I.2.0<br>
\\<br>
&nbsp; Recently, Transformers have shown promising performance in various vision<br>
tasks. To reduce the quadratic computation complexity caused by each query<br>
attending to all keys/values, various methods have constrained the range of<br>
attention within local regions, where each query only attends to keys/values<br>
within a hand-crafted window. However, these hand-crafted window partition<br>
mechanisms are data-agnostic and ignore their input content, so it is likely<br>
that one query maybe attends to irrelevant keys/values. To address this issue,<br>
we propose a Dynamic Group Attention (DG-Attention), which dynamically divides<br>
all queries into multiple groups and selects the most relevant keys/values for<br>
each group. Our DG-Attention can flexibly model more relevant dependencies<br>
without any spatial constraint that is used in hand-crafted window based<br>
attention. Built on the DG-Attention, we develop a general vision transformer<br>
backbone named Dynamic Group Transformer (DGT). Extensive experiments show that<br>
our models can outperform the state-of-the-art methods on multiple common<br>
vision tasks, including image classification, semantic segmentation, object<br>
detection, and instance segmentation.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03937" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03937&amp;source=gmail&amp;ust=1646963617829000&amp;usg=AOvVaw1ais1ol841vjBe6y2E27j8" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03937</a> ,&nbsp; 2604kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03949<br>
Date: Tue, 8 Mar 2022 09:24:05 GMT&nbsp; &nbsp;(36511kb,D)<br>
<br>
Title: RC-MVSNet: Unsupervised Multi-View Stereo with Neural Rendering<br>
Authors: Di Chang, Alja\v{z} Bo\v{z}i\v{c}, Tong Zhang, Qingsong Yan, Yingcong<br>
&nbsp; Chen, Sabine S\"usstrunk, Matthias Nie{\ss}ner<br>
Categories: cs.CV<br>
Comments: 10 pages, 7 figures<br>
\\<br>
&nbsp; Finding accurate correspondences among different views is the Achilles' heel<br>
of unsupervised Multi-View Stereo (MVS). Existing methods are built upon the<br>
assumption that corresponding pixels share similar photometric features.<br>
However, multi-view images in real scenarios observe non-Lambertian surfaces<br>
and experience occlusions. In this work, we propose a novel approach with<br>
neural rendering (RC-MVSNet) to solve such ambiguity issues of correspondences<br>
among views. Specifically, we impose a depth rendering consistency loss to<br>
constrain the geometry features close to the object surface to alleviate<br>
occlusions. Concurrently, we introduce a reference view synthesis loss to<br>
generate consistent supervision, even for non-Lambertian surfaces. Extensive<br>
experiments on DTU and Tanks\&amp;Temples benchmarks demonstrate that our RC-MVSNet<br>
approach achieves state-of-the-art performance over unsupervised MVS frameworks<br>
and competitive performance to many supervised methods.The trained models and<br>
code will be released at <a href="https://github.com/Boese0601/RC-MVSNet" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/Boese0601/RC-MVSNet&amp;source=gmail&amp;ust=1646963617829000&amp;usg=AOvVaw1Eq6h9P0NbecOJktOI9u_j" rel="noreferrer" target="_blank">https://github.com/Boese0601/<wbr>RC-MVSNet</a>.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03949" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03949&amp;source=gmail&amp;ust=1646963617829000&amp;usg=AOvVaw0LkB4h0rakXJwlWlIQwURJ" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03949</a> ,&nbsp; 36511kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03951<br>
Date: Tue, 8 Mar 2022 09:24:12 GMT&nbsp; &nbsp;(743kb)<br>
<br>
Title: Efficient and Accurate Hyperspectral Pansharpening Using 3D VolumeNet<br>
&nbsp; and 2.5D Texture Transfer<br>
Authors: Yinao Li, Yutaro Iwamoto, Ryousuke Nakamura, Lanfen Lin, Ruofeng Tong,<br>
&nbsp; Yen-Wei Chen<br>
Categories: cs.CV<br>
\\<br>
&nbsp; Recently, convolutional neural networks (CNN) have obtained promising results<br>
in single-image SR for hyperspectral pansharpening. However, enhancing CNNs'<br>
representation ability with fewer parameters and a shorter prediction time is a<br>
challenging and critical task. In this paper, we propose a novel multi-spectral<br>
image fusion method using a combination of the previously proposed 3D CNN model<br>
VolumeNet and 2.5D texture transfer method using other modality high resolution<br>
(HR) images. Since a multi-spectral (MS) image consists of several bands and<br>
each band is a 2D image slice, MS images can be seen as 3D data. Thus, we use<br>
the previously proposed VolumeNet to fuse HR panchromatic (PAN) images and<br>
bicubic interpolated MS images. Because the proposed 3D VolumeNet can<br>
effectively improve the accuracy by expanding the receptive field of the model,<br>
and due to its lightweight structure, we can achieve better performance against<br>
the existing method without purchasing a large number of remote sensing images<br>
for training. In addition, VolumeNet can restore the high-frequency information<br>
lost in the HR MR image as much as possible, reducing the difficulty of feature<br>
extraction in the following step: 2.5D texture transfer. As one of the latest<br>
technologies, deep learning-based texture transfer has been demonstrated to<br>
effectively and efficiently improve the visual performance and quality<br>
evaluation indicators of image reconstruction. Different from the texture<br>
transfer processing of RGB image, we use HR PAN images as the reference images<br>
and perform texture transfer for each frequency band of MS images, which is<br>
named 2.5D texture transfer. The experimental results show that the proposed<br>
method outperforms the existing methods in terms of objective accuracy<br>
assessment, method efficiency, and visual subjective evaluation.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03951" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03951&amp;source=gmail&amp;ust=1646963617830000&amp;usg=AOvVaw2U7tjs5TLh8BgZisCQ5R-g" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03951</a> ,&nbsp; 743kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03952<br>
Date: Tue, 8 Mar 2022 09:25:17 GMT&nbsp; &nbsp;(3712kb,D)<br>
<br>
Title: EdgeFormer: Improving Light-weight ConvNets by Learning from Vision<br>
&nbsp; Transformers<br>
Authors: Haokui Zhang, Wenze Hu, Xiaoyu Wang<br>
Categories: cs.CV<br>
Comments: 14 pages, 7 figures and 4 tables<br>
\\<br>
&nbsp; Recently, vision transformers started to show impressive results which<br>
outperform large convolution based models significantly. However, in the area<br>
of small models for mobile or resource constrained devices, ConvNet still has<br>
its own advantages in both performance and model complexity. We propose<br>
EdgeFormer, a pure ConvNet based backbone model that further strengthens these<br>
advantages by fusing the merits of vision transformers into ConvNets.<br>
Specifically, we propose global circular convolution (GCC) with position<br>
embeddings, a light-weight convolution op which boasts a global receptive field<br>
while producing location sensitive features as in local convolutions. We<br>
combine the GCCs and squeeze-exictation ops to form a meta-former like model<br>
block, which further has the attention mechanism like transformers. The<br>
aforementioned block can be used in plug-and-play manner to replace relevant<br>
blocks in ConvNets or transformers. Experiment results show that the proposed<br>
EdgeFormer achieves better performance than popular light-weight ConvNets and<br>
vision transformer based models in common vision tasks and datasets, while<br>
having fewer parameters and faster inference speed. For classification on<br>
ImageNet-1k, EdgeFormer achieves 78.6% top-1 accuracy with about 5.0 million<br>
parameters, saving 11% parameters and 13% computational cost but gaining 0.2%<br>
higher accuracy and 23% faster inference speed (on ARM based Rockchip RK3288)<br>
compared with MobileViT, and uses only 0.5 times parameters but gaining 2.7%<br>
accuracy compared with DeIT. On MS-COCO object detection and PASCAL VOC<br>
segmentation tasks, EdgeFormer also shows better performance.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03952" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03952&amp;source=gmail&amp;ust=1646963617830000&amp;usg=AOvVaw18K7oGMfSdPKg11_zUBfpE" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03952</a> ,&nbsp; 3712kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03962<br>
Date: Tue, 8 Mar 2022 09:36:51 GMT&nbsp; &nbsp;(645kb,D)<br>
<br>
Title: Generative Cooperative Learning for Unsupervised Video Anomaly Detection<br>
Authors: Muhammad Zaigham Zaheer, Arif Mahmood, Muhammad Haris Khan, Mattia<br>
&nbsp; Segu, Fisher Yu, Seung-Ik Lee<br>
Categories: cs.CV<br>
Comments: Accepted to the Conference on Computer Vision and Pattern Recognition<br>
&nbsp; CVPR 2022<br>
\\<br>
&nbsp; Video anomaly detection is well investigated in weakly-supervised and<br>
one-class classification (OCC) settings. However, unsupervised video anomaly<br>
detection methods are quite sparse, likely because anomalies are less frequent<br>
in occurrence and usually not well-defined, which when coupled with the absence<br>
of ground truth supervision, could adversely affect the performance of the<br>
learning algorithms. This problem is challenging yet rewarding as it can<br>
completely eradicate the costs of obtaining laborious annotations and enable<br>
such systems to be deployed without human intervention. To this end, we propose<br>
a novel unsupervised Generative Cooperative Learning (GCL) approach for video<br>
anomaly detection that exploits the low frequency of anomalies towards building<br>
a cross-supervision between a generator and a discriminator. In essence, both<br>
networks get trained in a cooperative fashion, thereby allowing unsupervised<br>
learning. We conduct extensive experiments on two large-scale video anomaly<br>
detection datasets, UCF crime, and ShanghaiTech. Consistent improvement over<br>
the existing state-of-the-art unsupervised and OCC methods corroborate the<br>
effectiveness of our approach.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03962" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03962&amp;source=gmail&amp;ust=1646963617830000&amp;usg=AOvVaw3T1Ad2yiAvZk4N2YrP0qsG" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03962</a> ,&nbsp; 645kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03966<br>
Date: Tue, 8 Mar 2022 09:49:48 GMT&nbsp; &nbsp;(2490kb,D)<br>
<br>
Title: GaitStrip: Gait Recognition via Effective Strip-based Feature<br>
&nbsp; Representations and Multi-Level Framework<br>
Authors: Ming Wang, Beibei Lin, Xianda Guo, Lincheng Li, Zheng Zhu, Jiande Sun,<br>
&nbsp; Shunli Zhang and Xin Yu<br>
Categories: cs.CV<br>
\\<br>
&nbsp; Many gait recognition methods first partition the human gait into N-parts and<br>
then combine them to establish part-based feature representations. Their gait<br>
recognition performance is often affected by partitioning strategies, which are<br>
empirically chosen in different datasets. However, we observe that strips as<br>
the basic component of parts are agnostic against different partitioning<br>
strategies. Motivated by this observation, we present a strip-based multi-level<br>
gait recognition network, named GaitStrip, to extract comprehensive gait<br>
information at different levels. To be specific, our high-level branch explores<br>
the context of gait sequences and our low-level one focuses on detailed posture<br>
changes. We introduce a novel StriP-Based feature extractor (SPB) to learn the<br>
strip-based feature representations by directly taking each strip of the human<br>
body as the basic unit. Moreover, we propose a novel multi-branch structure,<br>
called Enhanced Convolution Module (ECM), to extract different representations<br>
of gaits. ECM consists of the Spatial-Temporal feature extractor (ST), the<br>
Frame-Level feature extractor (FL) and SPB, and has two obvious advantages:<br>
First, each branch focuses on a specific representation, which can be used to<br>
improve the robustness of the network. Specifically, ST aims to extract<br>
spatial-temporal features of gait sequences, while FL is used to generate the<br>
feature representation of each frame. Second, the parameters of the ECM can be<br>
reduced in test by introducing a structural re-parameterization technique.<br>
Extensive experimental results demonstrate that our GaitStrip achieves<br>
state-of-the-art performance in both normal walking and complex conditions.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03966" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03966&amp;source=gmail&amp;ust=1646963617830000&amp;usg=AOvVaw2WaNk2bPZhM2yCY37vKtIf" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03966</a> ,&nbsp; 2490kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03971<br>
Date: Tue, 8 Mar 2022 09:58:40 GMT&nbsp; &nbsp;(1979kb,D)<br>
<br>
Title: Universal Prototype Transport for Zero-Shot Action Recognition and<br>
&nbsp; Localization<br>
Authors: Pascal Mettes<br>
Categories: cs.CV<br>
\\<br>
&nbsp; This work addresses the problem of recognizing action categories in videos<br>
for which no training examples are available. The current state-of-the-art<br>
enables such a zero-shot recognition by learning universal mappings from videos<br>
to a shared semantic space, either trained on large-scale seen actions or on<br>
objects. While effective, we find that universal action and object mappings are<br>
biased to their seen categories. Such biases are further amplified due to<br>
biases between seen and unseen categories in the semantic space. The<br>
compounding biases result in many unseen action categories simply never being<br>
selected during inference, hampering zero-shot progress. We seek to address<br>
this limitation and introduce universal prototype transport for zero-shot<br>
action recognition. The main idea is to re-position the semantic prototypes of<br>
unseen actions through transduction, i.e. by using the distribution of the<br>
unlabelled test set. For universal action models, we first seek to find a<br>
hyperspherical optimal transport mapping from unseen action prototypes to the<br>
set of all projected test videos. We then define a target prototype for each<br>
unseen action as the weighted Fr\'echet mean over the transport couplings.<br>
Equipped with a target prototype, we propose to re-position unseen action<br>
prototypes along the geodesic spanned by the original and target prototypes,<br>
acting as a form of semantic regularization. For universal object models, we<br>
outline a variant that defines target prototypes based on an optimal transport<br>
between unseen action prototypes and semantic object prototypes. Empirically,<br>
we show that universal prototype transport diminishes the biased selection of<br>
unseen action prototypes and boosts both universal action and object models,<br>
resulting in state-of-the-art performance for zero-shot classification and<br>
spatio-temporal localization.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03971" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03971&amp;source=gmail&amp;ust=1646963617831000&amp;usg=AOvVaw1ACJc8u5hR4OVx_0QHL9pZ" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03971</a> ,&nbsp; 1979kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03972<br>
Date: Tue, 8 Mar 2022 09:58:46 GMT&nbsp; &nbsp;(565kb,D)<br>
<br>
Title: GaitEdge: Beyond Plain End-to-end Gait Recognition for Better<br>
&nbsp; Practicality<br>
Authors: Junhao Liang, Chao Fan, Saihui Hou, Chuanfu Shen, Yongzhen Huang,<br>
&nbsp; Shiqi Yu<br>
Categories: cs.CV<br>
\\<br>
&nbsp; Gait is one of the most promising biometrics to identify individuals at a<br>
long distance. Although most previous methods have focused on recognizing the<br>
silhouettes, several end-to-end methods that extract gait features directly<br>
from RGB images perform better. However, we argue that these end-to-end methods<br>
inevitably suffer from the gait-unrelated noises, i.e., low-level texture and<br>
colorful information. Experimentally, we design both the cross-domain<br>
evaluation and visualization to stand for this view. In this work, we propose a<br>
novel end-to-end framework named GaitEdge which can effectively block<br>
gait-unrelated information and release end-to-end training potential.<br>
Specifically, GaitEdge synthesizes the output of the pedestrian segmentation<br>
network and then feeds it to the subsequent recognition network, where the<br>
synthetic silhouettes consist of trainable edges of bodies and fixed interiors<br>
to limit the information that the recognition network receives. Besides,<br>
GaitAlign for aligning silhouettes is embedded into the GaitEdge without loss<br>
of differentiability. Experimental results on CASIA-B and our newly built<br>
TTG-200 indicate that GaitEdge significantly outperforms the previous methods<br>
and provides a more practical end-to-end paradigm for blocking RGB noises<br>
effectively. All the source code will be released.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03972" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03972&amp;source=gmail&amp;ust=1646963617831000&amp;usg=AOvVaw1QyP8uSNhHOdKKvcUF3Kcv" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03972</a> ,&nbsp; 565kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03981<br>
Date: Tue, 8 Mar 2022 10:14:51 GMT&nbsp; &nbsp;(809kb,D)<br>
<br>
Title: End-to-end Multiple Instance Learning with Gradient Accumulation<br>
Authors: Axel Andersson, Nadezhda Koriakina, Nata\v{s}a Sladoje and Joakim<br>
&nbsp; Lindblad<br>
Categories: cs.CV cs.LG<br>
\\<br>
&nbsp; Being able to learn on weakly labeled data, and provide interpretability, are<br>
two of the main reasons why attention-based deep multiple instance learning<br>
(ABMIL) methods have become particularly popular for classification of<br>
histopathological images. Such image data usually come in the form of<br>
gigapixel-sized whole-slide-images (WSI) that are cropped into smaller patches<br>
(instances). However, the sheer size of the data makes training of ABMIL models<br>
challenging. All the instances from one WSI cannot be processed at once by<br>
conventional GPUs. Existing solutions compromise training by relying on<br>
pre-trained models, strategic sampling or selection of instances, or<br>
self-supervised learning. We propose a training strategy based on gradient<br>
accumulation that enables direct end-to-end training of ABMIL models without<br>
being limited by GPU memory. We conduct experiments on both QMNIST and<br>
Imagenette to investigate the performance and training time, and compare with<br>
the conventional memory-expensive baseline and a recent sampled-based approach.<br>
This memory-efficient approach, although slower, reaches performance<br>
indistinguishable from the memory-expensive baseline.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03981" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03981&amp;source=gmail&amp;ust=1646963617831000&amp;usg=AOvVaw234VhH1IXBATL0211tyXy7" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03981</a> ,&nbsp; 809kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03984<br>
Date: Tue, 8 Mar 2022 10:18:25 GMT&nbsp; &nbsp;(2071kb,D)<br>
<br>
Title: Attention-Based Lip Audio-Visual Synthesis for Talking Face Generation<br>
&nbsp; in the Wild<br>
Authors: Ganglai Wang, Peng Zhang, Lei Xie, Wei Huang and Yufei Zha<br>
Categories: cs.CV cs.MM<br>
\\<br>
&nbsp; Talking face generation with great practical significance has attracted more<br>
attention in recent audio-visual studies. How to achieve accurate lip<br>
synchronization is a long-standing challenge to be further investigated.<br>
Motivated by xxx, in this paper, an AttnWav2Lip model is proposed by<br>
incorporating spatial attention module and channel attention module into<br>
lip-syncing strategy. Rather than focusing on the unimportant regions of the<br>
face image, the proposed AttnWav2Lip model is able to pay more attention on the<br>
lip region reconstruction. To our limited knowledge, this is the first attempt<br>
to introduce attention mechanism to the scheme of talking face generation. An<br>
extensive experiments have been conducted to evaluate the effectiveness of the<br>
proposed model. Compared to the baseline measured by LSE-D and LSE-C metrics, a<br>
superior performance has been demonstrated on the benchmark lip synthesis<br>
datasets, including LRW, LRS2 and LRS3.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03984" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03984&amp;source=gmail&amp;ust=1646963617831000&amp;usg=AOvVaw3XP7T6uf-DuScVNEq8WPHQ" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03984</a> ,&nbsp; 2071kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03985<br>
Date: Tue, 8 Mar 2022 10:19:35 GMT&nbsp; &nbsp;(7253kb,D)<br>
<br>
Title: SimpleTrack: Rethinking and Improving the JDE Approach for Multi-Object<br>
&nbsp; Tracking<br>
Authors: Jiaxin Li and Yan Ding and Hualiang Wei<br>
Categories: cs.CV<br>
\\<br>
&nbsp; Joint detection and embedding (JDE) based methods usually estimate bounding<br>
boxes and embedding features of objects with a single network in Multi-Object<br>
Tracking (MOT). In the tracking stage, JDE-based methods fuse the target motion<br>
information and appearance information by applying the same rule, which could<br>
fail when the target is briefly lost or blocked. To overcome this problem, we<br>
propose a new association matrix, the Embedding and Giou matrix, which combines<br>
embedding cosine distance and Giou distance of objects. To further improve the<br>
performance of data association, we develop a simple, effective tracker named<br>
SimpleTrack, which designs a bottom-up fusion method for Re-identity and<br>
proposes a new tracking strategy based on our EG matrix. The experimental<br>
results indicate that SimpleTrack has powerful data association capability,<br>
e.g., 61.6 HOTA and 76.3 IDF1 on MOT17. In addition, we apply the EG matrix to<br>
5 different state-of-the-art JDE-based methods and achieve significant<br>
improvements in IDF1, HOTA and IDsw metrics, and increase the tracking speed of<br>
these methods by about 20%.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03985" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03985&amp;source=gmail&amp;ust=1646963617832000&amp;usg=AOvVaw1d0tQKC7qDLLzzduDs2R-e" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03985</a> ,&nbsp; 7253kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03990<br>
Date: Tue, 8 Mar 2022 10:36:55 GMT&nbsp; &nbsp;(2596kb,D)<br>
<br>
Title: Skating-Mixer: Multimodal MLP for Scoring Figure Skating<br>
Authors: Jingfei Xia, Mingchen Zhuge, Tiantian Geng, Shun Fan, Yuantai Wei,<br>
&nbsp; Zhenyu He and Feng Zheng<br>
Categories: cs.CV<br>
\\<br>
&nbsp; Figure skating scoring is a challenging task because it requires judging<br>
players' technical moves as well as coordination with the background music.<br>
Prior learning-based work cannot solve it well for two reasons: 1) each move in<br>
figure skating changes quickly, hence simply applying traditional frame<br>
sampling will lose a lot of valuable information, especially in a 3-5 minutes<br>
lasting video, so an extremely long-range representation learning is necessary;<br>
2) prior methods rarely considered the critical audio-visual relationship in<br>
their models. Thus, we introduce a multimodal MLP architecture, named<br>
Skating-Mixer. It extends the MLP-Mixer-based framework into a multimodal<br>
fashion and effectively learns long-term representations through our designed<br>
memory recurrent unit (MRU). Aside from the model, we also collected a<br>
high-quality audio-visual FS1000 dataset, which contains over 1000 videos on 8<br>
types of programs with 7 different rating metrics, overtaking other datasets in<br>
both quantity and diversity. Experiments show the proposed method outperforms<br>
SOTAs over all major metrics on the public Fis-V and our FS1000 dataset. In<br>
addition, we include an analysis applying our method to recent competitions<br>
that occurred in Beijing 2022 Winter Olympic Games, proving our method has<br>
strong robustness.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03990" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03990&amp;source=gmail&amp;ust=1646963617832000&amp;usg=AOvVaw3E-Pgdps9ED7ZNL_BrmzO4" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03990</a> ,&nbsp; 2596kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03996<br>
Date: Tue, 8 Mar 2022 10:54:00 GMT&nbsp; &nbsp;(10627kb,D)<br>
<br>
Title: DeltaCNN: End-to-End CNN Inference of Sparse Frame Differences in Videos<br>
Authors: Mathias Parger, Chengcheng Tang, Christopher D. Twigg, Cem Keskin,<br>
&nbsp; Robert Wang, Markus Steinberger<br>
Categories: cs.CV cs.LG<br>
Comments: CVPR 2022<br>
\\<br>
&nbsp; Convolutional neural network inference on video data requires powerful<br>
hardware for real-time processing. Given the inherent coherence across<br>
consecutive frames, large parts of a video typically change little. By skipping<br>
identical image regions and truncating insignificant pixel updates,<br>
computational redundancy can in theory be reduced significantly. However, these<br>
theoretical savings have been difficult to translate into practice, as sparse<br>
updates hamper computational consistency and memory access coherence; which are<br>
key for efficiency on real hardware. With DeltaCNN, we present a sparse<br>
convolutional neural network framework that enables sparse frame-by-frame<br>
updates to accelerate video inference in practice. We provide sparse<br>
implementations for all typical CNN layers and propagate sparse feature updates<br>
end-to-end - without accumulating errors over time. DeltaCNN is applicable to<br>
all convolutional neural networks without retraining. To the best of our<br>
knowledge, we are the first to significantly outperform the dense reference,<br>
cuDNN, in practical settings, achieving speedups of up to 7x with only marginal<br>
differences in accuracy.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03996" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03996&amp;source=gmail&amp;ust=1646963617832000&amp;usg=AOvVaw0OppoQu3wYUk8KZKbN9t1w" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03996</a> ,&nbsp; 10627kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04006<br>
Date: Tue, 8 Mar 2022 11:01:24 GMT&nbsp; &nbsp;(4962kb,D)<br>
<br>
Title: Visual-Language Navigation Pretraining via Prompt-based Environmental<br>
&nbsp; Self-exploration<br>
Authors: Xiwen Liang, Fengda Zhu, Lingling Li, Hang Xu, Xiaodan Liang<br>
Categories: cs.CV cs.CL<br>
Comments: Accepted by ACL 2022<br>
\\<br>
&nbsp; Vision-language navigation (VLN) is a challenging task due to its large<br>
searching space in the environment. To address this problem, previous works<br>
have proposed some methods of fine-tuning a large model that pretrained on<br>
large-scale datasets. However, the conventional fine-tuning methods require<br>
extra human-labeled navigation data and lack self-exploration capabilities in<br>
environments, which hinders their generalization of unseen scenes. To improve<br>
the ability of fast cross-domain adaptation, we propose Prompt-based<br>
Environmental Self-exploration (ProbES), which can self-explore the<br>
environments by sampling trajectories and automatically generates structured<br>
instructions via a large-scale cross-modal pretrained model (CLIP). Our method<br>
fully utilizes the knowledge learned from CLIP to build an in-domain dataset by<br>
self-exploration without human labeling. Unlike the conventional approach of<br>
fine-tuning, we introduce prompt-based learning to achieve fast adaptation for<br>
language embeddings, which substantially improves the learning efficiency by<br>
leveraging prior knowledge. By automatically synthesizing<br>
trajectory-instruction pairs in any environment without human supervision and<br>
efficient prompt-based learning, our model can adapt to diverse vision-language<br>
navigation tasks, including VLN and REVERIE. Both qualitative and quantitative<br>
results show that our ProbES significantly improves the generalization ability<br>
of the navigation model.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04006" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04006&amp;source=gmail&amp;ust=1646963617832000&amp;usg=AOvVaw0ZZ4AVX1BOuPw_z95y3sTy" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04006</a> ,&nbsp; 4962kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04007<br>
Date: Tue, 8 Mar 2022 11:01:41 GMT&nbsp; &nbsp;(2030kb,D)<br>
<br>
Title: DuMLP-Pin: A Dual-MLP-dot-product Permutation-invariant Network for Set<br>
&nbsp; Feature Extraction<br>
Authors: Jiajun Fei, Ziyu Zhu, Wenlei Liu, Zhidong Deng, Mingyang Li, Huanjun<br>
&nbsp; Deng, Shuo Zhang<br>
Categories: cs.CV cs.AI<br>
Comments: 16 pages, accepted by AAAI 2022, with technical appendix<br>
\\<br>
&nbsp; Existing permutation-invariant methods can be divided into two categories<br>
according to the aggregation scope, i.e. global aggregation and local one.<br>
Although the global aggregation methods, e. g., PointNet and Deep Sets, get<br>
involved in simpler structures, their performance is poorer than the local<br>
aggregation ones like PointNet++ and Point Transformer. It remains an open<br>
problem whether there exists a global aggregation method with a simple<br>
structure, competitive performance, and even much fewer parameters. In this<br>
paper, we propose a novel global aggregation permutation-invariant network<br>
based on dual MLP dot-product, called DuMLP-Pin, which is capable of being<br>
employed to extract features for set inputs, including unordered or<br>
unstructured pixel, attribute, and point cloud data sets. We strictly prove<br>
that any permutation-invariant function implemented by DuMLP-Pin can be<br>
decomposed into two or more permutation-equivariant ones in a dot-product way<br>
as the cardinality of the given input set is greater than a threshold. We also<br>
show that the DuMLP-Pin can be viewed as Deep Sets with strong constraints<br>
under certain conditions. The performance of DuMLP-Pin is evaluated on several<br>
different tasks with diverse data sets. The experimental results demonstrate<br>
that our DuMLP-Pin achieves the best results on the two classification problems<br>
for pixel sets and attribute sets. On both the point cloud classification and<br>
the part segmentation, the accuracy of DuMLP-Pin is very close to the so-far<br>
best-performing local aggregation method with only a 1-2% difference, while the<br>
number of required parameters is significantly reduced by more than 85% in<br>
classification and 69% in segmentation, respectively. The code is publicly<br>
available on <a href="https://github.com/JaronTHU/DuMLP-Pin" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/JaronTHU/DuMLP-Pin&amp;source=gmail&amp;ust=1646963617833000&amp;usg=AOvVaw0hKZ-NaxTCXDk2hANigbTn" rel="noreferrer" target="_blank">https://github.com/JaronTHU/<wbr>DuMLP-Pin</a>.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04007" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04007&amp;source=gmail&amp;ust=1646963617833000&amp;usg=AOvVaw0Vu-iJPDVPRRLwkGH1ruSg" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04007</a> ,&nbsp; 2030kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04011<br>
Date: Tue, 8 Mar 2022 11:06:01 GMT&nbsp; &nbsp;(4909kb,D)<br>
<br>
Title: Evolutionary Neural Cascade Search across Supernetworks<br>
Authors: Alexander Chebykin, Tanja Alderliesten, Peter A. N. Bosman<br>
Categories: cs.CV cs.NE<br>
Comments: 13 pages, 13 figures<br>
\\<br>
&nbsp; To achieve excellent performance with modern neural networks, having the<br>
right network architecture is important. Neural Architecture Search (NAS)<br>
concerns the automatic discovery of task-specific network architectures. Modern<br>
NAS approaches leverage supernetworks whose subnetworks encode candidate neural<br>
network architectures. These subnetworks can be trained simultaneously,<br>
removing the need to train each network from scratch, thereby increasing the<br>
efficiency of NAS. A recent method called Neural Architecture Transfer (NAT)<br>
further improves the efficiency of NAS for computer vision tasks by using a<br>
multi-objective evolutionary algorithm to find high-quality subnetworks of a<br>
supernetwork pretrained on ImageNet. Building upon NAT, we introduce ENCAS -<br>
Evolutionary Neural Cascade Search. ENCAS can be used to search over multiple<br>
pretrained supernetworks to achieve a trade-off front of cascades of different<br>
neural network architectures, maximizing accuracy while minimizing FLOPS count.<br>
We test ENCAS on common computer vision benchmarks (CIFAR-10, CIFAR-100,<br>
ImageNet) and achieve Pareto dominance over previous state-of-the-art NAS<br>
models up to 1.5 GFLOPS. Additionally, applying ENCAS to a pool of 518 publicly<br>
available ImageNet classifiers leads to Pareto dominance in all computation<br>
regimes and to increasing the maximum accuracy from 88.6% to 89.0%, accompanied<br>
by an 18\% decrease in computation effort from 362 to 296 GFLOPS. Our code is<br>
available at <a href="https://github.com/AwesomeLemon/ENCAS" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/AwesomeLemon/ENCAS&amp;source=gmail&amp;ust=1646963617833000&amp;usg=AOvVaw07HV0P93TYMdurpnbIyU2b" rel="noreferrer" target="_blank">https://github.com/<wbr>AwesomeLemon/ENCAS</a><br>
\\ ( <a href="https://arxiv.org/abs/2203.04011" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04011&amp;source=gmail&amp;ust=1646963617833000&amp;usg=AOvVaw0rRXt6Djk37imDiKISjwiu" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04011</a> ,&nbsp; 4909kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04031<br>
Date: Tue, 8 Mar 2022 11:46:41 GMT&nbsp; &nbsp;(3143kb,D)<br>
<br>
Title: Stage-Aware Feature Alignment Network for Real-Time Semantic<br>
&nbsp; Segmentation of Street Scenes<br>
Authors: Xi Weng, Yan Yan, Si Chen, Jing-Hao Xue, Hanzi Wang<br>
Categories: cs.CV<br>
DOI: 10.1109/TCSVT.2021.3121680<br>
\\<br>
&nbsp; Over the past few years, deep convolutional neural network-based methods have<br>
made great progress in semantic segmentation of street scenes. Some recent<br>
methods align feature maps to alleviate the semantic gap between them and<br>
achieve high segmentation accuracy. However, they usually adopt the feature<br>
alignment modules with the same network configuration in the decoder and thus<br>
ignore the different roles of stages of the decoder during feature aggregation,<br>
leading to a complex decoder structure. Such a manner greatly affects the<br>
inference speed. In this paper, we present a novel Stage-aware Feature<br>
Alignment Network (SFANet) based on the encoder-decoder structure for real-time<br>
semantic segmentation of street scenes. Specifically, a Stage-aware Feature<br>
Alignment module (SFA) is proposed to align and aggregate two adjacent levels<br>
of feature maps effectively. In the SFA, by taking into account the unique role<br>
of each stage in the decoder, a novel stage-aware Feature Enhancement Block<br>
(FEB) is designed to enhance spatial details and contextual information of<br>
feature maps from the encoder. In this way, we are able to address the<br>
misalignment problem with a very simple and efficient multi-branch decoder<br>
structure. Moreover, an auxiliary training strategy is developed to explicitly<br>
alleviate the multi-scale object problem without bringing additional<br>
computational costs during the inference phase. Experimental results show that<br>
the proposed SFANet exhibits a good balance between accuracy and speed for<br>
real-time semantic segmentation of street scenes. In particular, based on<br>
ResNet-18, SFANet respectively obtains 78.1% and 74.7% mean of class-wise<br>
Intersection-over-Union (mIoU) at inference speeds of 37 FPS and 96 FPS on the<br>
challenging Cityscapes and CamVid test datasets by using only a single GTX<br>
1080Ti GPU.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04031" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04031&amp;source=gmail&amp;ust=1646963617833000&amp;usg=AOvVaw2VrH7wc2S5oLk2qFX7ZYLG" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04031</a> ,&nbsp; 3143kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04036<br>
Date: Tue, 8 Mar 2022 12:06:12 GMT&nbsp; &nbsp;(23330kb,D)<br>
<br>
Title: StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via<br>
&nbsp; Pretrained StyleGAN<br>
Authors: Fei Yin and Yong Zhang and Xiaodong Cun and Mingdeng Cao and Yanbo Fan<br>
&nbsp; and Xuan Wang and Qingyan Bai and Baoyuan Wu and Jue Wang and Yujiu Yang<br>
Categories: cs.CV<br>
\\<br>
&nbsp; One-shot talking face generation aims at synthesizing a high-quality talking<br>
face video from an arbitrary portrait image, driven by a video or an audio<br>
segment. One challenging quality factor is the resolution of the output video:<br>
higher resolution conveys more details. In this work, we investigate the latent<br>
feature space of a pre-trained StyleGAN and discover some excellent spatial<br>
transformation properties. Upon the observation, we explore the possibility of<br>
using a pre-trained StyleGAN to break through the resolution limit of training<br>
datasets. We propose a novel unified framework based on a pre-trained StyleGAN<br>
that enables a set of powerful functionalities, i.e., high-resolution video<br>
generation, disentangled control by driving video or audio, and flexible face<br>
editing. Our framework elevates the resolution of the synthesized talking face<br>
to 1024*1024 for the first time, even though the training dataset has a lower<br>
resolution. We design a video-based motion generation module and an audio-based<br>
one, which can be plugged into the framework either individually or jointly to<br>
drive the video generation. The predicted motion is used to transform the<br>
latent features of StyleGAN for visual animation. To compensate for the<br>
transformation distortion, we propose a calibration network as well as a domain<br>
loss to refine the features. Moreover, our framework allows two types of facial<br>
editing, i.e., global editing via GAN inversion and intuitive editing based on<br>
3D morphable models. Comprehensive experiments show superior video quality,<br>
flexible controllability, and editability over state-of-the-art methods.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04036" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04036&amp;source=gmail&amp;ust=1646963617833000&amp;usg=AOvVaw15pkqRYmJffWLCmdO4YdGg" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04036</a> ,&nbsp; 23330kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04037<br>
Date: Tue, 8 Mar 2022 12:07:32 GMT&nbsp; &nbsp;(1971kb,D)<br>
<br>
Title: Deep Multi-Branch Aggregation Network for Real-Time Semantic<br>
&nbsp; Segmentation in Street Scenes<br>
Authors: Xi Weng, Yan Yan, Genshun Dong, Chang Shu, Biao Wang, Hanzi Wang, Ji<br>
&nbsp; Zhang<br>
Categories: cs.CV<br>
DOI: 10.1109/TITS.2022.3150350<br>
\\<br>
&nbsp; Real-time semantic segmentation, which aims to achieve high segmentation<br>
accuracy at real-time inference speed, has received substantial attention over<br>
the past few years. However, many state-of-the-art real-time semantic<br>
segmentation methods tend to sacrifice some spatial details or contextual<br>
information for fast inference, thus leading to degradation in segmentation<br>
quality. In this paper, we propose a novel Deep Multi-branch Aggregation<br>
Network (called DMA-Net) based on the encoder-decoder structure to perform<br>
real-time semantic segmentation in street scenes. Specifically, we first adopt<br>
ResNet-18 as the encoder to efficiently generate various levels of feature maps<br>
from different stages of convolutions. Then, we develop a Multi-branch<br>
Aggregation Network (MAN) as the decoder to effectively aggregate different<br>
levels of feature maps and capture the multi-scale information. In MAN, a<br>
lattice enhanced residual block is designed to enhance feature representations<br>
of the network by taking advantage of the lattice structure. Meanwhile, a<br>
feature transformation block is introduced to explicitly transform the feature<br>
map from the neighboring branch before feature aggregation. Moreover, a global<br>
context block is used to exploit the global contextual information. These key<br>
components are tightly combined and jointly optimized in a unified network.<br>
Extensive experimental results on the challenging Cityscapes and CamVid<br>
datasets demonstrate that our proposed DMA-Net respectively obtains 77.0% and<br>
73.6% mean Intersection over Union (mIoU) at the inference speed of 46.7 FPS<br>
and 119.8 FPS by only using a single NVIDIA GTX 1080Ti GPU. This shows that<br>
DMA-Net provides a good tradeoff between segmentation quality and speed for<br>
semantic segmentation in street scenes.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04037" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04037&amp;source=gmail&amp;ust=1646963617834000&amp;usg=AOvVaw3UoXg6ar0Sgarmtf2TbWzU" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04037</a> ,&nbsp; 1971kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04038<br>
Date: Tue, 8 Mar 2022 12:13:29 GMT&nbsp; &nbsp;(1240kb,D)<br>
<br>
Title: Gait Recognition with Mask-based Regularization<br>
Authors: Chuanfu Shen, Beibei Lin, Shunli Zhang, George Q. Huang, Shiqi Yu, Xin<br>
&nbsp; Yu<br>
Categories: cs.CV<br>
Comments: 14 pages,4 figues<br>
\\<br>
&nbsp; Most gait recognition methods exploit spatial-temporal representations from<br>
static appearances and dynamic walking patterns. However, we observe that many<br>
part-based methods neglect representations at boundaries. In addition, the<br>
phenomenon of overfitting on training data is relatively common in gait<br>
recognition, which is perhaps due to insufficient data and low-informative gait<br>
silhouettes. Motivated by these observations, we propose a novel mask-based<br>
regularization method named ReverseMask. By injecting perturbation on the<br>
feature map, the proposed regularization method helps convolutional<br>
architecture learn the discriminative representations and enhances<br>
generalization. Also, we design an Inception-like ReverseMask Block, which has<br>
three branches composed of a global branch, a feature dropping branch, and a<br>
feature scaling branch. Precisely, the dropping branch can extract fine-grained<br>
representations when partial activations are zero-outed. Meanwhile, the scaling<br>
branch randomly scales the feature map, keeping structural information of<br>
activations and preventing overfitting. The plug-and-play Inception-like<br>
ReverseMask block is simple and effective to generalize networks, and it also<br>
improves the performance of many state-of-the-art methods. Extensive<br>
experiments demonstrate that the ReverseMask regularization help baseline<br>
achieves higher accuracy and better generalization. Moreover, the baseline with<br>
Inception-like Block significantly outperforms state-of-the-art methods on the<br>
two most popular datasets, CASIA-B and OUMVLP. The source code will be<br>
released.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04038" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04038&amp;source=gmail&amp;ust=1646963617834000&amp;usg=AOvVaw01zWAYokicbbfsox0b6MzT" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04038</a> ,&nbsp; 1240kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04041<br>
Date: Tue, 8 Mar 2022 12:21:35 GMT&nbsp; &nbsp;(6475kb,D)<br>
<br>
Title: Shape-invariant 3D Adversarial Point Clouds<br>
Authors: Qidong Huang and Xiaoyi Dong and Dongdong Chen and Hang Zhou and<br>
&nbsp; Weiming Zhang and Nenghai Yu<br>
Categories: cs.CV<br>
Comments: Accepted at CVPR 2022<br>
\\<br>
&nbsp; Adversary and invisibility are two fundamental but conflict characters of<br>
adversarial perturbations. Previous adversarial attacks on 3D point cloud<br>
recognition have often been criticized for their noticeable point outliers,<br>
since they just involve an "implicit constrain" like global distance loss in<br>
the time-consuming optimization to limit the generated noise. While point cloud<br>
is a highly structured data format, it is hard to metric and constrain its<br>
perturbation with a simple loss properly. In this paper, we propose a novel<br>
Point-Cloud Sensitivity Map to boost both the efficiency and imperceptibility<br>
of point perturbations. This map reveals the vulnerability of point cloud<br>
recognition models when encountering shape-invariant adversarial noises. These<br>
noises are designed along the shape surface with an "explicit constrain"<br>
instead of extra distance loss. Specifically, we first apply a reversible<br>
coordinate transformation on each point of the point cloud input, to reduce one<br>
degree of point freedom and limit its movement on the tangent plane. Then we<br>
calculate the best attacking direction with the gradients of the transformed<br>
point cloud obtained on the white-box model. Finally we assign each point with<br>
a non-negative score to construct the sensitivity map, which benefits both<br>
white-box adversarial invisibility and black-box query-efficiency extended in<br>
our work. Extensive evaluations prove that our method can achieve the superior<br>
performance on various point cloud recognition models, with its satisfying<br>
adversarial imperceptibility and strong resistance to different point cloud<br>
defense settings. Our code is available at: <a href="https://github.com/shikiw/SI-Adv" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/shikiw/SI-Adv&amp;source=gmail&amp;ust=1646963617834000&amp;usg=AOvVaw1ZZgALGJ57WoXaKzwUtZd_" rel="noreferrer" target="_blank">https://github.com/shikiw/SI-<wbr>Adv</a>.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04041" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04041&amp;source=gmail&amp;ust=1646963617834000&amp;usg=AOvVaw04IQWj-T7_RLDm9WQJW7FT" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04041</a> ,&nbsp; 6475kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04049<br>
Date: Tue, 8 Mar 2022 12:39:05 GMT&nbsp; &nbsp;(4563kb,D)<br>
<br>
Title: Graph Attention Transformer Network for Multi-Label Image Classification<br>
Authors: Jin Yuan, Shikai Chen, Yao Zhang, Zhongchao Shi, Xin Geng, Jianping<br>
&nbsp; Fan, Yong Rui<br>
Categories: cs.CV<br>
\\<br>
&nbsp; Multi-label classification aims to recognize multiple objects or attributes<br>
from images. However, it is challenging to learn from proper label graphs to<br>
effectively characterize such inter-label correlations or dependencies. Current<br>
methods often use the co-occurrence probability of labels based on the training<br>
set as the adjacency matrix to model this correlation, which is greatly limited<br>
by the dataset and affects the model's generalization ability. In this paper,<br>
we propose a Graph Attention Transformer Network (GATN), a general framework<br>
for multi-label image classification that can effectively mine complex<br>
inter-label relationships. First, we use the cosine similarity based on the<br>
label word embedding as the initial correlation matrix, which can represent<br>
rich semantic information. Subsequently, we design the graph attention<br>
transformer layer to transfer this adjacency matrix to adapt to the current<br>
domain. Our extensive experiments have demonstrated that our proposed methods<br>
can achieve state-of-the-art performance on three datasets.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04049" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04049&amp;source=gmail&amp;ust=1646963617834000&amp;usg=AOvVaw1798zDUbqpMBSDrFAMPtxh" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04049</a> ,&nbsp; 4563kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04050<br>
Date: Tue, 8 Mar 2022 12:39:51 GMT&nbsp; &nbsp;(10956kb,D)<br>
<br>
Title: BEVSegFormer: Bird's Eye View Semantic Segmentation From Arbitrary<br>
&nbsp; Camera Rigs<br>
Authors: Lang Peng, Zhirong Chen, Zhangjie Fu, Pengpeng Liang, Erkang Cheng<br>
Categories: cs.CV<br>
\\<br>
&nbsp; Semantic segmentation in bird's eye view (BEV) is an important task for<br>
autonomous driving. Though this task has attracted a large amount of research<br>
efforts, it is still challenging to flexibly cope with arbitrary (single or<br>
multiple) camera sensors equipped on the autonomous vehicle. In this paper, we<br>
present BEVSegFormer, an effective transformer-based method for BEV semantic<br>
segmentation from arbitrary camera rigs. Specifically, our method first encodes<br>
image features from arbitrary cameras with a shared backbone. These image<br>
features are then enhanced by a deformable transformer-based encoder. Moreover,<br>
we introduce a BEV transformer decoder module to parse BEV semantic<br>
segmentation results. An efficient multi-camera deformable attention unit is<br>
designed to carry out the BEV-to-image view transformation. Finally, the<br>
queries are reshaped according the layout of grids in the BEV, and upsampled to<br>
produce the semantic segmentation result in a supervised manner. We evaluate<br>
the proposed algorithm on the public nuScenes dataset and a self-collected<br>
dataset. Experimental results show that our method achieves promising<br>
performance on BEV semantic segmentation from arbitrary camera rigs. We also<br>
demonstrate the effectiveness of each component via ablation study.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04050" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04050&amp;source=gmail&amp;ust=1646963617834000&amp;usg=AOvVaw0_sxyRksqfxLzRp7ZfKuT8" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04050</a> ,&nbsp; 10956kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04061<br>
Date: Tue, 8 Mar 2022 13:10:17 GMT&nbsp; &nbsp;(11424kb,D)<br>
<br>
Title: Counting with Adaptive Auxiliary Learning<br>
Authors: Yanda Meng, Joshua Bridge, Meng Wei, Yitian Zhao, Yihong Qiao, Xiaoyun<br>
&nbsp; Yang, Xiaowei Huang, Yalin Zheng<br>
Categories: cs.CV<br>
\\<br>
&nbsp; This paper proposes an adaptive auxiliary task learning based approach for<br>
object counting problems. Unlike existing auxiliary task learning based<br>
methods, we develop an attention-enhanced adaptively shared backbone network to<br>
enable both task-shared and task-tailored features learning in an end-to-end<br>
manner. The network seamlessly combines standard Convolution Neural Network<br>
(CNN) and Graph Convolution Network (GCN) for feature extraction and feature<br>
reasoning among different domains of tasks. Our approach gains enriched<br>
contextual information by iteratively and hierarchically fusing the features<br>
across different task branches of the adaptive CNN backbone. The whole<br>
framework pays special attention to the objects' spatial locations and varied<br>
density levels, informed by object (or crowd) segmentation and density level<br>
segmentation auxiliary tasks. In particular, thanks to the proposed dilated<br>
contrastive density loss function, our network benefits from individual and<br>
regional context supervision in terms of pixel-independent and pixel-dependent<br>
feature learning mechanisms, along with strengthened robustness. Experiments on<br>
seven challenging multi-domain datasets demonstrate that our method achieves<br>
superior performance to the state-of-the-art auxiliary task learning based<br>
counting methods. Our code is made publicly available at:<br>
<a href="https://github.com/smallmax00/Counting_With_Adaptive_Auxiliary" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/smallmax00/Counting_With_Adaptive_Auxiliary&amp;source=gmail&amp;ust=1646963617835000&amp;usg=AOvVaw2qSTs6TUwx41c3-JmdhMRO" rel="noreferrer" target="_blank">https://github.com/smallmax00/<wbr>Counting_With_Adaptive_<wbr>Auxiliary</a><br>
\\ ( <a href="https://arxiv.org/abs/2203.04061" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04061&amp;source=gmail&amp;ust=1646963617835000&amp;usg=AOvVaw0mgSX7o_Xgfb3XJrxKSiyP" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04061</a> ,&nbsp; 11424kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04067<br>
Date: Tue, 8 Mar 2022 13:25:35 GMT&nbsp; &nbsp;(3932kb,D)<br>
<br>
Title: Lane Detection with Versatile AtrousFormer and Local Semantic Guidance<br>
Authors: Jiaxing Yang, Lihe Zhang, Huchuan Lu<br>
Categories: cs.CV cs.AI<br>
\\<br>
&nbsp; Lane detection is one of the core functions in autonomous driving and has<br>
aroused widespread attention recently. The networks to segment lane instances,<br>
especially with bad appearance, must be able to explore lane distribution<br>
properties. Most existing methods tend to resort to CNN-based techniques. A few<br>
have a try on incorporating the recent adorable, the seq2seq Transformer<br>
\cite{transformer}. However, their innate drawbacks of weak global information<br>
collection ability and exorbitant computation overhead prohibit a wide range of<br>
the further applications. In this work, we propose Atrous Transformer<br>
(AtrousFormer) to solve the problem. Its variant local AtrousFormer is<br>
interleaved into feature extractor to enhance extraction. Their collecting<br>
information first by rows and then by columns in a dedicated manner finally<br>
equips our network with stronger information gleaning ability and better<br>
computation efficiency. To further improve the performance, we also propose a<br>
local semantic guided decoder to delineate the identities and shapes of lanes<br>
more accurately, in which the predicted Gaussian map of the starting point of<br>
each lane serves to guide the process. Extensive results on three challenging<br>
benchmarks (CULane, TuSimple, and BDD100K) show that our network performs<br>
favorably against the state of the arts.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04067" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04067&amp;source=gmail&amp;ust=1646963617835000&amp;usg=AOvVaw3nBxQ3ULpg99kXIxjz9L-3" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04067</a> ,&nbsp; 3932kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04074<br>
Date: Tue, 8 Mar 2022 13:36:23 GMT&nbsp; &nbsp;(5826kb,D)<br>
<br>
Title: E2EC: An End-to-End Contour-based Method for High-Quality High-Speed<br>
&nbsp; Instance Segmentation<br>
Authors: Tao Zhang, Shiqing Wei, Shunping Ji<br>
Categories: cs.CV cs.AI<br>
Comments: CVPR2022<br>
\\<br>
&nbsp; Contour-based instance segmentation methods have developed rapidly recently<br>
but feature rough and hand-crafted front-end contour initialization, which<br>
restricts the model performance, and an empirical and fixed backend<br>
predicted-label vertex pairing, which contributes to the learning difficulty.<br>
In this paper, we introduce a novel contour-based method, named E2EC, for<br>
high-quality instance segmentation. Firstly, E2EC applies a novel learnable<br>
contour initialization architecture instead of hand-crafted contour<br>
initialization. This consists of a contour initialization module for<br>
constructing more explicit learning goals and a global contour deformation<br>
module for taking advantage of all of the vertices' features better. Secondly,<br>
we propose a novel label sampling scheme, named multi-direction alignment, to<br>
reduce the learning difficulty. Thirdly, to improve the quality of the boundary<br>
details, we dynamically match the most appropriate predicted-ground truth<br>
vertex pairs and propose the corresponding loss function named dynamic matching<br>
loss. The experiments showed that E2EC can achieve a state-of-the-art<br>
performance on the KITTI INStance (KINS) dataset, the Semantic Boundaries<br>
Dataset (SBD), the Cityscapes and the COCO dataset. E2EC is also efficient for<br>
use in real-time applications, with an inference speed of 36 fps for 512*512<br>
images on an NVIDIA A6000 GPU. Code will be released at<br>
<a href="https://github.com/zhang-tao-whu/e2ec" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/zhang-tao-whu/e2ec&amp;source=gmail&amp;ust=1646963617835000&amp;usg=AOvVaw2SRZQA_agF1bA7DAfYW9vV" rel="noreferrer" target="_blank">https://github.com/zhang-tao-<wbr>whu/e2ec</a>.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04074" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04074&amp;source=gmail&amp;ust=1646963617835000&amp;usg=AOvVaw0g3fttsKSPhUQEPicYsEOT" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04074</a> ,&nbsp; 5826kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04076<br>
Date: Tue, 8 Mar 2022 13:40:51 GMT&nbsp; &nbsp;(28236kb,D)<br>
<br>
Title: Semantic Distillation Guided Salient Object Detection<br>
Authors: Bo Xu and Guanze Liu and Han Huang and Cheng Lu and Yandong Guo<br>
Categories: cs.CV cs.CL<br>
Comments: 14 pages, 10 figures<br>
\\<br>
&nbsp; Most existing CNN-based salient object detection methods can identify local<br>
segmentation details like hair and animal fur, but often misinterpret the real<br>
saliency due to the lack of global contextual information caused by the<br>
subjectiveness of the SOD task and the locality of convolution layers.<br>
Moreover, due to the unrealistically expensive labeling costs, the current<br>
existing SOD datasets are insufficient to cover the real data distribution. The<br>
limitation and bias of the training data add additional difficulty to fully<br>
exploring the semantic association between object-to-object and<br>
object-to-environment in a given image. In this paper, we propose a semantic<br>
distillation guided SOD (SDG-SOD) method that produces accurate results by<br>
fusing semantically distilled knowledge from generated image captioning into<br>
the Vision-Transformer-based SOD framework. SDG-SOD can better uncover<br>
inter-objects and object-to-environment saliency and cover the gap between the<br>
subjective nature of SOD and its expensive labeling. Comprehensive experiments<br>
on five benchmark datasets demonstrate that the SDG-SOD outperforms the<br>
state-of-the-art approaches on four evaluation metrics, and largely improves<br>
the model performance on DUTS, ECSSD, DUT, HKU-IS, and PASCAL-S datasets.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04076" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04076&amp;source=gmail&amp;ust=1646963617835000&amp;usg=AOvVaw1fw7GGzcPTLqC0oFwXl6nS" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04076</a> ,&nbsp; 28236kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04095<br>
Date: Tue, 8 Mar 2022 14:02:32 GMT&nbsp; &nbsp;(549kb,D)<br>
<br>
Title: Contrastive Enhancement Using Latent Prototype for Few-Shot Segmentation<br>
Authors: Xiaoyu Zhao, Xiaoqian Chen, Zhiqiang Gong, Wen Yao, Yunyang Zhang,<br>
&nbsp; Xiaohu Zheng<br>
Categories: cs.CV cs.AI<br>
Comments: 18 pages, 4 figures<br>
\\<br>
&nbsp; Few-shot segmentation enables the model to recognize unseen classes with few<br>
annotated examples. Most existing methods adopt prototype learning<br>
architecture, where support prototype vectors are expanded and concatenated<br>
with query features to perform conditional segmentation. However, such<br>
framework potentially focuses more on query features while may neglect the<br>
similarity between support and query features. This paper proposes a<br>
contrastive enhancement approach using latent prototypes to leverage latent<br>
classes and raise the utilization of similarity information between prototype<br>
and query features. Specifically, a latent prototype sampling module is<br>
proposed to generate pseudo-mask and novel prototypes based on features<br>
similarity. The module conveniently conducts end-to-end learning and has no<br>
strong dependence on clustering numbers like cluster-based method. Besides, a<br>
contrastive enhancement module is developed to drive models to provide<br>
different predictions with the same query features. Our method can be used as<br>
an auxiliary module to flexibly integrate into other baselines for a better<br>
segmentation performance. Extensive experiments show our approach remarkably<br>
improves the performance of state-of-the-art methods for 1-shot and 5-shot<br>
segmentation, especially outperforming baseline by 5.9% and 7.3% for 5-shot<br>
task on Pascal-5^i and COCO-20^i. Source code is available at<br>
<a href="https://github.com/zhaoxiaoyu1995/CELP-Pytorch" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/zhaoxiaoyu1995/CELP-Pytorch&amp;source=gmail&amp;ust=1646963617835000&amp;usg=AOvVaw3w9oiBG9OcBTkUgMA6FO3Y" rel="noreferrer" target="_blank">https://github.com/<wbr>zhaoxiaoyu1995/CELP-Pytorch</a><br>
\\ ( <a href="https://arxiv.org/abs/2203.04095" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04095&amp;source=gmail&amp;ust=1646963617835000&amp;usg=AOvVaw125SDPeSsOkDcRGoG17OG7" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04095</a> ,&nbsp; 549kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04109<br>
Date: Mon, 7 Mar 2022 12:21:06 GMT&nbsp; &nbsp;(17487kb,D)<br>
<br>
Title: Explaining Classifiers by Constructing Familiar Concepts<br>
Authors: Johannes Schneider and Michail Vlachos<br>
Categories: cs.CV cs.LG<br>
Comments: This paper is a journal version of the conference paper<br>
&nbsp; arXiv:2005.13630 . It adds about 60% new material. It was accepted at Machine<br>
&nbsp; Learning (Springer Journal) in March 2022<br>
\\<br>
&nbsp; Interpreting a large number of neurons in deep learning is difficult. Our<br>
proposed `CLAssifier-DECoder' architecture (ClaDec) facilitates the<br>
understanding of the output of an arbitrary layer of neurons or subsets<br>
thereof. It uses a decoder that transforms the incomprehensible representation<br>
of the given neurons to a representation that is more similar to the domain a<br>
human is familiar with. In an image recognition problem, one can recognize what<br>
information (or concepts) a layer maintains by contrasting reconstructed images<br>
of ClaDec with those of a conventional auto-encoder(AE) serving as reference.<br>
An extension of ClaDec allows trading comprehensibility and fidelity. We<br>
evaluate our approach for image classification using convolutional neural<br>
networks. We show that reconstructed visualizations using encodings from a<br>
classifier capture more relevant classification information than conventional<br>
AEs. This holds although AEs contain more information on the original input.<br>
Our user study highlights that even non-experts can identify a diverse set of<br>
concepts contained in images that are relevant (or irrelevant) for the<br>
classifier. We also compare against saliency based methods that focus on pixel<br>
relevance rather than concepts. We show that ClaDec tends to highlight more<br>
relevant input areas to classification though outcomes depend on classifier<br>
architecture. Code is at \url{<a href="https://github.com/JohnTailor/ClaDec" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/JohnTailor/ClaDec&amp;source=gmail&amp;ust=1646963617836000&amp;usg=AOvVaw28i1mU9PU_4itG6aAd30G8" rel="noreferrer" target="_blank">https://github.com/<wbr>JohnTailor/ClaDec</a>}<br>
\\ ( <a href="https://arxiv.org/abs/2203.04109" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04109&amp;source=gmail&amp;ust=1646963617836000&amp;usg=AOvVaw3ZUgX-1bafvpxxjmplmUt0" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04109</a> ,&nbsp; 17487kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04113<br>
Date: Tue, 8 Mar 2022 14:35:46 GMT&nbsp; &nbsp;(2602kb,D)<br>
<br>
Title: Quantification of Occlusion Handling Capability of a 3D Human Pose<br>
&nbsp; Estimation Framework<br>
Authors: Mehwish Ghafoor, Arif Mahmood<br>
Categories: cs.CV<br>
Comments: Accepted for publication in IEEE Transaction Multimedia, 2022<br>
\\<br>
&nbsp; 3D human pose estimation using monocular images is an important yet<br>
challenging task. Existing 3D pose detection methods exhibit excellent<br>
performance under normal conditions however their performance may degrade due<br>
to occlusion. Recently some occlusion aware methods have also been proposed,<br>
however, the occlusion handling capability of these networks has not yet been<br>
thoroughly investigated. In the current work, we propose an occlusion-guided 3D<br>
human pose estimation framework and quantify its occlusion handling capability<br>
by using different protocols. The proposed method estimates more accurate 3D<br>
human poses using 2D skeletons with missing joints as input. Missing joints are<br>
handled by introducing occlusion guidance that provides extra information about<br>
the absence or presence of a joint. Temporal information has also been<br>
exploited to better estimate the missing joints. A large number of experiments<br>
are performed for the quantification of occlusion handling capability of the<br>
proposed method on three publicly available datasets in various settings<br>
including random missing joints, fixed body parts missing, and complete frames<br>
missing, using mean per joint position error criterion. In addition to that,<br>
the quality of the predicted 3D poses is also evaluated using action<br>
classification performance as a criterion. 3D poses estimated by the proposed<br>
method achieved significantly improved action recognition performance in the<br>
presence of missing joints. Our experiments demonstrate the effectiveness of<br>
the proposed framework for handling the missing joints as well as<br>
quantification of the occlusion handling capability of the deep neural<br>
networks.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04113" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04113&amp;source=gmail&amp;ust=1646963617836000&amp;usg=AOvVaw2giMg9Ko1rgi5wlAJF2FG0" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04113</a> ,&nbsp; 2602kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04121<br>
Date: Sun, 6 Mar 2022 14:26:25 GMT&nbsp; &nbsp;(22881kb,D)<br>
<br>
Title: Few Shot Generative Model Adaption via Relaxed Spatial Structural<br>
&nbsp; Alignment<br>
Authors: Jiayu Xiao, Liang Li, Chaofei Wang, Zheng-Jun Zha, Qingming Huang<br>
Categories: cs.CV<br>
\\<br>
&nbsp; Training a generative adversarial network (GAN) with limited data has been a<br>
challenging task. A feasible solution is to start with a GAN well-trained on a<br>
large scale source domain and adapt it to the target domain with a few samples,<br>
termed as few shot generative model adaption. However, existing methods are<br>
prone to model overfitting and collapse in extremely few shot setting (less<br>
than 10). To solve this problem, we propose a relaxed spatial structural<br>
alignment method to calibrate the target generative models during the adaption.<br>
We design a cross-domain spatial structural consistency loss comprising the<br>
self-correlation and disturbance correlation consistency loss. It helps align<br>
the spatial structural information between the synthesis image pairs of the<br>
source and target domains. To relax the cross-domain alignment, we compress the<br>
original latent space of generative models to a subspace. Image pairs generated<br>
from the subspace are pulled closer. Qualitative and quantitative experiments<br>
show that our method consistently surpasses the state-of-the-art methods in few<br>
shot setting.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04121" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04121&amp;source=gmail&amp;ust=1646963617836000&amp;usg=AOvVaw18XK_guvp3e6tMuppc1jZC" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04121</a> ,&nbsp; 22881kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04129<br>
Date: Tue, 8 Mar 2022 14:55:10 GMT&nbsp; &nbsp;(1787kb,D)<br>
<br>
Title: YouTube-GDD: A challenging gun detection dataset with rich contextual<br>
&nbsp; information<br>
Authors: Yongxiang Gu, Xingbin Liao and Xiaolin Qin<br>
Categories: cs.CV cs.AI<br>
Comments: 4 pages, 1 figure<br>
\\<br>
&nbsp; An automatic gun detection system can detect potential gun-related violence<br>
at an early stage that is of paramount importance for citizens security. In the<br>
whole system, object detection algorithm is the key to perceive the environment<br>
so that the system can detect dangerous objects such as pistols and rifles.<br>
However, mainstream deep learning-based object detection algorithms depend<br>
heavily on large-scale high-quality annotated samples, and the existing gun<br>
datasets are characterized by low resolution, little contextual information and<br>
little data volume. To promote the development of security, this work presents<br>
a new challenging dataset called YouTube Gun Detection Dataset (YouTube-GDD).<br>
Our dataset is collected from 343 high-definition YouTube videos and contains<br>
5000 well-chosen images, in which 16064 instances of gun and 9046 instances of<br>
person are annotated. Compared to other datasets, YouTube-GDD is "dynamic",<br>
containing rich contextual information and recording shape changes of the gun<br>
during shooting. To build a baseline for gun detection, we evaluate YOLOv5 on<br>
YouTube-GDD and analyze the influence of additional related annotated<br>
information on gun detection. YouTube-GDD and subsequent updates will be<br>
released at <a href="https://github.com/UCAS-GYX/YouTube-GDD" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/UCAS-GYX/YouTube-GDD&amp;source=gmail&amp;ust=1646963617836000&amp;usg=AOvVaw1fDlAlSHKzoCi6sH48bgd9" rel="noreferrer" target="_blank">https://github.com/UCAS-GYX/<wbr>YouTube-GDD</a>.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04129" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04129&amp;source=gmail&amp;ust=1646963617836000&amp;usg=AOvVaw18EMi_lGqn26j1hxgufVOx" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04129</a> ,&nbsp; 1787kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04130<br>
Date: Tue, 8 Mar 2022 14:56:16 GMT&nbsp; &nbsp;(32783kb,D)<br>
<br>
Title: NeReF: Neural Refractive Field for Fluid Surface Reconstruction and<br>
&nbsp; Implicit Representation<br>
Authors: Ziyu Wang, Wei Yang, Junming Cao, Lan Xu, Junqing Yu, Jingyi Yu<br>
Categories: cs.CV<br>
\\<br>
&nbsp; Existing neural reconstruction schemes such as Neural Radiance Field (NeRF)<br>
are largely focused on modeling opaque objects. We present a novel neural<br>
refractive field(NeReF) to recover wavefront of transparent fluids by<br>
simultaneously estimating the surface position and normal of the fluid front.<br>
Unlike prior arts that treat the reconstruction target as a single layer of the<br>
surface, NeReF is specifically formulated to recover a volumetric normal field<br>
with its corresponding density field. A query ray will be refracted by NeReF<br>
according to its accumulated refractive point and normal, and we employ the<br>
correspondences and uniqueness of refracted ray for NeReF optimization. We show<br>
NeReF, as a global optimization scheme, can more robustly tackle refraction<br>
distortions detrimental to traditional methods for correspondence matching.<br>
Furthermore, the continuous NeReF representation of wavefront enables view<br>
synthesis as well as normal integration. We validate our approach on both<br>
synthetic and real data and show it is particularly suitable for sparse<br>
multi-view acquisition. We hence build a small light field array and experiment<br>
on various surface shapes to demonstrate high fidelity NeReF reconstruction.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04130" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04130&amp;source=gmail&amp;ust=1646963617837000&amp;usg=AOvVaw3E8lPAoBdSdTkGFe6d_Arq" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04130</a> ,&nbsp; 32783kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04132<br>
Date: Tue, 8 Mar 2022 14:58:41 GMT&nbsp; &nbsp;(3534kb,D)<br>
<br>
Title: Motron: Multimodal Probabilistic Human Motion Forecasting<br>
Authors: Tim Salzmann, Marco Pavone, Markus Ryll<br>
Categories: cs.CV cs.HC cs.LG cs.RO<br>
Comments: CVPR 2022<br>
\\<br>
&nbsp; Autonomous systems and humans are increasingly sharing the same space. Robots<br>
work side by side or even hand in hand with humans to balance each other's<br>
limitations. Such cooperative interactions are ever more sophisticated. Thus,<br>
the ability to reason not just about a human's center of gravity position, but<br>
also its granular motion is an important prerequisite for human-robot<br>
interaction. Though, many algorithms ignore the multimodal nature of humans or<br>
neglect uncertainty in their motion forecasts. We present Motron, a multimodal,<br>
probabilistic, graph-structured model, that captures human's multimodality<br>
using probabilistic methods while being able to output deterministic motions<br>
and corresponding confidence values for each mode. Our model aims to be tightly<br>
integrated with the robotic planning-control-interaction loop; outputting<br>
physically feasible human motions and being computationally efficient. We<br>
demonstrate the performance of our model on several challenging real-world<br>
motion forecasting datasets, outperforming a wide array of generative methods<br>
while providing state-of-the-art deterministic motions if required. Both using<br>
significantly less computational power than state-of-the art algorithms.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04132" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04132&amp;source=gmail&amp;ust=1646963617837000&amp;usg=AOvVaw0VxYTP3hlhzwsvDf9QLUGM" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04132</a> ,&nbsp; 3534kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04153<br>
Date: Tue, 8 Mar 2022 15:30:32 GMT&nbsp; &nbsp;(414kb,D)<br>
<br>
Title: Easy Ensemble: Simple Deep Ensemble Learning for Sensor-Based Human<br>
&nbsp; Activity Recognition<br>
Authors: Tatsuhito Hasegawa, Kazuma Kondo<br>
Categories: cs.CV<br>
Comments: 15 pages, 11 figures, this paper is a pre-print to submit to IEEE<br>
&nbsp; Internet of Things journal<br>
\\<br>
&nbsp; Sensor-based human activity recognition (HAR) is a paramount technology in<br>
the Internet of Things services. HAR using representation learning, which<br>
automatically learns a feature representation from raw data, is the mainstream<br>
method because it is difficult to interpret relevant information from raw<br>
sensor data to design meaningful features. Ensemble learning is a robust<br>
approach to improve generalization performance; however, deep ensemble learning<br>
requires various procedures, such as data partitioning and training multiple<br>
models, which are time-consuming and computationally expensive. In this study,<br>
we propose Easy Ensemble (EE) for HAR, which enables the easy implementation of<br>
deep ensemble learning in a single model. In addition, we propose input masking<br>
as a method for diversifying the input for EE. Experiments on a benchmark<br>
dataset for HAR demonstrated the effectiveness of EE and input masking and<br>
their characteristics compared with conventional ensemble learning methods.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04153" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04153&amp;source=gmail&amp;ust=1646963617837000&amp;usg=AOvVaw2mYAcLVR3S99tmwoCjAIvS" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04153</a> ,&nbsp; 414kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04156<br>
Date: Tue, 8 Mar 2022 15:37:44 GMT&nbsp; &nbsp;(45013kb,D)<br>
<br>
Title: Robust Local Preserving and Global Aligning Network for Adversarial<br>
&nbsp; Domain Adaptation<br>
Authors: Wenwen Qiang, Jiangmeng Li, Changwen Zheng, Bing Su, Hui Xiong<br>
Categories: cs.CV<br>
Comments: Accepted by IEEE Transactions on Knowledge and Data Engineering<br>
&nbsp; (TKDE) 2022; Refer to <a href="https://ieeexplore.ieee.org/document/9540279" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://ieeexplore.ieee.org/document/9540279&amp;source=gmail&amp;ust=1646963617837000&amp;usg=AOvVaw2RQLmfFlWsK4V6Xkh7TB_6" rel="noreferrer" target="_blank">https://ieeexplore.ieee.org/<wbr>document/9540279</a><br>
DOI: 10.1109/TKDE.2021.3112815<br>
\\<br>
&nbsp; Unsupervised domain adaptation (UDA) requires source domain samples with<br>
clean ground truth labels during training. Accurately labeling a large number<br>
of source domain samples is time-consuming and laborious. An alternative is to<br>
utilize samples with noisy labels for training. However, training with noisy<br>
labels can greatly reduce the performance of UDA. In this paper, we address the<br>
problem that learning UDA models only with access to noisy labels and propose a<br>
novel method called robust local preserving and global aligning network<br>
(RLPGA). RLPGA improves the robustness of the label noise from two aspects. One<br>
is learning a classifier by a robust informative-theoretic-based loss function.<br>
The other is constructing two adjacency weight matrices and two negative weight<br>
matrices by the proposed local preserving module to preserve the local topology<br>
structures of input data. We conduct theoretical analysis on the robustness of<br>
the proposed RLPGA and prove that the robust informative-theoretic-based loss<br>
and the local preserving module are beneficial to reduce the empirical risk of<br>
the target domain. A series of empirical studies show the effectiveness of our<br>
proposed RLPGA.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04156" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04156&amp;source=gmail&amp;ust=1646963617837000&amp;usg=AOvVaw2naFw6z20KJoVYdDDO8-fU" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04156</a> ,&nbsp; 45013kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04181<br>
Date: Tue, 8 Mar 2022 16:12:08 GMT&nbsp; &nbsp;(6652kb,D)<br>
<br>
Title: Selective-Supervised Contrastive Learning with Noisy Labels<br>
Authors: Shikun Li, Xiaobo Xia, Shiming Ge, Tongliang Liu<br>
Categories: cs.CV cs.AI cs.LG<br>
Comments: Accepted to CVPR 2022. 12 pages, 5 figure, and 10 tables<br>
\\<br>
&nbsp; Deep networks have strong capacities of embedding data into latent<br>
representations and finishing following tasks. However, the capacities largely<br>
come from high-quality annotated labels, which are expensive to collect. Noisy<br>
labels are more affordable, but result in corrupted representations, leading to<br>
poor generalization performance. To learn robust representations and handle<br>
noisy labels, we propose selective-supervised contrastive learning (Sel-CL) in<br>
this paper. Specifically, Sel-CL extend supervised contrastive learning<br>
(Sup-CL), which is powerful in representation learning, but is degraded when<br>
there are noisy labels. Sel-CL tackles the direct cause of the problem of<br>
Sup-CL. That is, as Sup-CL works in a \textit{pair-wise} manner, noisy pairs<br>
built by noisy labels mislead representation learning. To alleviate the issue,<br>
we select confident pairs out of noisy ones for Sup-CL without knowing noise<br>
rates. In the selection process, by measuring the agreement between learned<br>
representations and given labels, we first identify confident examples that are<br>
exploited to build confident pairs. Then, the representation similarity<br>
distribution in the built confident pairs is exploited to identify more<br>
confident pairs out of noisy pairs. All obtained confident pairs are finally<br>
used for Sup-CL to enhance representations. Experiments on multiple noisy<br>
datasets demonstrate the robustness of the learned representations by our<br>
method, following the state-of-the-art performance. Source codes are available<br>
at <a href="https://github.com/ShikunLi/Sel-CL" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/ShikunLi/Sel-CL&amp;source=gmail&amp;ust=1646963617837000&amp;usg=AOvVaw22a7hV2GjuQ_CAor_q2vPR" rel="noreferrer" target="_blank">https://github.com/ShikunLi/<wbr>Sel-CL</a><br>
\\ ( <a href="https://arxiv.org/abs/2203.04181" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04181&amp;source=gmail&amp;ust=1646963617837000&amp;usg=AOvVaw3Y0Jg9c2X1wPil9sJW9JkE" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04181</a> ,&nbsp; 6652kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04187<br>
Date: Tue, 8 Mar 2022 16:25:30 GMT&nbsp; &nbsp;(3125kb,D)<br>
<br>
Title: MLSeg: Image and Video Segmentation as Multi-Label Classification and<br>
&nbsp; Selected-Label Pixel Classification<br>
Authors: Haodi He and Yuhui Yuan and Xiangyu Yue and Han Hu<br>
Categories: cs.CV<br>
\\<br>
&nbsp; For a long period of time, research studies on segmentation have typically<br>
formulated the task as pixel classification that predicts a class for each<br>
pixel from a set of predefined, fixed number of semantic categories. Yet<br>
standard architectures following this formulation will inevitably encounter<br>
various challenges under more realistic settings where the total number of<br>
semantic categories scales up (e.g., beyond $1\rm{k}$ classes). On the other<br>
hand, a standard image or video usually contains only a small number of<br>
semantic categories from the entire label set. Motivated by this intuition, in<br>
this paper, we propose to decompose segmentation into two sub-problems: (i)<br>
image-level or video-level multi-label classification and (ii) pixel-level<br>
selected-label classification. Given an input image or video, our framework<br>
first conducts multi-label classification over the large complete label set and<br>
selects a small set of labels according to the class confidence scores. Then<br>
the follow-up pixel-wise classification is only performed among the selected<br>
subset of labels. Our approach is conceptually general and can be applied to<br>
various existing segmentation frameworks by simply adding a lightweight<br>
multi-label classification branch. We demonstrate the effectiveness of our<br>
framework with competitive experimental results across four tasks including<br>
image semantic segmentation, image panoptic segmentation, video instance<br>
segmentation, and video semantic segmentation. Especially, with our MLSeg,<br>
Mask$2$Former gains +$0.8\%$/+$0.7\%$/+$0.7\%$ on ADE$20$K panoptic<br>
segmentation/YouTubeVIS $2019$ video instance segmentation/VSPW video semantic<br>
segmentation benchmarks respectively. Code will be available<br>
at:<a href="https://github.com/openseg-group/MLSeg" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/openseg-group/MLSeg&amp;source=gmail&amp;ust=1646963617838000&amp;usg=AOvVaw20QbLax_0XkmsLjWgziAaG" rel="noreferrer" target="_blank">https://github.com/openseg-<wbr>group/MLSeg</a><br>
\\ ( <a href="https://arxiv.org/abs/2203.04187" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04187&amp;source=gmail&amp;ust=1646963617838000&amp;usg=AOvVaw2WAr6DO-Efc5t1JT02U8lU" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04187</a> ,&nbsp; 3125kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04203<br>
Date: Tue, 8 Mar 2022 17:07:09 GMT&nbsp; &nbsp;(18676kb,D)<br>
<br>
Title: AssistQ: Affordance-centric Question-driven Task Completion for<br>
&nbsp; Egocentric Assistant<br>
Authors: Benita Wong, Joya Chen, You Wu, Stan Weixian Lei, Dongxing Mao, Difei<br>
&nbsp; Gao, Mike Zheng Shou<br>
Categories: cs.CV<br>
\\<br>
&nbsp; A long-standing goal of intelligent assistants such as AR glasses/robots has<br>
been to assist users in affordance-centric real-world scenarios, such as "how<br>
can I run the microwave for 1 minute?". However, there is still no clear task<br>
definition and suitable benchmarks. In this paper, we define a new task called<br>
Affordance-centric Question-driven Task Completion, where the AI assistant<br>
should learn from instructional videos and scripts to guide the user<br>
step-by-step. To support the task, we constructed AssistQ, a new dataset<br>
comprising 529 question-answer samples derived from 100 newly filmed<br>
first-person videos. Each question should be completed with multi-step<br>
guidances by inferring from visual details (e.g., buttons' position) and<br>
textural details (e.g., actions like press/turn). To address this unique task,<br>
we developed a Question-to-Actions (Q2A) model that significantly outperforms<br>
several baseline methods while still having large room for improvement. We<br>
expect our task and dataset to advance Egocentric AI Assistant's development.<br>
Our project page is available at: <a href="https://showlab.github.io/assistq" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://showlab.github.io/assistq&amp;source=gmail&amp;ust=1646963617838000&amp;usg=AOvVaw20ryDqwvDxln1UDgwaGZTJ" rel="noreferrer" target="_blank">https://showlab.github.io/<wbr>assistq</a><br>
\\ ( <a href="https://arxiv.org/abs/2203.04203" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04203&amp;source=gmail&amp;ust=1646963617838000&amp;usg=AOvVaw07G5nVfeNCqITTFrGgFjsQ" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04203</a> ,&nbsp; 18676kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04206<br>
Date: Tue, 8 Mar 2022 17:11:36 GMT&nbsp; &nbsp;(1081kb,D)<br>
<br>
Title: Lightweight Monocular Depth Estimation through Guided Decoding<br>
Authors: Michael Rudolph, Youssef Dawoud, Ronja G\"uldenring, Lazaros<br>
&nbsp; Nalpantidis, Vasileios Belagiannis<br>
Categories: cs.CV cs.RO<br>
Comments: Accepted to ICRA 2022<br>
\\<br>
&nbsp; We present a lightweight encoder-decoder archi- tecture for monocular depth<br>
estimation, specifically designed for embedded platforms. Our main contribution<br>
is the Guided Upsampling Block (GUB) for building the decoder of our model.<br>
Motivated by the concept of guided image filtering, GUB relies on the image to<br>
guide the decoder on upsampling the feature representation and the depth map<br>
reconstruction, achieving high resolution results with fine-grained details.<br>
Based on multiple GUBs, our model outperforms the related methods on the NYU<br>
Depth V2 dataset in terms of accuracy while delivering up to 35.1 fps on the<br>
NVIDIA Jetson Nano and up to 144.5 fps on the NVIDIA Xavier NX. Similarly, on<br>
the KITTI dataset, inference is possible with up to 23.7 fps on the Jetson Nano<br>
and 102.9 fps on the Xavier NX. Our code and models are made publicly<br>
available.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04206" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04206&amp;source=gmail&amp;ust=1646963617838000&amp;usg=AOvVaw3CPuhlwfFVGB_fG1-hw0Pr" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04206</a> ,&nbsp; 1081kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04221<br>
Date: Tue, 8 Mar 2022 17:44:35 GMT&nbsp; &nbsp;(36453kb,D)<br>
<br>
Title: Towards Universal Texture Synthesis by Combining Texton Broadcasting<br>
&nbsp; with Noise Injection in StyleGAN-2<br>
Authors: Jue Lin, Gaurav Sharma, Thrasyvoulos N. Pappas<br>
Categories: cs.CV<br>
\\<br>
&nbsp; We present a new approach for universal texture synthesis by incorporating a<br>
multi-scale texton broadcasting module in the StyleGAN-2 framework. The texton<br>
broadcasting module introduces an inductive bias, enabling generation of<br>
broader range of textures, from those with regular structures to completely<br>
stochastic ones. To train and evaluate the proposed approach, we construct a<br>
comprehensive high-resolution dataset that captures the diversity of natural<br>
textures as well as stochastic variations within each perceptually uniform<br>
texture. Experimental results demonstrate that the proposed approach yields<br>
significantly better quality textures than the state of the art. The ultimate<br>
goal of this work is a comprehensive understanding of texture space.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04221" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04221&amp;source=gmail&amp;ust=1646963617838000&amp;usg=AOvVaw10M9D9rswuJXQoy6LNDktK" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04221</a> ,&nbsp; 36453kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04229<br>
Date: Tue, 8 Mar 2022 17:47:51 GMT&nbsp; &nbsp;(1433kb,D)<br>
<br>
Title: Neural Face Identification in a 2D Wireframe Projection of a Manifold<br>
&nbsp; Object<br>
Authors: Kehan Wang and Jia Zheng and Zihan Zhou<br>
Categories: cs.CV<br>
Comments: To Appear in CVPR 2022. The project page is at<br>
&nbsp; <a href="https://manycore-research.github.io/faceformer" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://manycore-research.github.io/faceformer&amp;source=gmail&amp;ust=1646963617838000&amp;usg=AOvVaw3__Dykf9sqHpoNoQLiIgCx" rel="noreferrer" target="_blank">https://manycore-research.<wbr>github.io/faceformer</a><br>
\\<br>
&nbsp; In computer-aided design (CAD) systems, 2D line drawings are commonly used to<br>
illustrate 3D object designs. To reconstruct the 3D models depicted by a single<br>
2D line drawing, an important key is finding the edge loops in the line drawing<br>
which correspond to the actual faces of the 3D object. In this paper, we<br>
approach the classical problem of face identification from a novel data-driven<br>
point of view. We cast it as a sequence generation problem: starting from an<br>
arbitrary edge, we adopt a variant of the popular Transformer model to predict<br>
the edges associated with the same face in a natural order. This allows us to<br>
avoid searching the space of all possible edge loops with various hand-crafted<br>
rules and heuristics as most existing methods do, deal with challenging cases<br>
such as curved surfaces and nested edge loops, and leverage additional cues<br>
such as face types. We further discuss how possibly imperfect predictions can<br>
be used for 3D object reconstruction.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04229" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04229&amp;source=gmail&amp;ust=1646963617838000&amp;usg=AOvVaw2OuXp8sZFVN4wcudQ805Jp" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04229</a> ,&nbsp; 1433kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04232<br>
Date: Tue, 8 Mar 2022 17:49:07 GMT&nbsp; &nbsp;(803kb,D)<br>
<br>
Title: A Lightweight and Detector-free 3D Single Object Tracker on Point Clouds<br>
Authors: Yan Xia, Qiangqiang Wu, Tianyu Yang, Wei Li, Antoni B. Chan, Uwe<br>
&nbsp; Stilla<br>
Categories: cs.CV<br>
Comments: 12 pages, 5 figures, 6 tables<br>
\\<br>
&nbsp; Recent works on 3D single object tracking treat the tracking as a<br>
target-specific 3D detection task, where an off-the-shelf 3D detector is<br>
commonly employed for tracking. However, it is non-trivial to perform accurate<br>
target-specific detection since the point cloud of objects in raw LiDAR scans<br>
is usually sparse and incomplete. In this paper, we address this issue by<br>
explicitly leveraging temporal motion cues and propose DMT, a Detector-free<br>
Motion prediction based 3D Tracking network that totally removes the usage of<br>
complicated 3D detectors, which is lighter, faster, and more accurate than<br>
previous trackers. Specifically, the motion prediction module is firstly<br>
introduced to estimate a potential target center of the current frame in a<br>
point-cloud free way. Then, an explicit voting module is proposed to directly<br>
regress the 3D box from the estimated target center. Extensive experiments on<br>
KITTI and NuScenes datasets demonstrate that our DMT, without applying any<br>
complicated 3D detectors, can still achieve better performance (~10%<br>
improvement on the NuScenes dataset) and faster tracking speed (i.e., 72 FPS)<br>
than state-of-the-art approaches. Our codes will be released publicly.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04232" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04232&amp;source=gmail&amp;ust=1646963617839000&amp;usg=AOvVaw1Ad5CoJD495Lke9PaajLeA" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04232</a> ,&nbsp; 803kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04251<br>
Date: Tue, 8 Mar 2022 18:11:25 GMT&nbsp; &nbsp;(1918kb,D)<br>
<br>
Title: End-to-End Semi-Supervised Learning for Video Action Detection<br>
Authors: Akash Kumar and Yogesh Singh Rawat<br>
Categories: cs.CV<br>
Comments: CVPR'22<br>
\\<br>
&nbsp; In this work, we focus on semi-supervised learning for video action detection<br>
which utilizes both labeled as well as unlabeled data. We propose a simple<br>
end-to-end consistency based approach which effectively utilizes the unlabeled<br>
data. Video action detection requires both, action class prediction as well as<br>
a spatio-temporal localization of actions. Therefore, we investigate two types<br>
of constraints, classification consistency, and spatio-temporal consistency.<br>
The presence of predominant background and static regions in a video makes it<br>
challenging to utilize spatio-temporal consistency for action detection. To<br>
address this, we propose two novel regularization constraints for<br>
spatio-temporal consistency; 1) temporal coherency, and 2) gradient smoothness.<br>
Both these aspects exploit the temporal continuity of action in videos and are<br>
found to be effective for utilizing unlabeled videos for action detection. We<br>
demonstrate the effectiveness of the proposed approach on two different action<br>
detection benchmark datasets, UCF101-24 and JHMDB-21. In addition, we also show<br>
the effectiveness of the proposed approach for video object segmentation on the<br>
Youtube-VOS dataset which demonstrates its generalization capability to other<br>
tasks. The proposed approach achieves competitive performance by using merely<br>
20% of annotations on UCF101-24 when compared with recent fully supervised<br>
methods. On UCF101-24, it improves the score by +8.9% and +11% at 0.5 f-mAP and<br>
v-mAP respectively, compared to supervised approach.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04251" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04251&amp;source=gmail&amp;ust=1646963617839000&amp;usg=AOvVaw0SXje3Brp89CxuRZy0i57P" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04251</a> ,&nbsp; 1918kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04275<br>
Date: Tue, 8 Mar 2022 18:49:34 GMT&nbsp; &nbsp;(7820kb,D)<br>
<br>
Title: Robust Multi-Task Learning and Online Refinement for Spacecraft Pose<br>
&nbsp; Estimation across Domain Gap<br>
Authors: Tae Ha Park and Simone D'Amico<br>
Categories: cs.CV cs.LG<br>
\\<br>
&nbsp; This work presents Spacecraft Pose Network v2 (SPNv2), a Convolutional Neural<br>
Network (CNN) for pose estimation of noncooperative spacecraft across domain<br>
gap. SPNv2 is a multi-scale, multi-task CNN which consists of a shared<br>
multi-scale feature encoder and multiple prediction heads that perform<br>
different tasks on a shared feature output. These tasks are all related to<br>
detection and pose estimation of a target spacecraft from an image, such as<br>
prediction of pre-defined satellite keypoints, direct pose regression, and<br>
binary segmentation of the satellite foreground. It is shown that by jointly<br>
training on different yet related tasks with extensive data augmentations on<br>
synthetic images only, the shared encoder learns features that are common<br>
across image domains that have fundamentally different visual characteristics<br>
compared to synthetic images. This work also introduces Online Domain<br>
Refinement (ODR) which refines the parameters of the normalization layers of<br>
SPNv2 on the target domain images online at deployment. Specifically, ODR<br>
performs self-supervised entropy minimization of the predicted satellite<br>
foreground, thereby improving the CNN's performance on the target domain images<br>
without their pose labels and with minimal computational efforts. The GitHub<br>
repository for SPNv2 will be made available in the near future.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04275" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04275&amp;source=gmail&amp;ust=1646963617839000&amp;usg=AOvVaw1AOIw9PskH1GP6pyilmCj7" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04275</a> ,&nbsp; 7820kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04279<br>
Date: Tue, 8 Mar 2022 18:55:11 GMT&nbsp; &nbsp;(13717kb,D)<br>
<br>
Title: Probabilistic Warp Consistency for Weakly-Supervised Semantic<br>
&nbsp; Correspondences<br>
Authors: Prune Truong and Martin Danelljan and Fisher Yu and Luc Van Gool<br>
Categories: cs.CV<br>
Comments: Accepted at CVPR 2022 code:<br>
&nbsp; <a href="https://github.com/PruneTruong/DenseMatching" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/PruneTruong/DenseMatching&amp;source=gmail&amp;ust=1646963617839000&amp;usg=AOvVaw3Iaa8Np3vyklASMXP-GfiY" rel="noreferrer" target="_blank">https://github.com/<wbr>PruneTruong/DenseMatching</a><br>
Journal-ref: IEEE/CVF Conference on Computer Vision and Pattern Recognition<br>
&nbsp; (CVPR) 2022<br>
\\<br>
&nbsp; We propose Probabilistic Warp Consistency, a weakly-supervised learning<br>
objective for semantic matching. Our approach directly supervises the dense<br>
matching scores predicted by the network, encoded as a conditional probability<br>
distribution. We first construct an image triplet by applying a known warp to<br>
one of the images in a pair depicting different instances of the same object<br>
class. Our probabilistic learning objectives are then derived using the<br>
constraints arising from the resulting image triplet. We further account for<br>
occlusion and background clutter present in real image pairs by extending our<br>
probabilistic output space with a learnable unmatched state. To supervise it,<br>
we design an objective between image pairs depicting different object classes.<br>
We validate our method by applying it to four recent semantic matching<br>
architectures. Our weakly-supervised approach sets a new state-of-the-art on<br>
four challenging semantic matching benchmarks. Lastly, we demonstrate that our<br>
objective also brings substantial improvements in the strongly-supervised<br>
regime, when combined with keypoint annotations.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04279" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04279&amp;source=gmail&amp;ust=1646963617840000&amp;usg=AOvVaw1orv2alPL0_CzX1DOvkntZ" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04279</a> ,&nbsp; 13717kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04286<br>
Date: Sat, 12 Feb 2022 15:49:13 GMT&nbsp; &nbsp;(24574kb,D)<br>
<br>
Title: Proximal PanNet: A Model-Based Deep Network for Pansharpening<br>
Authors: Xiangyong Cao, Yang Chen, Wenfei Cao<br>
Categories: cs.CV<br>
Comments: 9 pages, 6 figures<br>
\\<br>
&nbsp; Recently, deep learning techniques have been extensively studied for<br>
pansharpening, which aims to generate a high resolution multispectral (HRMS)<br>
image by fusing a low resolution multispectral (LRMS) image with a high<br>
resolution panchromatic (PAN) image. However, existing deep learning-based<br>
pansharpening methods directly learn the mapping from LRMS and PAN to HRMS.<br>
These network architectures always lack sufficient interpretability, which<br>
limits further performance improvements. To alleviate this issue, we propose a<br>
novel deep network for pansharpening by combining the model-based methodology<br>
with the deep learning method. Firstly, we build an observation model for<br>
pansharpening using the convolutional sparse coding (CSC) technique and design<br>
a proximal gradient algorithm to solve this model. Secondly, we unfold the<br>
iterative algorithm into a deep network, dubbed as Proximal PanNet, by learning<br>
the proximal operators using convolutional neural networks. Finally, all the<br>
learnable modules can be automatically learned in an end-to-end manner.<br>
Experimental results on some benchmark datasets show that our network performs<br>
better than other advanced methods both quantitatively and qualitatively.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04286" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04286&amp;source=gmail&amp;ust=1646963617840000&amp;usg=AOvVaw0WD1P72sQWpXzKT1z3eBzn" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04286</a> ,&nbsp; 24574kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04287<br>
Date: Tue, 8 Mar 2022 18:59:56 GMT&nbsp; &nbsp;(203kb,D)<br>
<br>
Title: A Simple Multi-Modality Transfer Learning Baseline for Sign Language<br>
&nbsp; Translation<br>
Authors: Yutong Chen, Fangyun Wei, Xiao Sun, Zhirong Wu, Stephen Lin<br>
Categories: cs.CV<br>
Comments: Accepted by CVPR 2022<br>
\\<br>
&nbsp; This paper proposes a simple transfer learning baseline for sign language<br>
translation. Existing sign language datasets (e.g. PHOENIX-2014T, CSL-Daily)<br>
contain only about 10K-20K pairs of sign videos, gloss annotations and texts,<br>
which are an order of magnitude smaller than typical parallel data for training<br>
spoken language translation models. Data is thus a bottleneck for training<br>
effective sign language translation models. To mitigate this problem, we<br>
propose to progressively pretrain the model from general-domain datasets that<br>
include a large amount of external supervision to within-domain datasets.<br>
Concretely, we pretrain the sign-to-gloss visual network on the general domain<br>
of human actions and the within-domain of a sign-to-gloss dataset, and pretrain<br>
the gloss-to-text translation network on the general domain of a multilingual<br>
corpus and the within-domain of a gloss-to-text corpus. The joint model is<br>
fine-tuned with an additional module named the visual-language mapper that<br>
connects the two networks. This simple baseline surpasses the previous<br>
state-of-the-art results on two sign language translation benchmarks,<br>
demonstrating the effectiveness of transfer learning. With its simplicity and<br>
strong performance, this approach can serve as a solid baseline for future<br>
research.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04287" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04287&amp;source=gmail&amp;ust=1646963617840000&amp;usg=AOvVaw2z7_xvnQR6sRHgr4RT11sN" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04287</a> ,&nbsp; 203kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03668<br>
Date: Fri, 4 Mar 2022 14:16:50 GMT&nbsp; &nbsp;(5477kb,D)<br>
<br>
Title: A Typology to Explore and Guide Explanatory Interactive Machine Learning<br>
Authors: Felix Friedrich, Wolfgang Stammer, Patrick Schramowski, Kristian<br>
&nbsp; Kersting<br>
Categories: cs.LG cs.AI cs.HC<br>
\\<br>
&nbsp; Recently, more and more eXplanatory Interactive machine Learning (XIL)<br>
methods have been proposed with the goal of extending a model's learning<br>
process by integrating human user supervision on the model's explanations.<br>
These methods were often developed independently, provide different motivations<br>
and stem from different applications. Notably, up to now, there has not been a<br>
comprehensive evaluation of these works. By identifying a common set of basic<br>
modules and providing a thorough discussion of these modules, our work, for the<br>
first time, comes up with a unification of the various methods into a single<br>
typology. This typology can thus be used to categorize existing and future XIL<br>
methods based on the identified modules. Moreover, our work contributes by<br>
surveying six existing XIL methods. In addition to benchmarking these methods<br>
on their overall ability to revise a model, we perform additional benchmarks<br>
regarding wrong reason revision, interaction efficiency, robustness to feedback<br>
quality, and the ability to revise a strongly corrupted model. Apart from<br>
introducing these novel benchmarking tasks, for improved quantitative<br>
evaluations, we further introduce a novel Wrong Reason (\wrnospace) metric<br>
which measures the average wrong reason activation in a model's explanations to<br>
complement a qualitative inspection. In our evaluations, all methods prove to<br>
revise a model successfully. However, we found significant differences between<br>
the methods on individual benchmark tasks, revealing valuable<br>
application-relevant aspects not only for comparing current methods but also to<br>
motivate the necessity of incorporating these benchmarks in the development of<br>
future XIL methods.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03668" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03668&amp;source=gmail&amp;ust=1646963617840000&amp;usg=AOvVaw0eUVGCSMoUaHaj_2Bhie9v" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03668</a> ,&nbsp; 5477kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03684<br>
Date: Mon, 7 Mar 2022 19:51:25 GMT&nbsp; &nbsp;(559kb,D)<br>
<br>
Title: Learn to Match with No Regret: Reinforcement Learning in Markov Matching<br>
&nbsp; Markets<br>
Authors: Yifei Min, Tianhao Wang, Ruitu Xu, Zhaoran Wang, Michael I. Jordan,<br>
&nbsp; Zhuoran Yang<br>
Categories: cs.LG cs.AI cs.GT math.ST stat.TH<br>
Comments: 40 pages<br>
\\<br>
&nbsp; We study a Markov matching market involving a planner and a set of strategic<br>
agents on the two sides of the market. At each step, the agents are presented<br>
with a dynamical context, where the contexts determine the utilities. The<br>
planner controls the transition of the contexts to maximize the cumulative<br>
social welfare, while the agents aim to find a myopic stable matching at each<br>
step. Such a setting captures a range of applications including ridesharing<br>
platforms. We formalize the problem by proposing a reinforcement learning<br>
framework that integrates optimistic value iteration with maximum weight<br>
matching. The proposed algorithm addresses the coupled challenges of sequential<br>
exploration, matching stability, and function approximation. We prove that the<br>
algorithm achieves sublinear regret.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03684" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03684&amp;source=gmail&amp;ust=1646963617840000&amp;usg=AOvVaw2GteqDRBGzNUHywgiEbupi" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03684</a> ,&nbsp; 559kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03692<br>
Date: Mon, 7 Mar 2022 20:30:44 GMT&nbsp; &nbsp;(228kb,D)<br>
<br>
Title: Low-Loss Subspace Compression for Clean Gains against Multi-Agent<br>
&nbsp; Backdoor Attacks<br>
Authors: Siddhartha Datta, Nigel Shadbolt<br>
Categories: cs.LG cs.CR cs.MA<br>
\\<br>
&nbsp; Recent exploration of the multi-agent backdoor attack demonstrated the<br>
backfiring effect, a natural defense against backdoor attacks where backdoored<br>
inputs are randomly classified. This yields a side-effect of low accuracy<br>
w.r.t. clean labels, which motivates this paper's work on the construction of<br>
multi-agent backdoor defenses that maximize accuracy w.r.t. clean labels and<br>
minimize that of poison labels. Founded upon agent dynamics and low-loss<br>
subspace construction, we contribute three defenses that yield improved<br>
multi-agent backdoor robustness.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03692" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03692&amp;source=gmail&amp;ust=1646963617841000&amp;usg=AOvVaw1aEhZB2IaMr0UDAWGDY5En" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03692</a> ,&nbsp; 228kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03695<br>
Date: Mon, 7 Mar 2022 20:31:53 GMT&nbsp; &nbsp;(1762kb,D)<br>
<br>
Title: Learning to Bound: A Generative Cram\'er-Rao Bound<br>
Authors: Hai Victor Habi, Hagit Messer and Yoram Bresler<br>
Categories: cs.LG eess.SP<br>
\\<br>
&nbsp; The Cram\'er-Rao bound (CRB), a well-known lower bound on the performance of<br>
any unbiased parameter estimator, has been used to study a wide variety of<br>
problems. However, to obtain the CRB, requires an analytical expression for the<br>
likelihood of the measurements given the parameters, or equivalently a precise<br>
and explicit statistical model for the data. In many applications, such a model<br>
is not available. Instead, this work introduces a novel approach to approximate<br>
the CRB using data-driven methods, which removes the requirement for an<br>
analytical statistical model. This approach is based on the recent success of<br>
deep generative models in modeling complex, high-dimensional distributions.<br>
Using a learned normalizing flow model, we model the distribution of the<br>
measurements and obtain an approximation of the CRB, which we call Generative<br>
Cram\'er-Rao Bound (GCRB). Numerical experiments on simple problems validate<br>
this approach, and experiments on two image processing tasks of image denoising<br>
and edge detection with a learned camera noise model demonstrate its power and<br>
benefits.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03695" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03695&amp;source=gmail&amp;ust=1646963617841000&amp;usg=AOvVaw2sc52UWmDYXxyVeR_vmlPd" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03695</a> ,&nbsp; 1762kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03729<br>
Date: Mon, 7 Mar 2022 21:30:48 GMT&nbsp; &nbsp;(22kb)<br>
<br>
Title: Robustness and Usefulness in AI Explanation Methods<br>
Authors: Erick Galinkin<br>
Categories: cs.LG<br>
\\<br>
&nbsp; Explainability in machine learning has become incredibly important as machine<br>
learning-powered systems become ubiquitous and both regulation and public<br>
sentiment begin to demand an understanding of how these systems make decisions.<br>
As a result, a number of explanation methods have begun to receive widespread<br>
adoption. This work summarizes, compares, and contrasts three popular<br>
explanation methods: LIME, SmoothGrad, and SHAP. We evaluate these methods with<br>
respect to: robustness, in the sense of sample complexity and stability;<br>
understandability, in the sense that provided explanations are consistent with<br>
user expectations; and usability, in the sense that the explanations allow for<br>
the model to be modified based on the output. This work concludes that current<br>
explanation methods are insufficient; that putting faith in and adopting these<br>
methods may actually be worse than simply not using them.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03729" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03729&amp;source=gmail&amp;ust=1646963617841000&amp;usg=AOvVaw0XSkkSsViswtfMl1v59sZi" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03729</a> ,&nbsp; 22kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03730<br>
Date: Mon, 7 Mar 2022 21:36:21 GMT&nbsp; &nbsp;(3130kb,D)<br>
<br>
Title: Provably Accurate and Scalable Linear Classifiers in Hyperbolic Spaces<br>
Authors: Chao Pan, Eli Chien, Puoya Tabaghi, Jianhao Peng, Olgica Milenkovic<br>
Categories: cs.LG<br>
\\<br>
&nbsp; Many high-dimensional practical data sets have hierarchical structures<br>
induced by graphs or time series. Such data sets are hard to process in<br>
Euclidean spaces and one often seeks low-dimensional embeddings in other space<br>
forms to perform the required learning tasks. For hierarchical data, the space<br>
of choice is a hyperbolic space because it guarantees low-distortion embeddings<br>
for tree-like structures. The geometry of hyperbolic spaces has properties not<br>
encountered in Euclidean spaces that pose challenges when trying to rigorously<br>
analyze algorithmic solutions. We propose a unified framework for learning<br>
scalable and simple hyperbolic linear classifiers with provable performance<br>
guarantees. The gist of our approach is to focus on Poincar\'e ball models and<br>
formulate the classification problems using tangent space formalisms. Our<br>
results include a new hyperbolic perceptron algorithm as well as an efficient<br>
and highly accurate convex optimization setup for hyperbolic support vector<br>
machine classifiers. Furthermore, we adapt our approach to accommodate<br>
second-order perceptrons, where data is preprocessed based on second-order<br>
information (correlation) to accelerate convergence, and strategic perceptrons,<br>
where potentially manipulated data arrives in an online manner and decisions<br>
are made sequentially. The excellent performance of the Poincar\'e second-order<br>
and strategic perceptrons shows that the proposed framework can be extended to<br>
general machine learning problems in hyperbolic spaces. Our experimental<br>
results, pertaining to synthetic, single-cell RNA-seq expression measurements,<br>
CIFAR10, Fashion-MNIST and mini-ImageNet, establish that all algorithms<br>
provably converge and have complexity comparable to those of their Euclidean<br>
counterparts. Accompanying codes can be found at:<br>
<a href="https://github.com/thupchnsky/PoincareLinearClassification" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/thupchnsky/PoincareLinearClassification&amp;source=gmail&amp;ust=1646963617841000&amp;usg=AOvVaw0OxO3AT2F6ds47CzWU9aLW" rel="noreferrer" target="_blank">https://github.com/thupchnsky/<wbr>PoincareLinearClassification</a>.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03730" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03730&amp;source=gmail&amp;ust=1646963617841000&amp;usg=AOvVaw2_ZpHHzqYsetw6lLAoSM6d" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03730</a> ,&nbsp; 3130kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03732<br>
Date: Mon, 7 Mar 2022 21:40:14 GMT&nbsp; &nbsp;(150kb,D)<br>
<br>
Title: A Push-Relabel Based Additive Approximation for Optimal Transport<br>
Authors: Nathaniel Lahn, Sharath Raghvendra, Kaiyi Zhang<br>
Categories: cs.LG cs.DS<br>
\\<br>
&nbsp; Optimal Transport is a popular distance metric for measuring similarity<br>
between distributions. Exact algorithms for computing Optimal Transport can be<br>
slow, which has motivated the development of approximate numerical solvers<br>
(e.g. Sinkhorn method). We introduce a new and very simple combinatorial<br>
approach to find an $\varepsilon$-approximation of the OT distance. Our<br>
algorithm achieves a near-optimal execution time of $O(n^2/\varepsilon^2)$ for<br>
computing OT distance and, for the special case of the assignment problem, the<br>
execution time improves to $O(n^2/\varepsilon)$. Our algorithm is based on the<br>
push-relabel framework for min-cost flow problems.<br>
&nbsp; Unlike the other combinatorial approach (Lahn, Mulchandani and Raghvendra,<br>
NeurIPS 2019) which does not have a fast parallel implementation, our algorithm<br>
has a parallel execution time of $O(\log n/\varepsilon^2)$. Interestingly,<br>
unlike the Sinkhorn algorithm, our method also readily provides a compact<br>
transport plan as well as a solution to an approximate version of the dual<br>
formulation of the OT problem, both of which have numerous applications in<br>
Machine Learning. For the assignment problem, we provide both a CPU<br>
implementation as well as an implementation that exploits GPU parallelism.<br>
Experiments suggest that our algorithm is faster than the Sinkhorn algorithm,<br>
both in terms of CPU and GPU implementations, especially while computing<br>
matchings with a high accuracy.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03732" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03732&amp;source=gmail&amp;ust=1646963617841000&amp;usg=AOvVaw0MD55lvK16LF5hFMS378ly" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03732</a> ,&nbsp; 150kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03756<br>
Date: Mon, 7 Mar 2022 22:35:20 GMT&nbsp; &nbsp;(351kb,D)<br>
<br>
Title: Flat minima generalize for low-rank matrix recovery<br>
Authors: Lijun Ding, Dmitriy Drusvyatskiy, Maryam Fazel<br>
Categories: cs.LG math.OC stat.ML<br>
Comments: 30 pages<br>
\\<br>
&nbsp; Empirical evidence suggests that for a variety of overparameterized nonlinear<br>
models, most notably in neural network training, the growth of the loss around<br>
a minimizer strongly impacts its performance. Flat minima -- those around which<br>
the loss grows slowly -- appear to generalize well. This work takes a step<br>
towards understanding this phenomenon by focusing on the simplest class of<br>
overparameterized nonlinear models: those arising in low-rank matrix recovery.<br>
We analyze overparameterized matrix and bilinear sensing, robust PCA,<br>
covariance matrix estimation, and single hidden layer neural networks with<br>
quadratic activation functions. In all cases, we show that flat minima,<br>
measured by the trace of the Hessian, exactly recover the ground truth under<br>
standard statistical assumptions. For matrix completion, we establish weak<br>
recovery, although empirical evidence suggests exact recovery holds here as<br>
well. We complete the paper with synthetic experiments that illustrate our<br>
findings.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03756" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03756&amp;source=gmail&amp;ust=1646963617842000&amp;usg=AOvVaw3RimMlpTZwK2uEzPgbc5fD" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03756</a> ,&nbsp; 351kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03761<br>
Date: Mon, 7 Mar 2022 22:56:09 GMT&nbsp; &nbsp;(1585kb,D)<br>
<br>
Title: The Fundamental Price of Secure Aggregation in Differentially Private<br>
&nbsp; Federated Learning<br>
Authors: Wei-Ning Chen, Christopher A. Choquette-Choo, Peter Kairouz, Ananda<br>
&nbsp; Theertha Suresh<br>
Categories: cs.LG stat.ML<br>
\\<br>
&nbsp; We consider the problem of training a $d$ dimensional model with distributed<br>
differential privacy (DP) where secure aggregation (SecAgg) is used to ensure<br>
that the server only sees the noisy sum of $n$ model updates in every training<br>
round. Taking into account the constraints imposed by SecAgg, we characterize<br>
the fundamental communication cost required to obtain the best accuracy<br>
achievable under $\varepsilon$ central DP (i.e. under a fully trusted server<br>
and no communication constraints). Our results show that $\tilde{O}\left(<br>
\min(n^2\varepsilon^2, d) \right)$ bits per client are both sufficient and<br>
necessary, and this fundamental limit can be achieved by a linear scheme based<br>
on sparse random projections. This provides a significant improvement relative<br>
to state-of-the-art SecAgg distributed DP schemes which use<br>
$\tilde{O}(d\log(d/\<wbr>varepsilon^2))$ bits per client.<br>
&nbsp; Empirically, we evaluate our proposed scheme on real-world federated learning<br>
tasks. We find that our theoretical analysis is well matched in practice. In<br>
particular, we show that we can reduce the communication cost significantly to<br>
under $1.2$ bits per parameter in realistic privacy settings without decreasing<br>
test-time performance. Our work hence theoretically and empirically specifies<br>
the fundamental price of using SecAgg.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03761" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03761&amp;source=gmail&amp;ust=1646963617842000&amp;usg=AOvVaw2Xx8TAUD-1fFjkCrQ47xC2" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03761</a> ,&nbsp; 1585kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03762<br>
Date: Mon, 7 Mar 2022 22:57:43 GMT&nbsp; &nbsp;(264kb,D)<br>
<br>
Title: Defending Graph Convolutional Networks against Dynamic Graph<br>
&nbsp; Perturbations via Bayesian Self-supervision<br>
Authors: Jun Zhuang, Mohammad Al Hasan<br>
Categories: cs.LG<br>
Comments: The paper is accepted by AAAI 2022<br>
\\<br>
&nbsp; In recent years, plentiful evidence illustrates that Graph Convolutional<br>
Networks (GCNs) achieve extraordinary accomplishments on the node<br>
classification task. However, GCNs may be vulnerable to adversarial attacks on<br>
label-scarce dynamic graphs. Many existing works aim to strengthen the<br>
robustness of GCNs; for instance, adversarial training is used to shield GCNs<br>
against malicious perturbations. However, these works fail on dynamic graphs<br>
for which label scarcity is a pressing issue. To overcome label scarcity,<br>
self-training attempts to iteratively assign pseudo-labels to highly confident<br>
unlabeled nodes but such attempts may suffer serious degradation under dynamic<br>
graph perturbations. In this paper, we generalize noisy supervision as a kind<br>
of self-supervised learning method and then propose a novel Bayesian<br>
self-supervision model, namely GraphSS, to address the issue. Extensive<br>
experiments demonstrate that GraphSS can not only affirmatively alert the<br>
perturbations on dynamic graphs but also effectively recover the prediction of<br>
a node classifier when the graph is under such perturbations. These two<br>
advantages prove to be generalized over three classic GCNs across five public<br>
graph datasets.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03762" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03762&amp;source=gmail&amp;ust=1646963617842000&amp;usg=AOvVaw21vU6YNyPFvIFIOx2sZG51" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03762</a> ,&nbsp; 264kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03771<br>
Date: Mon, 7 Mar 2022 23:17:17 GMT&nbsp; &nbsp;(259kb,D)<br>
<br>
Title: Static Prediction of Runtime Errors by Learning to Execute Programs with<br>
&nbsp; External Resource Descriptions<br>
Authors: David Bieber, Rishab Goel, Daniel Zheng, Hugo Larochelle, Daniel<br>
&nbsp; Tarlow<br>
Categories: cs.LG<br>
Comments: 20 pages, 7 figures<br>
\\<br>
&nbsp; The execution behavior of a program often depends on external resources, such<br>
as program inputs or file contents, and so cannot be run in isolation.<br>
Nevertheless, software developers benefit from fast iteration loops where<br>
automated tools identify errors as early as possible, even before programs can<br>
be compiled and run. This presents an interesting machine learning challenge:<br>
can we predict runtime errors in a "static" setting, where program execution is<br>
not possible? Here, we introduce a real-world dataset and task for predicting<br>
runtime errors, which we show is difficult for generic models like<br>
Transformers. We approach this task by developing an interpreter-inspired<br>
architecture with an inductive bias towards mimicking program executions, which<br>
models exception handling and "learns to execute" descriptions of the contents<br>
of external resources. Surprisingly, we show that the model can also predict<br>
the location of the error, despite being trained only on labels indicating the<br>
presence/absence and kind of error. In total, we present a practical and<br>
difficult-yet-approachable challenge problem related to learning program<br>
execution and we demonstrate promising new capabilities of interpreter-inspired<br>
machine learning models for code.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03771" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03771&amp;source=gmail&amp;ust=1646963617842000&amp;usg=AOvVaw04dr_7HiCEoN-PNOgAGVva" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03771</a> ,&nbsp; 259kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03776<br>
Date: Mon, 7 Mar 2022 23:38:01 GMT&nbsp; &nbsp;(207kb,D)<br>
<br>
Title: Zero-delay Consistent and Smooth Trainable Interpolation<br>
Authors: Emilio Ruiz-Moreno, Luis Miguel L\'opez-Ramos, Baltasar<br>
&nbsp; Beferull-Lozano<br>
Categories: cs.LG eess.SP<br>
Comments: 12 pages, 7 figures, submitted to IEEE Transactions on Neural<br>
&nbsp; Networks and Learning Systems<br>
ACM-class: I.2.6<br>
\\<br>
&nbsp; The question of how to produce a smooth interpolating curve from a stream of<br>
data points is addressed in this paper. To this end, we formalize the concept<br>
of real-time interpolator (RTI): a trainable unit that recovers smooth signals<br>
that are consistent with the received input samples in an online manner.<br>
Specifically, an RTI works under the requirement of producing a function<br>
section immediately after a sample is received (zero delay), without changing<br>
the reconstructed signal in past time sections. This work formulates the design<br>
of spline-based RTIs as a bi-level optimization problem. Their training<br>
consists in minimizing the average curvature of the interpolated signals over a<br>
set of example sequences. The latter are representative of the nature of the<br>
data sequence to be interpolated, allowing to tailor the RTI to a specific<br>
signal source. Our overall design allows for different possible schemes. In<br>
this work, we present two approaches, namely, the parametrized RTI and the<br>
recurrent neural network (RNN)-based RTI, including their architecture and<br>
properties. Experimental results show that the two proposed RTIs can be trained<br>
in a data-driven fashion to achieve improved performance (in terms of the<br>
curvature loss metric) with respect to a myopic-type RTI that only exploits the<br>
local information at each time sample, while maintaining smooth, zero-delay,<br>
and consistency requirements.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03776" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03776&amp;source=gmail&amp;ust=1646963617842000&amp;usg=AOvVaw3nB4LoSAosPnWcTvxFnihF" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03776</a> ,&nbsp; 207kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03794<br>
Date: Tue, 8 Mar 2022 01:24:36 GMT&nbsp; &nbsp;(6991kb,D)<br>
<br>
Title: YONO: Modeling Multiple Heterogeneous Neural Networks on<br>
&nbsp; Microcontrollers<br>
Authors: Young D. Kwon, Jagmohan Chauhan, and Cecilia Mascolo<br>
Categories: cs.LG<br>
Comments: Accepted for publication at IPSN 2022<br>
\\<br>
&nbsp; With the advancement of Deep Neural Networks (DNN) and large amounts of<br>
sensor data from Internet of Things (IoT) systems, the research community has<br>
worked to reduce the computational and resource demands of DNN to compute on<br>
low-resourced microcontrollers (MCUs). However, most of the current work in<br>
embedded deep learning focuses on solving a single task efficiently, while the<br>
multi-tasking nature and applications of IoT devices demand systems that can<br>
handle a diverse range of tasks (activity, voice, and context recognition) with<br>
input from a variety of sensors, simultaneously.<br>
&nbsp; In this paper, we propose YONO, a product quantization (PQ) based approach<br>
that compresses multiple heterogeneous models and enables in-memory model<br>
execution and switching for dissimilar multi-task learning on MCUs. We first<br>
adopt PQ to learn codebooks that store weights of different models. Also, we<br>
propose a novel network optimization and heuristics to maximize the compression<br>
rate and minimize the accuracy loss. Then, we develop an online component of<br>
YONO for efficient model execution and switching between multiple tasks on an<br>
MCU at run time without relying on an external storage device.<br>
&nbsp; YONO shows remarkable performance as it can compress multiple heterogeneous<br>
models with negligible or no loss of accuracy up to 12.37$\times$. Besides,<br>
YONO's online component enables an efficient execution (latency of 16-159 ms<br>
per operation) and reduces model loading/switching latency and energy<br>
consumption by 93.3-94.5% and 93.9-95.0%, respectively, compared to external<br>
storage access. Interestingly, YONO can compress various architectures trained<br>
with datasets that were not shown during YONO's offline codebook learning phase<br>
showing the generalizability of our method. To summarize, YONO shows great<br>
potential and opens further doors to enable multi-task learning systems on<br>
extremely resource-constrained devices.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03794" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03794&amp;source=gmail&amp;ust=1646963617843000&amp;usg=AOvVaw3BJfU3ZoQfvAOcLEKqZHS4" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03794</a> ,&nbsp; 6991kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03798<br>
Date: Tue, 8 Mar 2022 01:37:00 GMT&nbsp; &nbsp;(5460kb,D)<br>
<br>
Title: New Insights on Reducing Abrupt Representation Change in Online<br>
&nbsp; Continual Learning<br>
Authors: Lucas Caccia, Rahaf Aljundi, Nader Asadi, Tinne Tuytelaars, Joelle<br>
&nbsp; Pineau, Eugene Belilovsky<br>
Categories: cs.LG cs.AI<br>
Comments: Accepted at ICLR 2022. Code Available at<br>
&nbsp; <a href="https://github.com/pclucas14/AML" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/pclucas14/AML&amp;source=gmail&amp;ust=1646963617843000&amp;usg=AOvVaw1d8Rjgzl_VyAR4A-OG97t6" rel="noreferrer" target="_blank">https://github.com/pclucas14/<wbr>AML</a><br>
\\<br>
&nbsp; In the online continual learning paradigm, agents must learn from a changing<br>
distribution while respecting memory and compute constraints. Experience Replay<br>
(ER), where a small subset of past data is stored and replayed alongside new<br>
data, has emerged as a simple and effective learning strategy. In this work, we<br>
focus on the change in representations of observed data that arises when<br>
previously unobserved classes appear in the incoming data stream, and new<br>
classes must be distinguished from previous ones. We shed new light on this<br>
question by showing that applying ER causes the newly added classes'<br>
representations to overlap significantly with the previous classes, leading to<br>
highly disruptive parameter updates. Based on this empirical analysis, we<br>
propose a new method which mitigates this issue by shielding the learned<br>
representations from drastic adaptation to accommodate new classes. We show<br>
that using an asymmetric update rule pushes new classes to adapt to the older<br>
ones (rather than the reverse), which is more effective especially at task<br>
boundaries, where much of the forgetting typically occurs. Empirical results<br>
show significant gains over strong baselines on standard continual learning<br>
benchmarks<br>
\\ ( <a href="https://arxiv.org/abs/2203.03798" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03798&amp;source=gmail&amp;ust=1646963617843000&amp;usg=AOvVaw1OJOW7r4Q7JfBwt23QJdWD" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03798</a> ,&nbsp; 5460kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03810<br>
Date: Tue, 8 Mar 2022 02:05:40 GMT&nbsp; &nbsp;(4422kb,D)<br>
<br>
Title: Towards Efficient Data-Centric Robust Machine Learning with Noise-based<br>
&nbsp; Augmentation<br>
Authors: Xiaogeng Liu, Haoyu Wang, Yechao Zhang, Fangzhou Wu, Shengshan Hu<br>
Categories: cs.LG cs.AI cs.CR<br>
Comments: Competition paper of AAAI2022:Data-Centric Robust Learning on ML<br>
&nbsp; Models. Published in Workshop on Adversarial Machine Learning and Beyond at<br>
&nbsp; AAAI2022<br>
\\<br>
&nbsp; The data-centric machine learning aims to find effective ways to build<br>
appropriate datasets which can improve the performance of AI models. In this<br>
paper, we mainly focus on designing an efficient data-centric scheme to improve<br>
robustness for models towards unforeseen malicious inputs in the black-box test<br>
settings. Specifically, we introduce a noised-based data augmentation method<br>
which is composed of Gaussian Noise, Salt-and-Pepper noise, and the PGD<br>
adversarial perturbations. The proposed method is built on lightweight<br>
algorithms and proved highly effective based on comprehensive evaluations,<br>
showing good efficiency on computation cost and robustness enhancement. In<br>
addition, we share our insights about the data-centric robust machine learning<br>
gained from our experiments.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03810" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03810&amp;source=gmail&amp;ust=1646963617843000&amp;usg=AOvVaw1ejdAABBG0jIz4lRrdcJGZ" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03810</a> ,&nbsp; 4422kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03906<br>
Date: Tue, 8 Mar 2022 08:02:54 GMT&nbsp; &nbsp;(865kb,D)<br>
<br>
Title: Graph Reinforcement Learning for Predictive Power Allocation to Mobile<br>
&nbsp; Users<br>
Authors: Jianyu Zhao and Chenyang Yang<br>
Categories: cs.LG cs.SY eess.SY<br>
\\<br>
&nbsp; Allocating resources with future channels can save resource to ensure<br>
quality-of-service of video streaming. In this paper, we optimize predictive<br>
power allocation to minimize the energy consumed at distributed units (DUs) by<br>
using deep deterministic policy gradient (DDPG) to find optimal policy and<br>
predict average channel gains. To improve training efficiency, we resort to<br>
graph DDPG for exploiting two kinds of relational priors: (a) permutation<br>
equivariant (PE) and permutation invariant (PI) properties of policy function<br>
and action-value function, (b) topology relation among users and DUs. To design<br>
graph DDPG framework more systematically in harnessing the priors, we first<br>
demonstrate how to transform matrix-based DDPG into graph-based DDPG. Then, we<br>
respectively design the actor and critic networks to satisfy the permutation<br>
properties when graph neural networks are used in embedding and end to-end<br>
manners. To avoid destroying the PE/PI properties of the actor and critic<br>
networks, we conceive a batch normalization method. Finally, we show the impact<br>
of leveraging each prior. Simulation results show that the learned predictive<br>
policy performs close to the optimal solution with perfect future information,<br>
and the graph DDPG algorithms converge much faster than existing DDPG<br>
algorithms.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03906" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03906&amp;source=gmail&amp;ust=1646963617843000&amp;usg=AOvVaw19NQ0vZRTdFa133Ngu7Naj" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03906</a> ,&nbsp; 865kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03917<br>
Date: Tue, 8 Mar 2022 08:26:31 GMT&nbsp; &nbsp;(31353kb,D)<br>
<br>
Title: An Analysis of Measure-Valued Derivatives for Policy Gradients<br>
Authors: Joao Carvalho and Jan Peters<br>
Categories: cs.LG<br>
\\<br>
&nbsp; Reinforcement learning methods for robotics are increasingly successful due<br>
to the constant development of better policy gradient techniques. A precise<br>
(low variance) and accurate (low bias) gradient estimator is crucial to face<br>
increasingly complex tasks. Traditional policy gradient algorithms use the<br>
likelihood-ratio trick, which is known to produce unbiased but high variance<br>
estimates. More modern approaches exploit the reparametrization trick, which<br>
gives lower variance gradient estimates but requires differentiable value<br>
function approximators. In this work, we study a different type of stochastic<br>
gradient estimator - the Measure-Valued Derivative. This estimator is unbiased,<br>
has low variance, and can be used with differentiable and non-differentiable<br>
function approximators. We empirically evaluate this estimator in the<br>
actor-critic policy gradient setting and show that it can reach comparable<br>
performance with methods based on the likelihood-ratio or reparametrization<br>
tricks, both in low and high-dimensional action spaces. With this work, we want<br>
to show that the Measure-Valued Derivative estimator can be a useful<br>
alternative to other policy gradient estimators.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03917" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03917&amp;source=gmail&amp;ust=1646963617843000&amp;usg=AOvVaw3y0gKr5IHnVc4o8UVFN2Ru" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03917</a> ,&nbsp; 31353kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03929<br>
Date: Tue, 8 Mar 2022 08:50:34 GMT&nbsp; &nbsp;(1962kb,D)<br>
<br>
Title: Quantifying Privacy Risks of Masked Language Models Using Membership<br>
&nbsp; Inference Attacks<br>
Authors: Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor<br>
&nbsp; Berg-Kirkpatrick, Reza Shokri<br>
Categories: cs.LG cs.AI cs.CR<br>
\\<br>
&nbsp; The wide adoption and application of Masked language models~(MLMs) on<br>
sensitive data (from legal to medical) necessitates a thorough quantitative<br>
investigation into their privacy vulnerabilities -- to what extent do MLMs leak<br>
information about their training data? Prior attempts at measuring leakage of<br>
MLMs via membership inference attacks have been inconclusive, implying the<br>
potential robustness of MLMs to privacy attacks. In this work, we posit that<br>
prior attempts were inconclusive because they based their attack solely on the<br>
MLM's model score. We devise a stronger membership inference attack based on<br>
likelihood ratio hypothesis testing that involves an additional reference MLM<br>
to more accurately quantify the privacy risks of memorization in MLMs. We show<br>
that masked language models are extremely susceptible to likelihood ratio<br>
membership inference attacks: Our empirical results, on models trained on<br>
medical notes, show that our attack improves the AUC of prior membership<br>
inference attacks from 0.66 to an alarmingly high 0.90 level, with a<br>
significant improvement in the low-error region: at 1% false positive rate, our<br>
attack is 51X more powerful than prior work.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03929" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03929&amp;source=gmail&amp;ust=1646963617844000&amp;usg=AOvVaw19b153chkxqnJ2eN-R7Tfd" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03929</a> ,&nbsp; 1962kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03934<br>
Date: Tue, 8 Mar 2022 08:57:43 GMT&nbsp; &nbsp;(1171kb,D)<br>
<br>
Title: Nonlinear Isometric Manifold Learning for Injective Normalizing Flows<br>
Authors: Eike Cramer, Felix Rauh, Alexander Mitsos, Ra\'ul Tempone, Manuel<br>
&nbsp; Dahmen<br>
Categories: cs.LG<br>
Comments: 10 pages, 5 figures, 3 tables<br>
\\<br>
&nbsp; To model manifold data using normalizing flows, we propose to employ the<br>
isometric autoencoder to design nonlinear encodings with explicit inverses. The<br>
isometry allows us to separate manifold learning and density estimation and<br>
train both parts to high accuracy. Applied to the MNIST data set, the combined<br>
approach generates high-quality images.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03934" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03934&amp;source=gmail&amp;ust=1646963617844000&amp;usg=AOvVaw2Ub3iHPBd6eriiMbzbRTxz" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03934</a> ,&nbsp; 1171kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03965<br>
Date: Tue, 8 Mar 2022 09:46:50 GMT&nbsp; &nbsp;(2607kb,D)<br>
<br>
Title: Few-Shot Traffic Prediction with Graph Networks using Locale as<br>
&nbsp; Relational Inductive Biases<br>
Authors: Mingxi Li, Yihong Tang, Wei Ma<br>
Categories: cs.LG stat.AP<br>
\\<br>
&nbsp; Accurate short-term traffic prediction plays a pivotal role in various smart<br>
mobility operation and management systems. Currently, most of the<br>
state-of-the-art prediction models are based on graph neural networks (GNNs),<br>
and the required training samples are proportional to the size of the traffic<br>
network. In many cities, the available amount of traffic data is substantially<br>
below the minimum requirement due to the data collection expense. It is still<br>
an open question to develop traffic prediction models with a small size of<br>
training data on large-scale networks. We notice that the traffic states of a<br>
node for the near future only depend on the traffic states of its localized<br>
neighborhoods, which can be represented using the graph relational inductive<br>
biases. In view of this, this paper develops a graph network (GN)-based deep<br>
learning model LocaleGn that depicts the traffic dynamics using localized data<br>
aggregating and updating functions, as well as the node-wise recurrent neural<br>
networks. LocaleGn is a light-weighted model designed for training on few<br>
samples without over-fitting, and hence it can solve the problem of few-shot<br>
traffic prediction. The proposed model is examined on predicting both traffic<br>
speed and flow with six datasets, and the experimental results demonstrate that<br>
LocaleGn outperforms existing state-of-the-art baseline models. It is also<br>
demonstrated that the learned knowledge from LocaleGn can be transferred across<br>
cities. The research outcomes can help to develop light-weighted traffic<br>
prediction systems, especially for cities lacking in historically archived<br>
traffic data.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03965" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03965&amp;source=gmail&amp;ust=1646963617844000&amp;usg=AOvVaw1YA04W9saWb6B0bg_cBUGl" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03965</a> ,&nbsp; 2607kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03970<br>
Date: Tue, 8 Mar 2022 09:57:48 GMT&nbsp; &nbsp;(2256kb,D)<br>
<br>
Title: On Generalizing Beyond Domains in Cross-Domain Continual Learning<br>
Authors: Christian Simon, Masoud Faraki, Yi-Hsuan Tsai, Xiang Yu, Samuel<br>
&nbsp; Schulter, Yumin Suh, Mehrtash Harandi, Manmohan Chandraker<br>
Categories: cs.LG cs.CV<br>
Comments: Accepted to CVPR 2022<br>
\\<br>
&nbsp; Humans have the ability to accumulate knowledge of new tasks in varying<br>
conditions, but deep neural networks often suffer from catastrophic forgetting<br>
of previously learned knowledge after learning a new task. Many recent methods<br>
focus on preventing catastrophic forgetting under the assumption of train and<br>
test data following similar distributions. In this work, we consider a more<br>
realistic scenario of continual learning under domain shifts where the model<br>
must generalize its inference to an unseen domain. To this end, we encourage<br>
learning semantically meaningful features by equipping the classifier with<br>
class similarity metrics as learning parameters which are obtained through<br>
Mahalanobis similarity computations. Learning of the backbone representation<br>
along with these extra parameters is done seamlessly in an end-to-end manner.<br>
In addition, we propose an approach based on the exponential moving average of<br>
the parameters for better knowledge distillation. We demonstrate that, to a<br>
great extent, existing continual learning algorithms fail to handle the<br>
forgetting issue under multiple distributions, while our proposed approach<br>
learns new tasks under domain shift with accuracy boosts up to 10% on<br>
challenging datasets such as DomainNet and OfficeHome.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03970" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03970&amp;source=gmail&amp;ust=1646963617844000&amp;usg=AOvVaw3zmnlf5Wmd68qAdrGRxx_m" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03970</a> ,&nbsp; 2256kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03978<br>
Date: Tue, 8 Mar 2022 10:08:45 GMT&nbsp; &nbsp;(5252kb,D)<br>
<br>
Title: Contrastive Conditional Neural Processes<br>
Authors: Zesheng Ye, Lina Yao<br>
Categories: cs.LG stat.ML<br>
Comments: 8 pages, 7 figures, accepted to CVPR2022<br>
\\<br>
&nbsp; Conditional Neural Processes~(CNPs) bridge neural networks with probabilistic<br>
inference to approximate functions of Stochastic Processes under meta-learning<br>
settings. Given a batch of non-{\it i.i.d} function instantiations, CNPs are<br>
jointly optimized for in-instantiation observation prediction and<br>
cross-instantiation meta-representation adaptation within a generative<br>
reconstruction pipeline. There can be a challenge in tying together such two<br>
targets when the distribution of function observations scales to<br>
high-dimensional and noisy spaces. Instead, noise contrastive estimation might<br>
be able to provide more robust representations by learning distributional<br>
matching objectives to combat such inherent limitation of generative models. In<br>
light of this, we propose to equip CNPs by 1) aligning prediction with encoded<br>
ground-truth observation, and 2) decoupling meta-representation adaptation from<br>
generative reconstruction. Specifically, two auxiliary contrastive branches are<br>
set up hierarchically, namely in-instantiation temporal contrastive<br>
learning~({\tt TCL}) and cross-instantiation function contrastive<br>
learning~({\tt FCL}), to facilitate local predictive alignment and global<br>
function consistency, respectively. We empirically show that {\tt TCL} captures<br>
high-level abstraction of observations, whereas {\tt FCL} helps identify<br>
underlying functions, which in turn provides more efficient representations.<br>
Our model outperforms other CNPs variants when evaluating function distribution<br>
reconstruction and parameter identification across 1D, 2D and high-dimensional<br>
time-series.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03978" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03978&amp;source=gmail&amp;ust=1646963617844000&amp;usg=AOvVaw0tQD78tanqW8RR5I-7LdKC" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03978</a> ,&nbsp; 5252kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03991<br>
Date: Tue, 8 Mar 2022 10:44:30 GMT&nbsp; &nbsp;(314kb,D)<br>
<br>
Title: Sparsification and Filtering for Spatial-temporal GNN in Multivariate<br>
&nbsp; Time-series<br>
Authors: Yuanrong Wang, Tomaso Aste<br>
Categories: cs.LG q-fin.CP<br>
Comments: 7 pages, 1 figure, 3tables<br>
\\<br>
&nbsp; We propose an end-to-end architecture for multivariate time-series prediction<br>
that integrates a spatial-temporal graph neural network with a matrix filtering<br>
module. This module generates filtered (inverse) correlation graphs from<br>
multivariate time series before inputting them into a GNN. In contrast with<br>
existing sparsification methods adopted in graph neural network, our model<br>
explicitly leverage time-series filtering to overcome the low signal-to-noise<br>
ratio typical of complex systems data. We present a set of experiments, where<br>
we predict future sales from a synthetic time-series sales dataset. The<br>
proposed spatial-temporal graph neural network displays superior performances<br>
with respect to baseline approaches, with no graphical information, and with<br>
fully connected, disconnected graphs and unfiltered graphs.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03991" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03991&amp;source=gmail&amp;ust=1646963617845000&amp;usg=AOvVaw3o7L7zAWDTBzTexrw7ALaF" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03991</a> ,&nbsp; 314kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04027<br>
Date: Tue, 8 Mar 2022 11:41:38 GMT&nbsp; &nbsp;(2291kb,D)<br>
<br>
Title: Data augmentation with mixtures of max-entropy transformations for<br>
&nbsp; filling-level classification<br>
Authors: Apostolos Modas and Andrea Cavallaro and Pascal Frossard<br>
Categories: cs.LG cs.CV cs.RO<br>
\\<br>
&nbsp; We address the problem of distribution shifts in test-time data with a<br>
principled data augmentation scheme for the task of content-level<br>
classification. In such a task, properties such as shape or transparency of<br>
test-time containers (cup or drinking glass) may differ from those represented<br>
in the training data. Dealing with such distribution shifts using standard<br>
augmentation schemes is challenging and transforming the training images to<br>
cover the properties of the test-time instances requires sophisticated image<br>
manipulations. We therefore generate diverse augmentations using a family of<br>
max-entropy transformations that create samples with new shapes, colors and<br>
spectral characteristics. We show that such a principled augmentation scheme,<br>
alone, can replace current approaches that use transfer learning or can be used<br>
in combination with transfer learning to improve its performance.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04027" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04027&amp;source=gmail&amp;ust=1646963617845000&amp;usg=AOvVaw2_HIaZF4e9Mc5Kj_2XpI34" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04027</a> ,&nbsp; 2291kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04032<br>
Date: Tue, 8 Mar 2022 11:46:41 GMT&nbsp; &nbsp;(479kb,D)<br>
<br>
Title: Bayesian Optimisation-Assisted Neural Network Training Technique for<br>
&nbsp; Radio Localisation<br>
Authors: Xingchi Liu, Peizheng Li and Ziming Zhu<br>
Categories: cs.LG eess.SP<br>
Comments: 5 pages, 4 figures. This paper has been accepted for presentation at<br>
&nbsp; the VTC2022-Spring<br>
\\<br>
&nbsp; Radio signal-based (indoor) localisation technique is important for IoT<br>
applications such as smart factory and warehouse. Through machine learning,<br>
especially neural networks methods, more accurate mapping from signal features<br>
to target positions can be achieved. However, different radio protocols, such<br>
as WiFi, Bluetooth, etc., have different features in the transmitted signals<br>
that can be exploited for localisation purposes. Also, neural networks methods<br>
often rely on carefully configured models and extensive training processes to<br>
obtain satisfactory performance in individual localisation scenarios. The above<br>
poses a major challenge in the process of determining neural network model<br>
structure, or hyperparameters, as well as the selection of training features<br>
from the available data. This paper proposes a neural network model<br>
hyperparameter tuning and training method based on Bayesian optimisation.<br>
Adaptive selection of model hyperparameters and training features can be<br>
realised with minimal need for manual model training design. With the proposed<br>
technique, the training process is optimised in a more automatic and efficient<br>
way, enhancing the applicability of neural networks in localisation.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04032" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04032&amp;source=gmail&amp;ust=1646963617845000&amp;usg=AOvVaw3DbnDdYxxSeYETYWb9iniw" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04032</a> ,&nbsp; 479kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04071<br>
Date: Tue, 8 Mar 2022 13:31:16 GMT&nbsp; &nbsp;(700kb,D)<br>
<br>
Title: AdaPT: Fast Emulation of Approximate DNN Accelerators in PyTorch<br>
Authors: Dimitrios Danopoulos, Georgios Zervakis, Kostas Siozios, Dimitrios<br>
&nbsp; Soudris, J\"org Henkel<br>
Categories: cs.LG cs.AR<br>
\\<br>
&nbsp; Current state-of-the-art employs approximate multipliers to address the<br>
highly increased power demands of DNN accelerators. However, evaluating the<br>
accuracy of approximate DNNs is cumbersome due to the lack of adequate support<br>
for approximate arithmetic in DNN frameworks. We address this inefficiency by<br>
presenting AdaPT, a fast emulation framework that extends PyTorch to support<br>
approximate inference as well as approximation-aware retraining. AdaPT can be<br>
seamlessly deployed and is compatible with the most DNNs. We evaluate the<br>
framework on several DNN models and application fields including CNNs, LSTMs,<br>
and GANs for a number of approximate multipliers with distinct bitwidth values.<br>
The results show substantial error recovery from approximate re-training and<br>
reduced inference time up to 53.9x with respect to the baseline approximate<br>
implementation.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04071" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04071&amp;source=gmail&amp;ust=1646963617845000&amp;usg=AOvVaw0SwjfHhm-4oKYgC8MMHpBL" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04071</a> ,&nbsp; 700kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04098<br>
Date: Tue, 8 Mar 2022 14:08:36 GMT&nbsp; &nbsp;(5201kb,D)<br>
<br>
Title: COLA: Consistent Learning with Opponent-Learning Awareness<br>
Authors: Timon Willi, Johannes Treutlein, Alistair Letcher, Jakob Foerster<br>
Categories: cs.LG cs.AI cs.GT<br>
\\<br>
&nbsp; Learning in general-sum games can be unstable and often leads to socially<br>
undesirable, Pareto-dominated outcomes. To mitigate this, Learning with<br>
Opponent-Learning Awareness (LOLA) introduced opponent shaping to this setting,<br>
by accounting for the agent's influence on the anticipated learning steps of<br>
other agents. However, the original LOLA formulation (and follow-up work) is<br>
inconsistent because LOLA models other agents as naive learners rather than<br>
LOLA agents. In previous work, this inconsistency was suggested as a cause of<br>
LOLA's failure to preserve stable fixed points (SFPs). First, we formalize<br>
consistency and show that higher-order LOLA (HOLA) solves LOLA's inconsistency<br>
problem if it converges. Second, we correct a claim made in the literature, by<br>
proving that, contrary to Sch\"afer and Anandkumar (2019), Competitive Gradient<br>
Descent (CGD) does not recover HOLA as a series expansion. Hence, CGD also does<br>
not solve the consistency problem. Third, we propose a new method called<br>
Consistent LOLA (COLA), which learns update functions that are consistent under<br>
mutual opponent shaping. It requires no more than second-order derivatives and<br>
learns consistent update functions even when HOLA fails to converge. However,<br>
we also prove that even consistent update functions do not preserve SFPs,<br>
contradicting the hypothesis that this shortcoming is caused by LOLA's<br>
inconsistency. Finally, in an empirical evaluation on a set of general-sum<br>
games, we find that COLA finds prosocial solutions and that it converges under<br>
a wider range of learning rates than HOLA and LOLA. We support the latter<br>
finding with a theoretical result for a simple game.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04098" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04098&amp;source=gmail&amp;ust=1646963617845000&amp;usg=AOvVaw1V67W4APEjekmdk7LTj6A6" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04098</a> ,&nbsp; 5201kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04160<br>
Date: Tue, 8 Mar 2022 15:43:33 GMT&nbsp; &nbsp;(89kb,D)<br>
<br>
Title: Robustly-reliable learners under poisoning attacks<br>
Authors: Maria-Florina Balcan, Avrim Blum, Steve Hanneke, Dravyansh Sharma<br>
Categories: cs.LG cs.AI cs.CR cs.DS<br>
\\<br>
&nbsp; Data poisoning attacks, in which an adversary corrupts a training set with<br>
the goal of inducing specific desired mistakes, have raised substantial<br>
concern: even just the possibility of such an attack can make a user no longer<br>
trust the results of a learning system. In this work, we show how to achieve<br>
strong robustness guarantees in the face of such attacks across multiple axes.<br>
&nbsp; We provide robustly-reliable predictions, in which the predicted label is<br>
guaranteed to be correct so long as the adversary has not exceeded a given<br>
corruption budget, even in the presence of instance targeted attacks, where the<br>
adversary knows the test example in advance and aims to cause a specific<br>
failure on that example. Our guarantees are substantially stronger than those<br>
in prior approaches, which were only able to provide certificates that the<br>
prediction of the learning algorithm does not change, as opposed to certifying<br>
that the prediction is correct, as we are able to achieve in our work.<br>
Remarkably, we provide a complete characterization of learnability in this<br>
setting, in particular, nearly-tight matching upper and lower bounds on the<br>
region that can be certified, as well as efficient algorithms for computing<br>
this region given an ERM oracle. Moreover, for the case of linear separators<br>
over logconcave distributions, we provide efficient truly polynomial time<br>
algorithms (i.e., non-oracle algorithms) for such robustly-reliable<br>
predictions.<br>
&nbsp; We also extend these results to the active setting where the algorithm<br>
adaptively asks for labels of specific informative examples, and the difficulty<br>
is that the adversary might even be adaptive to this interaction, as well as to<br>
the agnostic learning setting where there is no perfect classifier even over<br>
the uncorrupted data.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04160" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04160&amp;source=gmail&amp;ust=1646963617846000&amp;usg=AOvVaw0w9dLUXubUKbWCyiavzEK4" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04160</a> ,&nbsp; 89kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04183<br>
Date: Tue, 8 Mar 2022 16:15:54 GMT&nbsp; &nbsp;(39628kb,D)<br>
<br>
Title: Enhancing Mechanical Metamodels with a Generative Model-Based Augmented<br>
&nbsp; Training Dataset<br>
Authors: Hiba Kobeissi, Saeed Mohammadzadeh, Emma Lejeune<br>
Categories: cs.LG physics.data-an<br>
Comments: 10 pages, 5 figures<br>
MSC-class: 74A40, 74B20, 74S05<br>
ACM-class: I.6.3; I.6.5; J.2<br>
\\<br>
&nbsp; Modeling biological soft tissue is complex in part due to material<br>
heterogeneity. Microstructural patterns, which play a major role in defining<br>
the mechanical behavior of these tissues, are both challenging to characterize,<br>
and difficult to simulate. Recently, machine learning-based methods to predict<br>
the mechanical behavior of heterogeneous materials have made it possible to<br>
more thoroughly explore the massive input parameter space associated with<br>
heterogeneous blocks of material. Specifically, we can train machine learning<br>
(ML) models to closely approximate computationally expensive heterogeneous<br>
material simulations where the ML model is trained on a dataset of simulations<br>
that capture the range of spatial heterogeneity present in the material of<br>
interest. However, when it comes to applying these techniques to biological<br>
tissue more broadly, there is a major limitation: the relevant microstructural<br>
patterns are both challenging to obtain and difficult to analyze. Consequently,<br>
the number of useful examples available to characterize the input domain under<br>
study is limited. In this work, we investigate the efficacy of ML-based<br>
generative models as a tool for augmenting limited input pattern datasets. We<br>
find that a Style-based Generative Adversarial Network with an adaptive<br>
discriminator augmentation mechanism is able to successfully leverage just<br>
1,000 example patterns to create meaningful generated patterns that can be used<br>
as inputs to finite element simulations to augment the training dataset. To<br>
enable this methodological contribution, we have created an open access dataset<br>
of Finite Element Analysis simulations based on Cahn-Hilliard patterns. We<br>
anticipate that future researchers will be able to leverage this dataset and<br>
build on the work presented here.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04183" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04183&amp;source=gmail&amp;ust=1646963617846000&amp;usg=AOvVaw0mMh070lf5Tk6-zdicmxCn" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04183</a> ,&nbsp; 39628kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04192<br>
Date: Tue, 8 Mar 2022 16:33:36 GMT&nbsp; &nbsp;(8184kb)<br>
<br>
Title: Neural Contextual Bandits via Reward-Biased Maximum Likelihood<br>
&nbsp; Estimation<br>
Authors: Yu-Heng Hung, Ping-Chun Hsieh<br>
Categories: cs.LG stat.ML<br>
\\<br>
&nbsp; Reward-biased maximum likelihood estimation (RBMLE) is a classic principle in<br>
the adaptive control literature for tackling explore-exploit trade-offs. This<br>
paper studies the stochastic contextual bandit problem with general bounded<br>
reward functions and proposes NeuralRBMLE, which adapts the RBMLE principle by<br>
adding a bias term to the log-likelihood to enforce exploration. NeuralRBMLE<br>
leverages the representation power of neural networks and directly encodes<br>
exploratory behavior in the parameter space, without constructing confidence<br>
intervals of the estimated rewards. We propose two variants of NeuralRBMLE<br>
algorithms: The first variant directly obtains the RBMLE estimator by gradient<br>
ascent, and the second variant simplifies RBMLE to a simple index policy<br>
through an approximation. We show that both algorithms achieve<br>
$\widetilde{\mathcal{O}}(\<wbr>sqrt{T})$ regret. Through extensive experiments, we<br>
demonstrate that the NeuralRBMLE algorithms achieve comparable or better<br>
empirical regrets than the state-of-the-art methods on real-world datasets with<br>
non-linear reward functions.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04192" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04192&amp;source=gmail&amp;ust=1646963617846000&amp;usg=AOvVaw2KZ-bET-2IJC1vaELXyCt4" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04192</a> ,&nbsp; 8184kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04195<br>
Date: Tue, 8 Mar 2022 16:41:06 GMT&nbsp; &nbsp;(12591kb,D)<br>
<br>
Title: A Gating Model for Bias Calibration in Generalized Zero-shot Learning<br>
Authors: Gukyeong Kwon, Ghassan AlRegib<br>
Categories: cs.LG cs.CV<br>
Comments: IEEE Transactions on Image Processing, 2022. Code is available at<br>
&nbsp; <a href="https://github.com/gukyeongkwon/gating-ae" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/gukyeongkwon/gating-ae&amp;source=gmail&amp;ust=1646963617846000&amp;usg=AOvVaw1Fs0D9ysIcOTzjtHFXW4It" rel="noreferrer" target="_blank">https://github.com/<wbr>gukyeongkwon/gating-ae</a><br>
\\<br>
&nbsp; Generalized zero-shot learning (GZSL) aims at training a model that can<br>
generalize to unseen class data by only using auxiliary information. One of the<br>
main challenges in GZSL is a biased model prediction toward seen classes caused<br>
by overfitting on only available seen class data during training. To overcome<br>
this issue, we propose a two-stream autoencoder-based gating model for GZSL.<br>
Our gating model predicts whether the query data is from seen classes or unseen<br>
classes, and utilizes separate seen and unseen experts to predict the class<br>
independently from each other. This framework avoids comparing the biased<br>
prediction scores for seen classes with the prediction scores for unseen<br>
classes. In particular, we measure the distance between visual and attribute<br>
representations in the latent space and the cross-reconstruction space of the<br>
autoencoder. These distances are utilized as complementary features to<br>
characterize unseen classes at different levels of data abstraction. Also, the<br>
two-stream autoencoder works as a unified framework for the gating model and<br>
the unseen expert, which makes the proposed method computationally efficient.<br>
We validate our proposed method in four benchmark image recognition datasets.<br>
In comparison with other state-of-the-art methods, we achieve the best harmonic<br>
mean accuracy in SUN and AWA2, and the second best in CUB and AWA1.<br>
Furthermore, our base model requires at least 20% less number of model<br>
parameters than state-of-the-art methods relying on generative models.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04195" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04195&amp;source=gmail&amp;ust=1646963617846000&amp;usg=AOvVaw3kR4Mt4eE597RglLBxvZaf" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04195</a> ,&nbsp; 12591kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04199<br>
Date: Tue, 8 Mar 2022 16:57:00 GMT&nbsp; &nbsp;(4778kb,D)<br>
<br>
Title: Trustable Co-label Learning from Multiple Noisy Annotators<br>
Authors: Shikun Li, Tongliang Liu, Jiyong Tan, Dan Zeng, Shiming Ge<br>
Categories: cs.LG cs.AI cs.CV<br>
Comments: Accepted by IEEE TMM. 13 pages, 9 figures and 6 tables<br>
DOI: 10.1109/TMM.2021.3137752<br>
\\<br>
&nbsp; Supervised deep learning depends on massive accurately annotated examples,<br>
which is usually impractical in many real-world scenarios. A typical<br>
alternative is learning from multiple noisy annotators. Numerous earlier works<br>
assume that all labels are noisy, while it is usually the case that a few<br>
trusted samples with clean labels are available. This raises the following<br>
important question: how can we effectively use a small amount of trusted data<br>
to facilitate robust classifier learning from multiple annotators? This paper<br>
proposes a data-efficient approach, called \emph{Trustable Co-label Learning}<br>
(TCL), to learn deep classifiers from multiple noisy annotators when a small<br>
set of trusted data is available. This approach follows the coupled-view<br>
learning manner, which jointly learns the data classifier and the label<br>
aggregator. It effectively uses trusted data as a guide to generate trustable<br>
soft labels (termed co-labels). A co-label learning can then be performed by<br>
alternately reannotating the pseudo labels and refining the classifiers. In<br>
addition, we further improve TCL for a special complete data case, where each<br>
instance is labeled by all annotators and the label aggregator is represented<br>
by multilayer neural networks to enhance model capacity. Extensive experiments<br>
on synthetic and real datasets clearly demonstrate the effectiveness and<br>
robustness of the proposed approach. Source code is available at<br>
<a href="https://github.com/ShikunLi/TCL" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/ShikunLi/TCL&amp;source=gmail&amp;ust=1646963617847000&amp;usg=AOvVaw3Ev1dICQNnPpZGdKhfpg_b" rel="noreferrer" target="_blank">https://github.com/ShikunLi/<wbr>TCL</a><br>
\\ ( <a href="https://arxiv.org/abs/2203.04199" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04199&amp;source=gmail&amp;ust=1646963617847000&amp;usg=AOvVaw0nSkoEx7nIpxw4wZF4H7co" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04199</a> ,&nbsp; 4778kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04236<br>
Date: Tue, 8 Mar 2022 17:52:57 GMT&nbsp; &nbsp;(56kb,D)<br>
<br>
Title: A Sharp Characterization of Linear Estimators for Offline Policy<br>
&nbsp; Evaluation<br>
Authors: Juan C. Perdomo, Akshay Krishnamurthy, Peter Bartlett, Sham Kakade<br>
Categories: cs.LG stat.ML<br>
\\<br>
&nbsp; Offline policy evaluation is a fundamental statistical problem in<br>
reinforcement learning that involves estimating the value function of some<br>
decision-making policy given data collected by a potentially different policy.<br>
In order to tackle problems with complex, high-dimensional observations, there<br>
has been significant interest from theoreticians and practitioners alike in<br>
understanding the possibility of function approximation in reinforcement<br>
learning. Despite significant study, a sharp characterization of when we might<br>
expect offline policy evaluation to be tractable, even in the simplest setting<br>
of linear function approximation, has so far remained elusive, with a<br>
surprising number of strong negative results recently appearing in the<br>
literature.<br>
&nbsp; In this work, we identify simple control-theoretic and linear-algebraic<br>
conditions that are necessary and sufficient for classical methods, in<br>
particular Fitted Q-iteration (FQI) and least squares temporal difference<br>
learning (LSTD), to succeed at offline policy evaluation. Using this<br>
characterization, we establish a precise hierarchy of regimes under which these<br>
estimators succeed. We prove that LSTD works under strictly weaker conditions<br>
than FQI. Furthermore, we establish that if a problem is not solvable via LSTD,<br>
then it cannot be solved by a broad class of linear estimators, even in the<br>
limit of infinite data. Taken together, our results provide a complete picture<br>
of the behavior of linear estimators for offline policy evaluation (OPE), unify<br>
previously disparate analyses of canonical algorithms, and provide<br>
significantly sharper notions of the underlying statistical complexity of OPE.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04236" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04236&amp;source=gmail&amp;ust=1646963617847000&amp;usg=AOvVaw2dHmS-ucnqeR-I9ou5kP8p" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04236</a> ,&nbsp; 56kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04248<br>
Date: Tue, 8 Mar 2022 18:06:26 GMT&nbsp; &nbsp;(638kb,D)<br>
<br>
Title: Dual Lottery Ticket Hypothesis<br>
Authors: Yue Bai, Huan Wang, Zhiqiang Tao, Kunpeng Li, Yun Fu<br>
Categories: cs.LG<br>
\\<br>
&nbsp; Fully exploiting the learning capacity of neural networks requires<br>
overparameterized dense networks. On the other side, directly training sparse<br>
neural networks typically results in unsatisfactory performance. Lottery Ticket<br>
Hypothesis (LTH) provides a novel view to investigate sparse network training<br>
and maintain its capacity. Concretely, it claims there exist winning tickets<br>
from a randomly initialized network found by iterative magnitude pruning and<br>
preserving promising trainability (or we say being in trainable condition). In<br>
this work, we regard the winning ticket from LTH as the subnetwork which is in<br>
trainable condition and its performance as our benchmark, then go from a<br>
complementary direction to articulate the Dual Lottery Ticket Hypothesis<br>
(DLTH): Randomly selected subnetworks from a randomly initialized dense network<br>
can be transformed into a trainable condition and achieve admirable performance<br>
compared with LTH -- random tickets in a given lottery pool can be transformed<br>
into winning tickets. Specifically, by using uniform-randomly selected<br>
subnetworks to represent the general cases, we propose a simple sparse network<br>
training strategy, Random Sparse Network Transformation (RST), to substantiate<br>
our DLTH. Concretely, we introduce a regularization term to borrow learning<br>
capacity and realize information extrusion from the weights which will be<br>
masked. After finishing the transformation for the randomly selected<br>
subnetworks, we conduct the regular finetuning to evaluate the model using fair<br>
comparisons with LTH and other strong baselines. Extensive experiments on<br>
several public datasets and comparisons with competitive approaches validate<br>
our DLTH as well as the effectiveness of the proposed model RST. Our work is<br>
expected to pave a way for inspiring new research directions of sparse network<br>
training in the future. Our code is available at<br>
<a href="https://github.com/yueb17/DLTH" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/yueb17/DLTH&amp;source=gmail&amp;ust=1646963617847000&amp;usg=AOvVaw1jagZcpbpii_EKiGzBjyyk" rel="noreferrer" target="_blank">https://github.com/yueb17/DLTH</a><wbr>.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04248" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04248&amp;source=gmail&amp;ust=1646963617847000&amp;usg=AOvVaw2fO-Id_0Z0FiSBXovVjee5" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04248</a> ,&nbsp; 638kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04272<br>
Date: Tue, 8 Mar 2022 18:47:01 GMT&nbsp; &nbsp;(644kb,D)<br>
<br>
Title: Policy-Based Bayesian Experimental Design for Non-Differentiable<br>
&nbsp; Implicit Models<br>
Authors: Vincent Lim, Ellen Novoseller, Jeffrey Ichnowski, Huang Huang, Ken<br>
&nbsp; Goldberg<br>
Categories: cs.LG cs.AI stat.ME<br>
Comments: 15 pages, 3 figures<br>
\\<br>
&nbsp; For applications in healthcare, physics, energy, robotics, and many other<br>
fields, designing maximally informative experiments is valuable, particularly<br>
when experiments are expensive, time-consuming, or pose safety hazards. While<br>
existing approaches can sequentially design experiments based on prior<br>
observation history, many of these methods do not extend to implicit models,<br>
where simulation is possible but computing the likelihood is intractable.<br>
Furthermore, they often require either significant online computation during<br>
deployment or a differentiable simulation system. We introduce Reinforcement<br>
Learning for Deep Adaptive Design (RL-DAD), a method for simulation-based<br>
optimal experimental design for non-differentiable implicit models. RL-DAD<br>
extends prior work in policy-based Bayesian Optimal Experimental Design (BOED)<br>
by reformulating it as a Markov Decision Process with a reward function based<br>
on likelihood-free information lower bounds, which is used to learn a policy<br>
via deep reinforcement learning. The learned design policy maps prior histories<br>
to experiment designs offline and can be quickly deployed during online<br>
execution. We evaluate RL-DAD and find that it performs competitively with<br>
baselines on three benchmarks.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04272" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04272&amp;source=gmail&amp;ust=1646963617848000&amp;usg=AOvVaw3FlOED1TRtiiIIGqnJjPAs" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04272</a> ,&nbsp; 644kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04274<br>
Date: Tue, 8 Mar 2022 18:48:55 GMT&nbsp; &nbsp;(67kb)<br>
<br>
Title: Leveraging Initial Hints for Free in Stochastic Linear Bandits<br>
Authors: Ashok Cutkosky, Chris Dann, Abhimanyu Das, Qiuyi (Richard) Zhang<br>
Categories: cs.LG cs.DS<br>
Comments: ALT 2022<br>
\\<br>
&nbsp; We study the setting of optimizing with bandit feedback with additional prior<br>
knowledge provided to the learner in the form of an initial hint of the optimal<br>
action. We present a novel algorithm for stochastic linear bandits that uses<br>
this hint to improve its regret to $\tilde O(\sqrt{T})$ when the hint is<br>
accurate, while maintaining a minimax-optimal $\tilde O(d\sqrt{T})$ regret<br>
independent of the quality of the hint. Furthermore, we provide a Pareto<br>
frontier of tight tradeoffs between best-case and worst-case regret, with<br>
matching lower bounds. Perhaps surprisingly, our work shows that leveraging a<br>
hint shows provable gains without sacrificing worst-case performance, implying<br>
that our algorithm adapts to the quality of the hint for free. We also provide<br>
an extension of our algorithm to the case of $m$ initial hints, showing that we<br>
can achieve a $\tilde O(m^{2/3}\sqrt{T})$ regret.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04274" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04274&amp;source=gmail&amp;ust=1646963617848000&amp;usg=AOvVaw0uW1aFnztwV_pgnFiHnhtt" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04274</a> ,&nbsp; 67kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04026<br>
Date: Tue, 8 Mar 2022 11:40:23 GMT&nbsp; &nbsp;(2853kb,D)<br>
<br>
Title: Toward Understanding Deep Learning Framework Bugs<br>
Authors: Junjie Chen, Yihua Liang, Qingchao Shen, Jiajun Jiang<br>
Categories: cs.SE<br>
\\<br>
&nbsp; DL frameworks are the basis of constructing all DL programs and models, and<br>
thus their bugs could lead to the unexpected behaviors of any DL program or<br>
model relying on them. Such wide effect demonstrates the necessity and<br>
importance of guaranteeing DL frameworks' quality. Understanding the<br>
characteristics of DL framework bugs is a fundamental step for this quality<br>
assurance task, facilitating to design effective bug detection and debugging<br>
approaches. Hence, in this work we conduct the most large-scale study on 800<br>
bugs from four popular and diverse DL frameworks (i.e., TensorFlow, PyTorch,<br>
MXNet, and DL4J). By analyzing the root causes and symptoms of DL framework<br>
bugs associated with 5 components decomposed from DL frameworks, as well as<br>
measuring test coverage achieved by three state-of-the-art testing techniques<br>
and developers' efforts on fixing those bugs, we obtain 14 major findings for<br>
the comprehensive understanding of DL framework bugs and the current status of<br>
existing DL framework testing and debugging practice, and then provide a series<br>
of actionable guidelines for better DL framework bug detection and debugging.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04026" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04026&amp;source=gmail&amp;ust=1646963617848000&amp;usg=AOvVaw2Wyr0LHKyPLHOM6TmlS7ju" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04026</a> ,&nbsp; 2853kb)<br>
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-<wbr>%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-<wbr>%-%-%-%-%-%-%-%-%-<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03616 (*cross-listing*)<br>
Date: Sat, 19 Feb 2022 14:48:22 GMT&nbsp; &nbsp;(31kb,D)<br>
<br>
Title: Responsible AI in Healthcare<br>
Authors: Federico Cabitza, Davide Ciucci, Gabriella Pasi, Marco Viviani<br>
Categories: cs.CY cs.AI cs.IR cs.LG<br>
Comments: 5 pages, 0 figures<br>
\\<br>
&nbsp; This article discusses open problems, implemented solutions, and future<br>
research in the area of responsible AI in healthcare. In particular, we<br>
illustrate two main research themes related to the work of two laboratories<br>
within the Department of Informatics, Systems, and Communication at the<br>
University of Milano-Bicocca. The problems addressed concern, in particular,<br>
{uncertainty in medical data and machine advice}, and the problem of online<br>
health information disorder.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03616" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03616&amp;source=gmail&amp;ust=1646963617848000&amp;usg=AOvVaw0VRoMR7_ww8g-nNJiItsvh" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03616</a> ,&nbsp; 31kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03621 (*cross-listing*)<br>
Date: Sat, 5 Mar 2022 04:39:42 GMT&nbsp; &nbsp;(666kb)<br>
<br>
Title: Triple Motion Estimation and Frame Interpolation based on Adaptive<br>
&nbsp; Threshold for Frame Rate Up-Conversion<br>
Authors: Hanieh Naderi, Mohammad Rahmati<br>
Categories: eess.IV cs.AI cs.CV cs.MM<br>
Comments: Frame rate up-conversion, frame interpolation, motion estimation,<br>
&nbsp; motion compensation<br>
\\<br>
&nbsp; In this paper, we propose a novel motion-compensated frame rate up-conversion<br>
(MC-FRUC) algorithm. The proposed algorithm creates interpolated frames by<br>
first estimating motion vectors using unilateral (jointing forward and<br>
backward) and bilateral motion estimation. Then motion vectors are combined<br>
based on adaptive threshold, in order to creates high-quality interpolated<br>
frames and reduce block artifacts. Since motion-compensated frame interpolation<br>
along unilateral motion trajectories yields holes, a new algorithm is<br>
introduced to resolve this problem. The experimental results show that the<br>
quality of the interpolated frames using the proposed algorithm is much higher<br>
than the existing algorithms.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03621" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03621&amp;source=gmail&amp;ust=1646963617848000&amp;usg=AOvVaw1UUJhJtoNqpXzrevbqe4wy" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03621</a> ,&nbsp; 666kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03638 (*cross-listing*)<br>
Date: Mon, 7 Mar 2022 12:54:33 GMT&nbsp; &nbsp;(816kb)<br>
<br>
Title: Unsupervised Image Registration Towards Enhancing Performance and<br>
&nbsp; Explainability in Cardiac And Brain Image Analysis<br>
Authors: Chengjia Wang, Guang Yang, Giorgos Papanastasiou<br>
Categories: eess.IV cs.AI cs.CV cs.LG<br>
Comments: 38 pages, 7 figures, will be published in Sensors journal by MDPI<br>
\\<br>
&nbsp; Magnetic Resonance Imaging (MRI) typically recruits multiple sequences<br>
(defined here as "modalities"). As each modality is designed to offer different<br>
anatomical and functional clinical information, there are evident disparities<br>
in the imaging content across modalities. Inter- and intra-modality affine and<br>
non-rigid image registration is an essential medical image analysis process in<br>
clinical imaging, as for example before imaging biomarkers need to be derived<br>
and clinically evaluated across different MRI modalities, time phases and<br>
slices. Although commonly needed in real clinical scenarios, affine and<br>
non-rigid image registration is not extensively investigated using a single<br>
unsupervised model architecture. In our work, we present an un-supervised deep<br>
learning registration methodology which can accurately model affine and<br>
non-rigid trans-formations, simultaneously. Moreover, inverse-consistency is a<br>
fundamental inter-modality registration property that is not considered in deep<br>
learning registration algorithms. To address inverse-consistency, our<br>
methodology performs bi-directional cross-modality image synthesis to learn<br>
modality-invariant latent rep-resentations, while involves two factorised<br>
transformation networks and an inverse-consistency loss to learn<br>
topology-preserving anatomical transformations. Overall, our model (named<br>
"FIRE") shows improved performances against the reference standard baseline<br>
method on multi-modality brain 2D and 3D MRI and intra-modality cardiac 4D MRI<br>
data experiments.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03638" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03638&amp;source=gmail&amp;ust=1646963617848000&amp;usg=AOvVaw0joftYGh21zFTWmCha7MZB" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03638</a> ,&nbsp; 816kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03682 (*cross-listing*)<br>
Date: Mon, 7 Mar 2022 19:47:52 GMT&nbsp; &nbsp;(15455kb,D)<br>
<br>
Title: Monocular Robot Navigation with Self-Supervised Pretrained Vision<br>
&nbsp; Transformers<br>
Authors: Miguel Saavedra-Ruiz, Sacha Morin and Liam Paull<br>
Categories: cs.RO cs.AI cs.CV<br>
\\<br>
&nbsp; In this work, we consider the problem of learning a perception model for<br>
monocular robot navigation using few annotated images. Using a Vision<br>
Transformer (ViT) pretrained with a label-free self-supervised method, we<br>
successfully train a coarse image segmentation model for the Duckietown<br>
environment using 70 training images. Our model performs coarse image<br>
segmentation at the 8x8 patch level, and the inference resolution can be<br>
adjusted to balance prediction granularity and real-time perception<br>
constraints. We study how best to adapt a ViT to our task and environment, and<br>
find that some lightweight architectures can yield good single-image<br>
segmentations at a usable frame rate, even on CPU. The resulting perception<br>
model is used as the backbone for a simple yet robust visual servoing agent,<br>
which we deploy on a differential drive mobile robot to perform two tasks: lane<br>
following and obstacle avoidance.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03682" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03682&amp;source=gmail&amp;ust=1646963617849000&amp;usg=AOvVaw3GB-AAIP8GDxS5ljTuj6op" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03682</a> ,&nbsp; 15455kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03691 (*cross-listing*)<br>
Date: Mon, 7 Mar 2022 20:23:46 GMT&nbsp; &nbsp;(218kb,D)<br>
<br>
Title: HyperMixer: An MLP-based Green AI Alternative to Transformers<br>
Authors: Florian Mai, Arnaud Pannatier, Fabio Fehr, Haolin Chen, Francois<br>
&nbsp; Marelli, Francois Fleuret, James Henderson<br>
Categories: cs.CL cs.AI cs.LG<br>
\\<br>
&nbsp; Transformer-based architectures are the model of choice for natural language<br>
understanding, but they come at a significant cost, as they have quadratic<br>
complexity in the input length and can be difficult to tune. In the pursuit of<br>
Green AI, we investigate simple MLP-based architectures. We find that existing<br>
architectures such as MLPMixer, which achieves token mixing through a static<br>
MLP applied to each feature independently, are too detached from the inductive<br>
biases required for natural language understanding. In this paper, we propose a<br>
simple variant, HyperMixer, which forms the token mixing MLP dynamically using<br>
hypernetworks. Empirically, we demonstrate that our model performs better than<br>
alternative MLP-based models, and on par with Transformers. In contrast to<br>
Transformers, HyperMixer achieves these results at substantially lower costs in<br>
terms of processing time, training data, and hyperparameter tuning.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03691" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03691&amp;source=gmail&amp;ust=1646963617849000&amp;usg=AOvVaw3KNYz9d8-bS3loncBwE62F" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03691</a> ,&nbsp; 218kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03711 (*cross-listing*)<br>
Date: Tue, 15 Feb 2022 14:20:53 GMT&nbsp; &nbsp;(48kb,D)<br>
<br>
Title: Algorithmic audits of algorithms, and the law<br>
Authors: Erwan Le Merrer and Ronan Pons and Gilles Tr\'edan<br>
Categories: cs.CY cs.AI<br>
\\<br>
&nbsp; Algorithmic decision making is now widespread, ranging from health care<br>
allocation to more common actions such as recommendation or information<br>
ranking. The aim to audit these algorithms has grown alongside. In this paper,<br>
we focus on external audits that are conducted by interacting with the user<br>
side of the target algorithm, hence considered as a black box. Yet, the legal<br>
framework in which these audits take place is mostly ambiguous to researchers<br>
developing them: on the one hand, the legal value of the audit outcome is<br>
uncertain; on the other hand the auditors' rights and obligations are unclear.<br>
The contribution of this paper is to articulate two canonical audit forms to<br>
law, to shed light on these aspects: 1) the first audit form (we coin the Bobby<br>
audit form) checks a predicate against the algorithm, while the second<br>
(Sherlock) is more loose and opens up to multiple investigations. We find that:<br>
Bobby audits are more amenable to prosecution, yet are delicate as operating on<br>
real user data. This can lead to reject by a court (notion of admissibility).<br>
Sherlock audits craft data for their operation, most notably to build<br>
surrogates of the audited algorithm. It is mostly used for acts for<br>
whistleblowing, as even if accepted as a proof, the evidential value will be<br>
low in practice. 2) these two forms require the prior respect of a proper right<br>
to audit, granted by law or by the platform being audited; otherwise the<br>
auditor will be also prone to prosecutions regardless of the audit outcome.<br>
This article thus highlights the relation of current audits with law, in order<br>
to structure the growing field of algorithm auditing.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03711" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03711&amp;source=gmail&amp;ust=1646963617849000&amp;usg=AOvVaw0dH-TlikcM9eTu21Zn4ART" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03711</a> ,&nbsp; 48kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03712 (*cross-listing*)<br>
Date: Wed, 16 Feb 2022 11:56:41 GMT&nbsp; &nbsp;(511kb)<br>
<br>
Title: Trusted Data Forever: Is AI the Answer?<br>
Authors: Emanuele Frontoni, Marina Paolanti, Tracey P. Lauriault, Michael<br>
&nbsp; Stiber, Luciana Duranti, Abdul-Mageed Muhammad<br>
Categories: cs.CY cs.AI<br>
\\<br>
&nbsp; Archival institutions and programs worldwide work to ensure that the records<br>
of governments, organizations, communities, and individuals are preserved for<br>
future generations as cultural heritage, as sources of rights, and as vehicles<br>
for holding the past accountable and to inform the future. This commitment is<br>
guaranteed through the adoption of strategic and technical measures for the<br>
long-term preservation of digital assets in any medium and form - textual,<br>
visual, or aural. Public and private archives are the largest providers of data<br>
big and small in the world and collectively host yottabytes of trusted data, to<br>
be preserved forever. Several aspects of retention and preservation,<br>
arrangement and description, management and administrations, and access and use<br>
are still open to improvement. In particular, recent advances in Artificial<br>
Intelligence (AI) open the discussion as to whether AI can support the ongoing<br>
availability and accessibility of trustworthy public records. This paper<br>
presents preliminary results of the InterPARES Trust AI (I Trust AI)<br>
international research partnership, which aims to (1) identify and develop<br>
specific AI technologies to address critical records and archives challenges;<br>
(2) determine the benefits and risks of employing AI technologies on records<br>
and archives; (3) ensure that archival concepts and principles inform the<br>
development of responsible AI; and (4) validate outcomes through a conglomerate<br>
of case studies and demonstrations.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03712" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03712&amp;source=gmail&amp;ust=1646963617849000&amp;usg=AOvVaw0qu71boR7j-22Su_wzzRZY" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03712</a> ,&nbsp; 511kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03715 (*cross-listing*)<br>
Date: Fri, 18 Feb 2022 15:16:22 GMT&nbsp; &nbsp;(1402kb)<br>
<br>
Title: Needs and Artificial Intelligence<br>
Authors: Soheil Human and Ryan Watkins<br>
Categories: cs.CY cs.AI<br>
\\<br>
&nbsp; Throughout their history, homo sapiens have used technologies to better<br>
satisfy their needs. The relation between needs and technology is so<br>
fundamental that the US National Research Council defined the distinguishing<br>
characteristic of technology as its goal "to make modifications in the world to<br>
meet human needs". Artificial intelligence (AI) is one of the most promising<br>
emerging technologies of our time. Similar to other technologies, AI is<br>
expected "to meet [human] needs". In this article, we reflect on the<br>
relationship between needs and AI, and call for the realisation of needs-aware<br>
AI systems. We argue that re-thinking needs for, through, and by AI can be a<br>
very useful means towards the development of realistic approaches for<br>
Sustainable, Human-centric, Accountable, Lawful, and Ethical (HALE) AI systems.<br>
We discuss some of the most critical gaps, barriers, enablers, and drivers of<br>
co-creating future AI-based socio-technical systems in which [human] needs are<br>
well considered and met. Finally, we provide an overview of potential threats<br>
and HALE considerations that should be carefully taken into account, and call<br>
for joint, immediate, and interdisciplinary efforts and collaborations.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03715" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03715&amp;source=gmail&amp;ust=1646963617849000&amp;usg=AOvVaw3H-66p9dxJMWSQ7KkdkTUd" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03715</a> ,&nbsp; 1402kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03718 (*cross-listing*)<br>
Date: Tue, 22 Feb 2022 15:12:58 GMT&nbsp; &nbsp;(29kb)<br>
<br>
Title: Towards User-Centered Metrics for Trustworthy AI in Immersive Cyberspace<br>
Authors: Pengyuan Zhou, Benjamin Finley, Lik-Hang Lee, Yong Liao, Haiyong Xie,<br>
&nbsp; Pan Hui<br>
Categories: cs.CY cs.AI<br>
\\<br>
&nbsp; AI plays a key role in current cyberspace and future immersive ecosystems<br>
that pinpoint user experiences. Thus, the trustworthiness of such AI systems is<br>
vital as failures in these systems can cause serious user harm. Although there<br>
are related works on exploring trustworthy AI (TAI) metrics in the current<br>
cyberspace, ecosystems towards user-centered services, such as the metaverse,<br>
are much more complicated in terms of system performance and user experience<br>
assessment, thus posing challenges for the applicability of existing<br>
approaches. Thus, we give an overlook on fairness, privacy and robustness,<br>
across the historical path from existing approaches. Eventually, we propose a<br>
research agenda towards systematic yet user-centered TAI in immersive<br>
ecosystems.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03718" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03718&amp;source=gmail&amp;ust=1646963617850000&amp;usg=AOvVaw0m6eQVwwglXqg60yA-3AD3" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03718</a> ,&nbsp; 29kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03724 (*cross-listing*)<br>
Date: Thu, 3 Mar 2022 18:55:33 GMT&nbsp; &nbsp;(1176kb,D)<br>
<br>
Title: A New Era: Intelligent Tutoring Systems Will Transform Online Learning<br>
&nbsp; for Millions<br>
Authors: Francois St-Hilaire, Dung Do Vu, Antoine Frau, Nathan Burns, Farid<br>
&nbsp; Faraji, Joseph Potochny, Stephane Robert, Arnaud Roussel, Selene Zheng,<br>
&nbsp; Taylor Glazier, Junfel Vincent Romano, Robert Belfer, Muhammad Shayan,<br>
&nbsp; Ariella Smofsky, Tommy Delarosbil, Seulmin Ahn, Simon Eden-Walker, Kritika<br>
&nbsp; Sony, Ansona Onyi Ching, Sabina Elkins, Anush Stepanyan, Adela Matajova,<br>
&nbsp; Victor Chen, Hossein Sahraei, Robert Larson, Nadia Markova, Andrew Barkett,<br>
&nbsp; Laurent Charlin, Yoshua Bengio, Iulian Vlad Serban, Ekaterina Kochmar<br>
Categories: cs.CY cs.AI cs.HC cs.LG<br>
Comments: 9 pages, 6 figures<br>
ACM-class: I.2.0; K.3.1; K.4.0<br>
\\<br>
&nbsp; Despite artificial intelligence (AI) having transformed major aspects of our<br>
society, less than a fraction of its potential has been explored, let alone<br>
deployed, for education. AI-powered learning can provide millions of learners<br>
with a highly personalized, active and practical learning experience, which is<br>
key to successful learning. This is especially relevant in the context of<br>
online learning platforms. In this paper, we present the results of a<br>
comparative head-to-head study on learning outcomes for two popular online<br>
learning platforms (n=199 participants): A MOOC platform following a<br>
traditional model delivering content using lecture videos and multiple-choice<br>
quizzes, and the Korbit learning platform providing a highly personalized,<br>
active and practical learning experience. We observe a huge and statistically<br>
significant increase in the learning outcomes, with students on the Korbit<br>
platform providing full feedback resulting in higher course completion rates<br>
and achieving learning gains 2 to 2.5 times higher than both students on the<br>
MOOC platform and students in a control group who don't receive personalized<br>
feedback on the Korbit platform. The results demonstrate the tremendous impact<br>
that can be achieved with a personalized, active learning AI-powered system.<br>
Making this technology and learning experience available to millions of<br>
learners around the world will represent a significant leap forward towards the<br>
democratization of education.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03724" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03724&amp;source=gmail&amp;ust=1646963617850000&amp;usg=AOvVaw0gQUWui050Yh04U_YB6yff" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03724</a> ,&nbsp; 1176kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03737 (*cross-listing*)<br>
Date: Mon, 7 Mar 2022 21:56:17 GMT&nbsp; &nbsp;(6289kb,D)<br>
<br>
Title: Battery Cloud with Advanced Algorithms<br>
Authors: Xiaojun Li, David Jauernig, Mengzhu Gao, Trevor Jones<br>
Categories: eess.SY cs.AI cs.SY<br>
\\<br>
&nbsp; A Battery Cloud or cloud battery management system leverages the cloud<br>
computational power and data storage to improve battery safety, performance,<br>
and economy. This work will present the Battery Cloud that collects measured<br>
battery data from electric vehicles and energy storage systems. Advanced<br>
algorithms are applied to improve battery performance. Using remote vehicle<br>
data, we train and validate an artificial neural network to estimate pack SOC<br>
during vehicle charging. The strategy is then tested on vehicles. Furthermore,<br>
high accuracy and onboard battery state of health estimation methods for<br>
electric vehicles are developed based on the differential voltage (DVA) and<br>
incremental capacity analysis (ICA). Using cycling data from battery cells at<br>
various temperatures, we extract the charging cycles and calculate the DVA and<br>
ICA curves, from which multiple features are extracted, analyzed, and<br>
eventually used to estimate the state of health. For battery safety, a<br>
data-driven thermal anomaly detection method is developed. The method can<br>
detect unforeseen anomalies such as thermal runaways at the very early stage.<br>
With the further development of the internet of things, more and more battery<br>
data will be available. Potential applications of battery cloud also include<br>
areas such as battery manufacture, recycling, and electric vehicle battery<br>
swap.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03737" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03737&amp;source=gmail&amp;ust=1646963617850000&amp;usg=AOvVaw3TJjI5truH7LfxiDuM51fs" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03737</a> ,&nbsp; 6289kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03847 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 04:38:34 GMT&nbsp; &nbsp;(512kb)<br>
<br>
Title: Trust in AI and Implications for the AEC Research: A Literature Analysis<br>
Authors: Newsha Emaminejad, Alexa Maria North, and Reza Akhavian<br>
Categories: cs.HC cs.AI cs.RO<br>
Comments: 2021 ASCE International Conference on Computing in Civil Engineering<br>
&nbsp; (i3CE2021)<br>
\\<br>
&nbsp; Engendering trust in technically acceptable and psychologically embraceable<br>
systems requires domain-specific research to capture unique characteristics of<br>
the field of application. The architecture, engineering, and construction (AEC)<br>
research community has been recently harnessing advanced solutions offered by<br>
artificial intelligence (AI) to improve project workflows. Despite the unique<br>
characteristics of work, workers, and workplaces in the AEC industry, the<br>
concept of trust in AI has received very little attention in the literature.<br>
This paper presents a comprehensive analysis of the academic literature in two<br>
main areas of trust in AI and AI in the AEC, to explore the interplay between<br>
AEC projects unique aspects and the sociotechnical concepts that lead to trust<br>
in AI. A total of 490 peer-reviewed scholarly articles are analyzed in this<br>
study. The main constituents of human trust in AI are identified from the<br>
literature and are characterized within the AEC project types, processes, and<br>
technologies.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03847" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03847&amp;source=gmail&amp;ust=1646963617850000&amp;usg=AOvVaw1w7wMS7oX1zeXseXtWHQis" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03847</a> ,&nbsp; 512kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03876 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 06:45:31 GMT&nbsp; &nbsp;(422kb)<br>
<br>
Title: High-order Order Proximity-Incorporated, Symmetry and Graph-Regularized<br>
&nbsp; Nonnegative Matrix Factorization for Community Detection<br>
Authors: Zhigang Liu and Xin Luo<br>
Categories: cs.SI cs.AI cs.LG<br>
Comments: 7 pages<br>
\\<br>
&nbsp; Community describes the functional mechanism of a network, making community<br>
detection serve as a fundamental graph tool for various real applications like<br>
discovery of social circle. To date, a Symmetric and Non-negative Matrix<br>
Factorization (SNMF) model has been frequently adopted to address this issue<br>
owing to its high interpretability and scalability. However, most existing<br>
SNMF-based community detection methods neglect the high-order connection<br>
patterns in a network. Motivated by this discovery, in this paper, we propose a<br>
High-Order Proximity (HOP)-incorporated, Symmetry and Graph-regularized NMF<br>
(HSGN) model that adopts the following three-fold ideas: a) adopting a weighted<br>
pointwise mutual information (PMI)-based approach to measure the HOP indices<br>
among nodes in a network; b) leveraging an iterative reconstruction scheme to<br>
encode the captured HOP into the network; and c) introducing a symmetry and<br>
graph-regularized NMF algorithm to detect communities accurately. Extensive<br>
empirical studies on eight real-world networks demonstrate that an HSGN-based<br>
community detector significantly outperforms both benchmark and<br>
state-of-the-art community detectors in providing highly-accurate community<br>
detection results.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03876" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03876&amp;source=gmail&amp;ust=1646963617850000&amp;usg=AOvVaw0WM3gW-ukoI7H-P51ZDxxt" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03876</a> ,&nbsp; 422kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03910 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 08:08:45 GMT&nbsp; &nbsp;(265kb,D)<br>
<br>
Title: Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced<br>
&nbsp; Training for Neural Machine Translation<br>
Authors: Chenze Shao, Yang Feng<br>
Categories: cs.CL cs.AI<br>
Comments: ACL 2022 main conference<br>
ACM-class: I.2.7<br>
\\<br>
&nbsp; Neural networks tend to gradually forget the previously learned knowledge<br>
when learning multiple tasks sequentially from dynamic data distributions. This<br>
problem is called \textit{catastrophic forgetting}, which is a fundamental<br>
challenge in the continual learning of neural networks. In this work, we<br>
observe that catastrophic forgetting not only occurs in continual learning but<br>
also affects the traditional static training. Neural networks, especially<br>
neural machine translation models, suffer from catastrophic forgetting even if<br>
they learn from a static training set. To be specific, the final model pays<br>
imbalanced attention to training samples, where recently exposed samples<br>
attract more attention than earlier samples. The underlying cause is that<br>
training samples do not get balanced training in each model update, so we name<br>
this problem \textit{imbalanced training}. To alleviate this problem, we<br>
propose Complementary Online Knowledge Distillation (COKD), which uses<br>
dynamically updated teacher models trained on specific data orders to<br>
iteratively provide complementary knowledge to the student model. Experimental<br>
results on multiple machine translation tasks show that our method successfully<br>
alleviates the problem of imbalanced training and achieves substantial<br>
improvements over strong baseline systems.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03910" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03910&amp;source=gmail&amp;ust=1646963617851000&amp;usg=AOvVaw27E7UUTFL11EWRxeTMQw21" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03910</a> ,&nbsp; 265kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03944 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 09:14:37 GMT&nbsp; &nbsp;(8934kb,D)<br>
<br>
Title: An Online Semantic Mapping System for Extending and Enhancing Visual<br>
&nbsp; SLAM<br>
Authors: Thorsten Hempel and Ayoub Al-Hamadi<br>
Categories: cs.RO cs.AI cs.CV<br>
Comments: Accepted by Engineering Applications of Artificial Intelligence,<br>
&nbsp; Elsevier, 7 Mar 2022<br>
\\<br>
&nbsp; We present a real-time semantic mapping approach for mobile vision systems<br>
with a 2D to 3D object detection pipeline and rapid data association for<br>
generated landmarks. Besides the semantic map enrichment the associated<br>
detections are further introduced as semantic constraints into a simultaneous<br>
localization and mapping (SLAM) system for pose correction purposes. This way,<br>
we are able generate additional meaningful information that allows to achieve<br>
higher-level tasks, while simultaneously leveraging the view-invariance of<br>
object detections to improve the accuracy and the robustness of the odometry<br>
estimation. We propose tracklets of locally associated object observations to<br>
handle ambiguous and false predictions and an uncertainty-based greedy<br>
association scheme for an accelerated processing time. Our system reaches<br>
real-time capabilities with an average iteration duration of 65~ms and is able<br>
to improve the pose estimation of a state-of-the-art SLAM by up to 68% on a<br>
public dataset. Additionally, we implemented our approach as a modular ROS<br>
package that makes it straightforward for integration in arbitrary graph-based<br>
SLAM methods.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03944" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03944&amp;source=gmail&amp;ust=1646963617851000&amp;usg=AOvVaw2bVytNI-idYmf2cVqsjLKR" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03944</a> ,&nbsp; 8934kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03989 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 10:34:52 GMT&nbsp; &nbsp;(6349kb,D)<br>
<br>
Title: Adapt$\mathcal{O}$r: Objective-Centric Adaptation Framework for Language<br>
&nbsp; Models<br>
Authors: Michal \v{S}tef\'anik, V\'it Novotn\'y, Nikola Groverov\'a and Petr<br>
&nbsp; Sojka<br>
Categories: cs.CL cs.AI cs.LG<br>
Comments: 60th Annual Meeting of the ACL (ACL 2022): System Demonstrations<br>
&nbsp; paper<br>
\\<br>
&nbsp; Progress in natural language processing research is catalyzed by the<br>
possibilities given by the widespread software frameworks. This paper<br>
introduces Adaptor library that transposes the traditional model-centric<br>
approach composed of pre-training + fine-tuning steps to objective-centric<br>
approach, composing the training process by applications of selected<br>
objectives. We survey research directions that can benefit from enhanced<br>
objective-centric experimentation in multitask training, custom objectives<br>
development, dynamic training curricula, or domain adaptation. Adaptor aims to<br>
ease reproducibility of these research directions in practice. Finally, we<br>
demonstrate the practical applicability of Adaptor in selected unsupervised<br>
domain adaptation scenarios.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03989" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03989&amp;source=gmail&amp;ust=1646963617851000&amp;usg=AOvVaw2BI2IZ5ZFJNpC-Uagqdmvt" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03989</a> ,&nbsp; 6349kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04051 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 12:44:42 GMT&nbsp; &nbsp;(17573kb,D)<br>
<br>
Title: Robot Learning of Mobile Manipulation with Reachability Behavior Priors<br>
Authors: Snehal Jauhri, Jan Peters, Georgia Chalvatzaki<br>
Categories: cs.RO cs.AI cs.LG<br>
Comments: Submitted to RAL-IROS 2022<br>
\\<br>
&nbsp; Mobile Manipulation (MM) systems are ideal candidates for taking up the role<br>
of a personal assistant in unstructured real-world environments. Among other<br>
challenges, MM requires effective coordination of the robot's embodiments for<br>
executing tasks that require both mobility and manipulation. Reinforcement<br>
Learning (RL) holds the promise of endowing robots with adaptive behaviors, but<br>
most methods require prohibitively large amounts of data for learning a useful<br>
control policy. In this work, we study the integration of robotic reachability<br>
priors in actor-critic RL methods for accelerating the learning of MM for<br>
reaching and fetching tasks. Namely, we consider the problem of optimal base<br>
placement and the subsequent decision of whether to activate the arm for<br>
reaching a 6D target. For this, we devise a novel Hybrid RL method that handles<br>
discrete and continuous actions jointly, resorting to the Gumbel-Softmax<br>
reparameterization. Next, we train a reachability prior using data from the<br>
operational robot workspace, inspired by classical methods. Subsequently, we<br>
derive Boosted Hybrid RL (BHyRL), a novel algorithm for learning Q-functions by<br>
modeling them as a sum of residual approximators. Every time a new task needs<br>
to be learned, we can transfer our learned residuals and learn the component of<br>
the Q-function that is task-specific, hence, maintaining the task structure<br>
from prior behaviors. Moreover, we find that regularizing the target policy<br>
with a prior policy yields more expressive behaviors. We evaluate our method in<br>
simulation in reaching and fetching tasks of increasing difficulty, and we show<br>
the superior performance of BHyRL against baseline methods. Finally, we<br>
zero-transfer our learned 6D fetching policy with BHyRL to our MM robot<br>
TIAGo++. For more details and code release, please refer to our project site:<br>
<a href="http://irosalab.com/rlmmbp" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=http://irosalab.com/rlmmbp&amp;source=gmail&amp;ust=1646963617851000&amp;usg=AOvVaw0I26SoF1NIspYPSlJmr_oD" rel="noreferrer" target="_blank">irosalab.com/rlmmbp</a><br>
\\ ( <a href="https://arxiv.org/abs/2203.04051" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04051&amp;source=gmail&amp;ust=1646963617851000&amp;usg=AOvVaw1aPNOvWoSNhyFoWouJuPzH" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04051</a> ,&nbsp; 17573kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04090 (*cross-listing*)<br>
Date: Wed, 2 Mar 2022 12:16:09 GMT&nbsp; &nbsp;(635kb,D)<br>
<br>
Title: Foundations for Grassroots Democratic Metaverse<br>
Authors: Nimrod Talmon and Ehud Shapiro<br>
Categories: cs.CY cs.AI cs.DC cs.MA cs.SI<br>
\\<br>
&nbsp; While the physical lives of many of us are in democracies (one person, one<br>
vote - e.g., the EU and the US), our digital lives are mostly in autocracies<br>
(one person, all votes - e.g., Facebook). Cryptocurrencies promise liberation<br>
but stop short, at plutocracy (one coin, one vote). What would it take for us<br>
to live in a digital democracy? This paper offers a vision, a theoretical<br>
framework, and an architecture for a grassroots network of autonomous,<br>
people-owned, people-operated, and people-governed digital communities, namely<br>
a grassroots democratic metaverse. It also charts a roadmap towards realizing<br>
it, and identifies unexplored territory for MAS research.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04090" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04090&amp;source=gmail&amp;ust=1646963617852000&amp;usg=AOvVaw0ptZ72Lipc9ToyxslceNaP" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04090</a> ,&nbsp; 635kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04107 (*cross-listing*)<br>
Date: Mon, 7 Mar 2022 14:34:42 GMT&nbsp; &nbsp;(11014kb,D)<br>
<br>
Title: Comparing representations of biological data learned with different AI<br>
&nbsp; paradigms, augmenting and cropping strategies<br>
Authors: Andrei Dmitrenko, Mauro M. Masiero and Nicola Zamboni<br>
Categories: eess.IV cs.AI cs.CV cs.LG q-bio.QM<br>
Comments: Accepted to MIDL 2022 conference. 17 pages, 8 figures, 4 tables<br>
\\<br>
&nbsp; Recent advances in computer vision and robotics enabled automated large-scale<br>
biological image analysis. Various machine learning approaches have been<br>
successfully applied to phenotypic profiling. However, it remains unclear how<br>
they compare in terms of biological feature extraction. In this study, we<br>
propose a simple CNN architecture and implement 4 different representation<br>
learning approaches. We train 16 deep learning setups on the 770k cancer cell<br>
images dataset under identical conditions, using different augmenting and<br>
cropping strategies. We compare the learned representations by evaluating<br>
multiple metrics for each of three downstream tasks: i) distance-based<br>
similarity analysis of known drugs, ii) classification of drugs versus<br>
controls, iii) clustering within cell lines. We also compare training times and<br>
memory usage. Among all tested setups, multi-crops and random augmentations<br>
generally improved performance across tasks, as expected. Strikingly,<br>
self-supervised (implicit contrastive learning) models showed competitive<br>
performance being up to 11 times faster to train. Self-supervised regularized<br>
learning required the most of memory and computation to deliver arguably the<br>
most informative features. We observe that no single combination of augmenting<br>
and cropping strategies consistently results in top performance across tasks<br>
and recommend prospective research directions.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04107" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04107&amp;source=gmail&amp;ust=1646963617852000&amp;usg=AOvVaw0YF30AMiVQdXUGrUACHSF4" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04107</a> ,&nbsp; 11014kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04111 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 14:33:45 GMT&nbsp; &nbsp;(1098kb,D)<br>
<br>
Title: Plumeria at SemEval-2022 Task 6: Robust Approaches for Sarcasm Detection<br>
&nbsp; for English and Arabic Using Transformers and Data Augmentation<br>
Authors: Shubham Kumar Nigam and Mosab Shaheen<br>
Categories: cs.CL cs.AI cs.LG<br>
Comments: SemEval-2022 workshop paper, submitted in NAACL-2022 conference. 8<br>
&nbsp; figures and 29 tables. 8 main pages, 4 appendix pages<br>
\\<br>
&nbsp; This paper describes our submission to SemEval-2022 Task 6 on sarcasm<br>
detection and its five subtasks for English and Arabic. Sarcasm conveys a<br>
meaning which contradicts the literal meaning, and it is mainly found on social<br>
networks. It has a significant role in understanding the intention of the user.<br>
For detecting sarcasm, we used deep learning techniques based on transformers<br>
due to its success in the field of Natural Language Processing (NLP) without<br>
the need for feature engineering. The datasets were taken from tweets. We<br>
created new datasets by augmenting with external data or by using word<br>
embeddings and repetition of instances. Experiments were done on the datasets<br>
with different types of preprocessing because it is crucial in this task. The<br>
rank of our team was consistent across four subtasks (fourth rank in three<br>
subtasks and sixth rank in one subtask); whereas other teams might be in the<br>
top ranks for some subtasks but rank drastically less in other subtasks. This<br>
implies the robustness and stability of the models and the techniques we used.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04111" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04111&amp;source=gmail&amp;ust=1646963617852000&amp;usg=AOvVaw2fFNZ9haEKmWDM4N0dz0CG" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04111</a> ,&nbsp; 1098kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04120 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 14:44:51 GMT&nbsp; &nbsp;(5103kb,D)<br>
<br>
Title: Graph-based Reinforcement Learning meets Mixed Integer Programs: An<br>
&nbsp; application to 3D robot assembly discovery<br>
Authors: Niklas Funk, Svenja Menzenbach, Georgia Chalvatzaki, Jan Peters<br>
Categories: cs.RO cs.AI cs.LG<br>
\\<br>
&nbsp; Robot assembly discovery is a challenging problem that lives at the<br>
intersection of resource allocation and motion planning. The goal is to combine<br>
a predefined set of objects to form something new while considering task<br>
execution with the robot-in-the-loop. In this work, we tackle the problem of<br>
building arbitrary, predefined target structures entirely from scratch using a<br>
set of Tetris-like building blocks and a robotic manipulator. Our novel<br>
hierarchical approach aims at efficiently decomposing the overall task into<br>
three feasible levels that benefit mutually from each other. On the high level,<br>
we run a classical mixed-integer program for global optimization of block-type<br>
selection and the blocks' final poses to recreate the desired shape. Its output<br>
is then exploited to efficiently guide the exploration of an underlying<br>
reinforcement learning (RL) policy. This RL policy draws its generalization<br>
properties from a flexible graph-based representation that is learned through<br>
Q-learning and can be refined with search. Moreover, it accounts for the<br>
necessary conditions of structural stability and robotic feasibility that<br>
cannot be effectively reflected in the previous layer. Lastly, a grasp and<br>
motion planner transforms the desired assembly commands into robot joint<br>
movements. We demonstrate the performance of the proposed method on a set of<br>
competitive simulated robot assembly discovery environments and report<br>
performance and robustness gains compared to an unstructured end-to-end<br>
approach. Videos are available at <a href="https://sites.google.com/view/rl-meets-milp" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://sites.google.com/view/rl-meets-milp&amp;source=gmail&amp;ust=1646963617852000&amp;usg=AOvVaw392z_zMB-whNHVOGHAUeaI" rel="noreferrer" target="_blank">https://sites.google.com/view/<wbr>rl-meets-milp</a> .<br>
\\ ( <a href="https://arxiv.org/abs/2203.04120" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04120&amp;source=gmail&amp;ust=1646963617852000&amp;usg=AOvVaw1DSdm6TbuELNVWr4z-mBfK" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04120</a> ,&nbsp; 5103kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04197 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 16:49:15 GMT&nbsp; &nbsp;(352kb,D)<br>
<br>
Title: Locate This, Not That: Class-Conditioned Sound Event DOA Estimation<br>
Authors: Olga Slizovskaia, Gordon Wichern, Zhong-Qiu Wang, Jonathan Le Roux<br>
Categories: eess.AS cs.AI cs.LG cs.SD<br>
Comments: Accepted for publication at ICASSP 2022<br>
\\<br>
&nbsp; Existing systems for sound event localization and detection (SELD) typically<br>
operate by estimating a source location for all classes at every time instant.<br>
In this paper, we propose an alternative class-conditioned SELD model for<br>
situations where we may not be interested in localizing all classes all of the<br>
time. This class-conditioned SELD model takes as input the spatial and spectral<br>
features from the sound file, and also a one-hot vector indicating the class we<br>
are currently interested in localizing. We inject the conditioning information<br>
at several points in our model using feature-wise linear modulation (FiLM)<br>
layers. Through experiments on the DCASE 2020 Task 3 dataset, we show that the<br>
proposed class-conditioned SELD model performs better in terms of common SELD<br>
metrics than the baseline model that locates all classes simultaneously, and<br>
also outperforms specialist models that are trained to locate only a single<br>
class of interest. We also evaluate performance on the DCASE 2021 Task 3<br>
dataset, which includes directional interference (sound events from classes we<br>
are not interested in localizing) and notice especially strong improvement from<br>
the class-conditioned model.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04197" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04197&amp;source=gmail&amp;ust=1646963617852000&amp;usg=AOvVaw2IheFmTrzm0TxB6yDCLAqZ" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04197</a> ,&nbsp; 352kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04218 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 17:39:16 GMT&nbsp; &nbsp;(793kb,D)<br>
<br>
Title: Learning Bidirectional Translation between Descriptions and Actions with<br>
&nbsp; Small Paired Data<br>
Authors: Minori Toyoda, Kanata Suzuki, Yoshihiko Hayashi, Tetsuya Ogata<br>
Categories: cs.RO cs.AI cs.CL cs.LG<br>
Comments: 8 pages, 7 figures. Submitted to RA-L (IEEE Robotics and Automation<br>
&nbsp; Letters) with IROS 2022 Option. An accompanying video is available at<br>
&nbsp; <a href="https://youtu.be/YlxM_kw6YLE" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://youtu.be/YlxM_kw6YLE&amp;source=gmail&amp;ust=1646963617853000&amp;usg=AOvVaw1KnPJIfqHREJ1-rsO6SoPc" rel="noreferrer" target="_blank">https://youtu.be/YlxM_kw6YLE</a><br>
\\<br>
&nbsp; This study achieved bidirectional translation between descriptions and<br>
actions using small paired data. The ability to mutually generate descriptions<br>
and actions is essential for robots to collaborate with humans in their daily<br>
lives. The robot is required to associate real-world objects with linguistic<br>
expressions, and large-scale paired data are required for machine learning<br>
approaches. However, a paired dataset is expensive to construct and difficult<br>
to collect. This study proposes a two-stage training method for bidirectional<br>
translation. In the proposed method, we train recurrent autoencoders (RAEs) for<br>
descriptions and actions with a large amount of non-paired data. Then, we<br>
fine-tune the entire model to bind their intermediate representations using<br>
small paired data. Because the data used for pre-training do not require<br>
pairing, behavior-only data or a large language corpus can be used. We<br>
experimentally evaluated our method using a paired dataset consisting of<br>
motion-captured actions and descriptions. The results showed that our method<br>
performed well, even when the amount of paired data to train was small. The<br>
visualization of the intermediate representations of each RAE showed that<br>
similar actions were encoded in a clustered position and the corresponding<br>
feature vectors well aligned.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04218" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04218&amp;source=gmail&amp;ust=1646963617853000&amp;usg=AOvVaw33FVG-Z94DuOGaZr-JfKzC" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04218</a> ,&nbsp; 793kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04234 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 17:52:09 GMT&nbsp; &nbsp;(926kb)<br>
<br>
Title: Adaptative Perturbation Patterns: Realistic Adversarial Learning for<br>
&nbsp; Robust NIDS<br>
Authors: Jo\~ao Vitorino, Nuno Oliveira, Isabel Pra\c{c}a<br>
Categories: cs.CR cs.AI<br>
Comments: 16 pages, 6 tables, 8 figures, Future Internet journal<br>
\\<br>
&nbsp; Adversarial attacks pose a major threat to machine learning and to the<br>
systems that rely on it. Nonetheless, adversarial examples cannot be freely<br>
generated for domains with tabular data, such as cybersecurity. This work<br>
establishes the fundamental constraint levels required to achieve realism and<br>
introduces the Adaptative Perturbation Pattern Method (A2PM) to fulfill these<br>
constraints in a gray-box setting. A2PM relies on pattern sequences that are<br>
independently adapted to the characteristics of each class to create valid and<br>
coherent data perturbations. The developed method was evaluated in a<br>
cybersecurity case study with two scenarios: Enterprise and Internet of Things<br>
(IoT) networks. Multilayer Perceptron (MLP) and Random Forest (RF) classifiers<br>
were created with regular and adversarial training, using the CIC-IDS2017 and<br>
IoT-23 datasets. In each scenario, targeted and untargeted attacks were<br>
performed against the classifiers, and the generated examples were compared<br>
with the original network traffic flows to assess their realism. The obtained<br>
results demonstrate that A2PM provides a time efficient generation of realistic<br>
adversarial examples, which can be advantageous for both adversarial training<br>
and attacks.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04234" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04234&amp;source=gmail&amp;ust=1646963617853000&amp;usg=AOvVaw2iJUSXm-M3zhdDKMFicx63" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04234</a> ,&nbsp; 926kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03618 (*cross-listing*)<br>
Date: Fri, 4 Mar 2022 19:22:35 GMT&nbsp; &nbsp;(9508kb,D)<br>
<br>
Title: Mammograms Classification: A Review<br>
Authors: Marawan Elbatel<br>
Categories: eess.IV cs.CV cs.LG<br>
\\<br>
&nbsp; An advanced reliable low-cost form of screening method, Digital mammography<br>
has been used as an effective imaging method for breast cancer detection. With<br>
an increased focus on technologies to aid healthcare, Mammogram images have<br>
been utilized in developing computer-aided diagnosis systems that will<br>
potentially help in clinical diagnosis. Researchers have proved that artificial<br>
intelligence with its emerging technologies can be used in the early detection<br>
of the disease and improve radiologists' performance in assessing breast<br>
cancer. In this paper, we review the methods developed for mammogram mass<br>
classification in two categories. The first one is classifying manually<br>
provided cropped region of interests (ROI) as either malignant or benign, and<br>
the second one is the classification of automatically segmented ROIs as either<br>
malignant or benign. We also provide an overview of datasets and evaluation<br>
metrics used in the classification task. Finally, we compare and discuss the<br>
deep learning approach to classical image processing and learning approach in<br>
this domain.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03618" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03618&amp;source=gmail&amp;ust=1646963617853000&amp;usg=AOvVaw0gTQc8HGN98OdYy59-H_In" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03618</a> ,&nbsp; 9508kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03619 (*cross-listing*)<br>
Date: Fri, 4 Mar 2022 22:16:18 GMT&nbsp; &nbsp;(32121kb)<br>
<br>
Title: Adaptive Cross-Layer Attention for Image Restoration<br>
Authors: Yancheng Wang, Ning Xu, Chong Chen, Yingzhen Yang<br>
Categories: eess.IV cs.CV<br>
\\<br>
&nbsp; Non-local attention module has been proven to be crucial for image<br>
restoration. Conventional non-local attention processes features of each layer<br>
separately, so it risks missing correlation between features among different<br>
layers. To address this problem, we propose Cross-Layer Attention (CLA) module<br>
in this paper. Instead of finding correlated key pixels within the same layer,<br>
each query pixel can attend to key pixels at previous layers of the network. In<br>
order to further enhance the learning capability and reduce the inference cost<br>
of CLA, we further propose Adaptive CLA, or ACLA, as an improved CLA. Two<br>
adaptive designs are proposed for ACLA: 1) adaptively selecting the keys for<br>
non-local attention at each layer; 2) automatically searching for the insertion<br>
locations for ACLA modules. By these two adaptive designs, ACLA dynamically<br>
selects the number of keys to be aggregated for non-local attention at layer.<br>
In addition, ACLA searches for the optimal insert positions of ACLA modules by<br>
a neural architecture search method to render a compact neural network with<br>
compelling performance. Extensive experiments on image restoration tasks,<br>
including single image super-resolution, image denoising, image demosaicing,<br>
and image compression artifacts reduction, validate the effectiveness and<br>
efficiency of ACLA.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03619" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03619&amp;source=gmail&amp;ust=1646963617853000&amp;usg=AOvVaw0WfEtPf1qkXPMeOnJcttLm" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03619</a> ,&nbsp; 32121kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03622 (*cross-listing*)<br>
Date: Sat, 5 Mar 2022 06:12:49 GMT&nbsp; &nbsp;(18650kb,D)<br>
<br>
Title: Deep-ASPECTS: A Segmentation-Assisted Model for Stroke Severity<br>
&nbsp; Measurement<br>
Authors: Ujjwal Upadhyay, Mukul Ranjan, Satish Golla, Swetha Tanamala, Preetham<br>
&nbsp; Sreenivas, Sasank Chilamkurthy, Jeyaraj Pandian, and Jason Tarpley<br>
Categories: eess.IV cs.CV stat.AP<br>
\\<br>
&nbsp; A stroke occurs when an artery in the brain ruptures and bleeds or when the<br>
blood supply to the brain is cut off. Blood and oxygen cannot reach the brain's<br>
tissues due to the rupture or obstruction resulting in tissue death. The Middle<br>
cerebral artery (MCA) is the largest cerebral artery and the most commonly<br>
damaged vessel in stroke. The quick onset of a focused neurological deficit<br>
caused by interruption of blood flow in the territory supplied by the MCA is<br>
known as an MCA stroke. Alberta stroke programme early CT score (ASPECTS) is<br>
used to estimate the extent of early ischemic changes in patients with MCA<br>
stroke. This study proposes a deep learning-based method to score the CT scan<br>
for ASPECTS. Our work has three highlights. First, we propose a novel method<br>
for medical image segmentation for stroke detection. Second, we show the<br>
effectiveness of AI solution for fully-automated ASPECT scoring with reduced<br>
diagnosis time for a given non-contrast CT (NCCT) Scan. Our algorithms show a<br>
dice similarity coefficient of 0.64 for the MCA anatomy segmentation and 0.72<br>
for the infarcts segmentation. Lastly, we show that our model's performance is<br>
inline with inter-reader variability between radiologists.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03622" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03622&amp;source=gmail&amp;ust=1646963617854000&amp;usg=AOvVaw1G0IpTqDhZWVJYfnmBIi0g" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03622</a> ,&nbsp; 18650kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03623 (*cross-listing*)<br>
Date: Sat, 5 Mar 2022 07:37:45 GMT&nbsp; &nbsp;(18217kb,D)<br>
<br>
Title: Measurement-conditioned Denoising Diffusion Probabilistic Model for<br>
&nbsp; Under-sampled Medical Image Reconstruction<br>
Authors: Yutong Xie and Quanzheng Li<br>
Categories: eess.IV cs.CV<br>
\\<br>
&nbsp; We propose a novel and unified method, measurement-conditioned denoising<br>
diffusion probabilistic model (MC-DDPM), for under-sampled medical image<br>
reconstruction based on DDPM. Different from previous works, MC-DDPM is defined<br>
in measurement domain (e.g. k-space in MRI reconstruction) and conditioned on<br>
under-sampling mask. We apply this method to accelerate MRI reconstruction and<br>
the experimental results show excellent performance, outperforming full<br>
supervision baseline and the state-of-the-art score-based reconstruction<br>
method. Due to its generative nature, MC-DDPM can also quantify the uncertainty<br>
of reconstruction. Our code is available on github.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03623" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03623&amp;source=gmail&amp;ust=1646963617854000&amp;usg=AOvVaw2YvRgo9mk8I8YuvBxZlaGA" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03623</a> ,&nbsp; 18217kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03624 (*cross-listing*)<br>
Date: Sat, 5 Mar 2022 09:06:29 GMT&nbsp; &nbsp;(8519kb,D)<br>
<br>
Title: Fusion-Correction Network for Single-Exposure Correction and<br>
&nbsp; Multi-Exposure Fusion<br>
Authors: Jin Liang, Anran Zhang, Jun Xu, Hui Li, Xiantong Zhen<br>
Categories: eess.IV cs.CV<br>
\\<br>
&nbsp; The photographs captured by digital cameras usually suffer from over-exposure<br>
or under-exposure problems. The Single-Exposure Correction (SEC) and<br>
Multi-Exposure Fusion (MEF) are two widely studied image processing tasks for<br>
image exposure enhancement. However, current SEC and MEF methods ignore the<br>
internal correlation between SEC and MEF, and are proposed under distinct<br>
frameworks. What's more, most MEF methods usually fail at processing a sequence<br>
containing only under-exposed or over-exposed images. To alleviate these<br>
problems, in this paper, we develop an integrated framework to simultaneously<br>
tackle the SEC and MEF tasks. Built upon the Laplacian Pyramid (LP)<br>
decomposition, we propose a novel Fusion-Correction Network (FCNet) to fuse and<br>
correct an image sequence sequentially in a multi-level scheme. In each LP<br>
level, the image sequence is feed into a Fusion block and a Correction block<br>
for consecutive image fusion and exposure correction. The corrected image is<br>
upsampled and re-composed with the high-frequency detail components in<br>
next-level, producing the base sequence for the next-level blocks. Experiments<br>
on the benchmark dataset demonstrate that our FCNet is effective on both the<br>
SEC and MEF tasks.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03624" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03624&amp;source=gmail&amp;ust=1646963617854000&amp;usg=AOvVaw1KTcxFoyB1DwmbXdxBQVzP" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03624</a> ,&nbsp; 8519kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03626 (*cross-listing*)<br>
Date: Sat, 5 Mar 2022 21:23:03 GMT&nbsp; &nbsp;(10069kb,D)<br>
<br>
Title: Coordinate Translator for Learning Deformable Medical Image Registration<br>
Authors: Yihao Liu, Lianrui Zuo, Shuo Han, Jerry L. Prince, Aaron Carass<br>
Categories: eess.IV cs.CV<br>
\\<br>
&nbsp; The majority of deep learning (DL) based deformable image registration<br>
methods use convolutional neural networks (CNNs) to estimate displacement<br>
fields from pairs of moving and fixed images. This, however, requires the<br>
convolutional kernels in the CNN to not only extract intensity features from<br>
the inputs but also understand image coordinate systems. We argue that the<br>
latter task is challenging for traditional CNNs, limiting their performance in<br>
registration tasks. To tackle this problem, we first introduce Coordinate<br>
Translator (CoTr), a differentiable module that identifies matched features<br>
between the fixed and moving image and outputs their coordinate correspondences<br>
without the need for training. It unloads the burden of understanding image<br>
coordinate systems for CNNs, allowing them to focus on feature extraction. We<br>
then propose a novel deformable registration network, im2grid, that uses<br>
multiple CoTr's with the hierarchical features extracted from a CNN encoder and<br>
outputs a deformation field in a coarse-to-fine fashion. We compared im2grid<br>
with the state-of-the-art DL and non-DL methods for unsupervised 3D magnetic<br>
resonance image registration. Our experiments show that im2grid outperforms<br>
these methods both qualitatively and quantitatively.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03626" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03626&amp;source=gmail&amp;ust=1646963617854000&amp;usg=AOvVaw3640h0lmKM0c_qkpcWunmK" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03626</a> ,&nbsp; 10069kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03627 (*cross-listing*)<br>
Date: Sun, 6 Mar 2022 04:14:41 GMT&nbsp; &nbsp;(890kb)<br>
<br>
Title: Multi-channel deep convolutional neural networks for multi-classifying<br>
&nbsp; thyroid disease<br>
Authors: Xinyu Zhang, Vincent CS. Lee, Jia Rong, James C. Lee, Jiangning Song,<br>
&nbsp; Feng Liu<br>
Categories: eess.IV cs.CV<br>
\\<br>
&nbsp; Thyroid disease instances have been continuously increasing since the 1990s,<br>
and thyroid cancer has become the most rapidly rising disease among all the<br>
malignancies in recent years. Most existing studies focused on applying deep<br>
convolutional neural networks for detecting thyroid cancer. Despite their<br>
satisfactory performance on binary classification tasks, limited studies have<br>
explored multi-class classification of thyroid disease types; much less is<br>
known of the diagnosis of co-existence situation for different types of thyroid<br>
diseases. Therefore, this study proposed a novel multi-channel convolutional<br>
neural network (CNN) architecture to address the multi-class classification<br>
task of thyroid disease. The multi-channel CNN merits from computed tomography<br>
to drive a comprehensive diagnostic decision for the overall thyroid gland,<br>
emphasizing the disease co-existence circumstance. Moreover, this study also<br>
examined alternative strategies to enhance the diagnostic accuracy of CNN<br>
models through concatenation of different scales of feature maps. Benchmarking<br>
experiments demonstrate the improved performance of the proposed multi-channel<br>
CNN architecture compared with the standard single-channel CNN architecture.<br>
More specifically, the multi-channel CNN achieved an accuracy of 0.909,<br>
precision of 0.944, recall of 0.896, specificity of 0.994, and F1 of 0.917, in<br>
contrast to the single-channel CNN, which obtained 0.902, 0.892, 0.909, 0.993,<br>
0.898, respectively. In addition, the proposed model was evaluated in different<br>
gender groups; it reached a diagnostic accuracy of 0.908 for the female group<br>
and 0.901 for the male group. Collectively, the results highlight that the<br>
proposed multi-channel CNN has excellent generalization and has the potential<br>
to be deployed to provide computational decision support in clinical settings.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03627" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03627&amp;source=gmail&amp;ust=1646963617854000&amp;usg=AOvVaw1iN1uCMEsKyjXDs4N3pd9A" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03627</a> ,&nbsp; 890kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03631 (*cross-listing*)<br>
Date: Mon, 7 Mar 2022 02:20:14 GMT&nbsp; &nbsp;(10818kb,D)<br>
<br>
Title: Student Become Decathlon Master in Retinal Vessel Segmentation via<br>
&nbsp; Dual-teacher Multi-target Domain Adaptation<br>
Authors: Linkai Peng, Li Lin, Pujin Cheng, Huaqing He, Xiaoying Tang<br>
Categories: eess.IV cs.CV<br>
Comments: Submitted in MICCAI 2022<br>
\\<br>
&nbsp; Unsupervised domain adaptation has been proposed recently to tackle the<br>
so-called domain shift between training data and test data with different<br>
distributions. However, most of them only focus on single-target domain<br>
adaptation and cannot be applied to the scenario with multiple target domains.<br>
In this paper, we propose RVms, a novel unsupervised multi-target domain<br>
adaptation approach to segment retinal vessels (RVs) from multimodal and<br>
multicenter retinal images. RVms mainly consists of a style augmentation and<br>
transfer (SAT) module and a dual-teacher knowledge distillation (DTKD) module.<br>
SAT augments and clusters images into source-similar domains and<br>
source-dissimilar domains via B\'ezier and Fourier transformations. DTKD<br>
utilizes the augmented and transformed data to train two teachers, one for<br>
source-similar domains and the other for source-dissimilar domains. Afterwards,<br>
knowledge distillation is performed to iteratively distill different domain<br>
knowledge from teachers to a generic student. The local relative intensity<br>
transformation is employed to characterize RVs in a domain invariant manner and<br>
promote the generalizability of teachers and student models. Moreover, we<br>
construct a new multimodal and multicenter vascular segmentation dataset from<br>
existing publicly-available datasets, which can be used to benchmark various<br>
domain adaptation and domain generalization methods. Through extensive<br>
experiments, RVms is found to be very close to the target-trained Oracle in<br>
terms of segmenting the RVs, largely outperforming other state-of-the-art<br>
methods.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03631" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03631&amp;source=gmail&amp;ust=1646963617855000&amp;usg=AOvVaw0KWxIlOqpa1kMzClg0RZ7C" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03631</a> ,&nbsp; 10818kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03634 (*cross-listing*)<br>
Date: Mon, 7 Mar 2022 07:06:42 GMT&nbsp; &nbsp;(891kb,D)<br>
<br>
Title: InsightNet: non-contact blood pressure measuring network based on face<br>
&nbsp; video<br>
Authors: Jialiang Zhuang and Bin Li and Yun Zhang and Xiujuan Zheng<br>
Categories: eess.IV cs.CV cs.HC<br>
Comments: 23 pages, 7 figures<br>
\\<br>
&nbsp; Blood pressure indicates cardiac function and peripheral vascular resistance<br>
and is critical for disease diagnosis. Traditionally, blood pressure data are<br>
mainly acquired through contact sensors, which require high maintenance and may<br>
be inconvenient and unfriendly to some people (e.g., burn patients). In this<br>
paper, an efficient non-contact blood pressure measurement network based on<br>
face videos is proposed for the first time. An innovative oversampling training<br>
strategy is proposed to handle the unbalanced data distribution. The input<br>
video sequences are first normalized and converted to our proposed YUVT color<br>
space. Then, the Spatio-temporal slicer encodes it into a multi-domain<br>
Spatio-temporal mapping. Finally, the neural network computation module, used<br>
for high-dimensional feature extraction of the multi-domain spatial feature<br>
mapping, after which the extracted high-dimensional features are used to<br>
enhance the time-domain feature association using LSTM, is computed by the<br>
blood pressure classifier to obtain the blood pressure measurement intervals.<br>
Combining the output of feature extraction and the result after classification,<br>
the blood pressure calculator, calculates the blood pressure measurement<br>
values. The solution uses a blood pressure classifier to calculate blood<br>
pressure intervals, which can help the neural network distinguish between the<br>
high-dimensional features of different blood pressure intervals and alleviate<br>
the overfitting phenomenon. It can also locate the blood pressure intervals,<br>
correct the final blood pressure values and improve the network performance.<br>
Experimental results on two datasets show that the network outperforms existing<br>
state-of-the-art methods.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03634" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03634&amp;source=gmail&amp;ust=1646963617855000&amp;usg=AOvVaw3TBEO7CxYh9DhbHvfFV5bc" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03634</a> ,&nbsp; 891kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03635 (*cross-listing*)<br>
Date: Mon, 7 Mar 2022 10:36:38 GMT&nbsp; &nbsp;(2168kb)<br>
<br>
Title: Stepwise Feature Fusion: Local Guides Global<br>
Authors: Jinfeng Wang, Qiming Huang, Feilong Tang, Jia Meng, Jionglong Su, and<br>
&nbsp; Sifan Song<br>
Categories: eess.IV cs.CV<br>
Comments: 10 pages, 5 figures<br>
\\<br>
&nbsp; Colonoscopy, currently the most efficient and recognized colon polyp<br>
detection technology, is necessary for early screening and prevention of<br>
colorectal cancer. However, due to the varying size and complex morphological<br>
features of colonic polyps as well as the indistinct boundary between polyps<br>
and mucosa, accurate segmentation of polyps is still challenging. Deep learning<br>
has become popular for accurate polyp segmentation tasks with excellent<br>
results. However, due to the structure of polyps image and the varying shapes<br>
of polyps, it easy for existing deep learning models to overfitting the current<br>
dataset. As a result, the model may not process unseen colonoscopy data. To<br>
address this, we propose a new State-Of-The-Art model for medical image<br>
segmentation, the SSFormer, which uses a pyramid Transformer encoder to improve<br>
the generalization ability of models. Specifically, our proposed Progressive<br>
Locality Decoder can be adapted to the pyramid Transformer backbone to<br>
emphasize local features and restrict attention dispersion. The SSFormer<br>
achieves statet-of-the-art performance in both learning and generalization<br>
assessment.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03635" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03635&amp;source=gmail&amp;ust=1646963617855000&amp;usg=AOvVaw1cEoz9llM46n3KB6Aq_jtt" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03635</a> ,&nbsp; 2168kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03636 (*cross-listing*)<br>
Date: Mon, 7 Mar 2022 11:56:06 GMT&nbsp; &nbsp;(4180kb,D)<br>
<br>
Title: Clustering and classification of low-dimensional data in explicit<br>
&nbsp; feature map domain: intraoperative pixel-wise diagnosis of adenocarcinoma of<br>
&nbsp; a colon in a liver<br>
Authors: Dario Sitnik and Ivica Kopriva<br>
Categories: eess.IV cs.CV cs.LG<br>
Comments: 18 pages, 4 figures, 6 tables, appendix<br>
\\<br>
&nbsp; Application of artificial intelligence in medicine brings in highly accurate<br>
predictions achieved by complex models, the reasoning of which is hard to<br>
interpret. Their generalization ability can be reduced because of the lack of<br>
pixel wise annotated images that occurs in frozen section tissue analysis. To<br>
partially overcome this gap, this paper explores the approximate explicit<br>
feature map (aEFM) transform of low-dimensional data into a low-dimensional<br>
subspace in Hilbert space. There, with a modest increase in computational<br>
complexity, linear algorithms yield improved performance and keep<br>
interpretability. They remain amenable to incremental learning that is not a<br>
trivial issue for some nonlinear algorithms. We demonstrate proposed<br>
methodology on a very large-scale problem related to intraoperative pixel-wise<br>
semantic segmentation and clustering of adenocarcinoma of a colon in a liver.<br>
Compared to the results in the input space, logistic classifier achieved<br>
statistically significant performance improvements in micro balanced accuracy<br>
and F1 score in the amounts of 12.04% and 12.58%, respectively. Support vector<br>
machine classifier yielded the increase of 8.04% and 9.41%. For clustering,<br>
increases of 0.79% and 0.85% are obtained with ultra large-scale spectral<br>
clustering algorithm. Results are supported by a discussion of interpretability<br>
using Shapely additive explanation values for predictions of linear classifier<br>
in input space and aEFM induced space.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03636" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03636&amp;source=gmail&amp;ust=1646963617855000&amp;usg=AOvVaw32dygN88oiey3t50eCwAiv" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03636</a> ,&nbsp; 4180kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03640 (*cross-listing*)<br>
Date: Mon, 7 Mar 2022 14:31:26 GMT&nbsp; &nbsp;(5826kb,D)<br>
<br>
Title: Conquering Data Variations in Resolution: A Slice-Aware Multi-Branch<br>
&nbsp; Decoder Network<br>
Authors: Shuxin Wang, Shilei Cao, Zhizhong Chai, Dong Wei, Kai Ma, Liansheng<br>
&nbsp; Wang, Yefeng Zheng<br>
Categories: eess.IV cs.CV<br>
Comments: Published by IEEE TMI<br>
DOI: 10.1109/TMI.2020.3014433<br>
\\<br>
&nbsp; Fully convolutional neural networks have made promising progress in joint<br>
liver and liver tumor segmentation. Instead of following the debates over 2D<br>
versus 3D networks (for example, pursuing the balance between large-scale 2D<br>
pretraining and 3D context), in this paper, we novelly identify the wide<br>
variation in the ratio between intra- and inter-slice resolutions as a crucial<br>
obstacle to the performance. To tackle the mismatch between the intra- and<br>
inter-slice information, we propose a slice-aware 2.5D network that emphasizes<br>
extracting discriminative features utilizing not only in-plane semantics but<br>
also out-of-plane coherence for each separate slice. Specifically, we present a<br>
slice-wise multi-input multi-output architecture to instantiate such a design<br>
paradigm, which contains a Multi-Branch Decoder (MD) with a Slice-centric<br>
Attention Block (SAB) for learning slice-specific features and a Densely<br>
Connected Dice (DCD) loss to regularize the inter-slice predictions to be<br>
coherent and continuous. Based on the aforementioned innovations, we achieve<br>
state-of-the-art results on the MICCAI 2017 Liver Tumor Segmentation (LiTS)<br>
dataset. Besides, we also test our model on the ISBI 2019 Segmentation of<br>
THoracic Organs at Risk (SegTHOR) dataset, and the result proves the robustness<br>
and generalizability of the proposed method in other segmentation tasks.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03640" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03640&amp;source=gmail&amp;ust=1646963617856000&amp;usg=AOvVaw1c4YCatIqAcqEEEeKCxQp5" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03640</a> ,&nbsp; 5826kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03719 (*cross-listing*)<br>
Date: Wed, 23 Feb 2022 11:45:54 GMT&nbsp; &nbsp;(309kb)<br>
<br>
Title: Biometric recognition: why not massively adopted yet?<br>
Authors: Marcos Faundez-Zanuy<br>
Categories: cs.CY cs.CR cs.CV cs.LG<br>
Comments: 5 pages<br>
Journal-ref: IEEE Aerospace and Electronic Systems Magazine, vol. 20, no. 8,<br>
&nbsp; pp. 25-28, Aug. 2005<br>
DOI: 10.1109/MAES.2005.1499300<br>
\\<br>
&nbsp; Although there has been a dramatically reduction on the prices of capturing<br>
devices and an increase on computing power in the last decade, it seems that<br>
biometric systems are still far from massive adoption for civilian<br>
applications. This paper deals with the causes of this phenomenon, as well as<br>
some misconceptions regarding biometric identification.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03719" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03719&amp;source=gmail&amp;ust=1646963617856000&amp;usg=AOvVaw3C9S1pmFBhvWATU1f1rGJ5" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03719</a> ,&nbsp; 309kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03814 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 02:29:32 GMT&nbsp; &nbsp;(12663kb,D)<br>
<br>
Title: Generating 3D Bio-Printable Patches Using Wound Segmentation and<br>
&nbsp; Reconstruction to Treat Diabetic Foot Ulcers<br>
Authors: Han Joo Chae, Seunghwan Lee, Hyewon Son, Seungyeob Han, Taebin Lim<br>
Categories: eess.IV cs.CV cs.HC cs.LG<br>
Comments: Accepted to CVPR 2022<br>
\\<br>
&nbsp; We introduce AiD Regen, a novel system that generates 3D wound models<br>
combining 2D semantic segmentation with 3D reconstruction so that they can be<br>
printed via 3D bio-printers during the surgery to treat diabetic foot ulcers<br>
(DFUs). AiD Regen seamlessly binds the full pipeline, which includes RGB-D<br>
image capturing, semantic segmentation, boundary-guided point-cloud processing,<br>
3D model reconstruction, and 3D printable G-code generation, into a single<br>
system that can be used out of the box. We developed a multi-stage data<br>
preprocessing method to handle small and unbalanced DFU image datasets. AiD<br>
Regen's human-in-the-loop machine learning interface enables clinicians to not<br>
only create 3D regenerative patches with just a few touch interactions but also<br>
customize and confirm wound boundaries. As evidenced by our experiments, our<br>
model outperforms prior wound segmentation models and our reconstruction<br>
algorithm is capable of generating 3D wound models with compelling accuracy. We<br>
further conducted a case study on a real DFU patient and demonstrated the<br>
effectiveness of AiD Regen in treating DFU wounds.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03814" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03814&amp;source=gmail&amp;ust=1646963617856000&amp;usg=AOvVaw3d0-3WZVdRQ6Nvr3_lXVwj" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03814</a> ,&nbsp; 12663kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03844 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 04:26:18 GMT&nbsp; &nbsp;(1735kb,D)<br>
<br>
Title: Dynamic Dual Trainable Bounds for Ultra-low Precision Super-Resolution<br>
&nbsp; Networks<br>
Authors: Yunshan Zhong, Mingbao Lin, Xunchao Li, Ke Li, Yunhang Shen, Fei Chao,<br>
&nbsp; Yongjian Wu, Rongrong Ji<br>
Categories: eess.IV cs.CV<br>
\\<br>
&nbsp; Light-weight super-resolution (SR) models have received considerable<br>
attention for their serviceability in mobile devices. Many efforts employ<br>
network quantization to compress SR models. However, these methods suffer from<br>
severe performance degradation when quantizing the SR models to ultra-low<br>
precision (e.g., 2-bit and 3-bit) with the low-cost layer-wise quantizer. In<br>
this paper, we identify that the performance drop comes from the contradiction<br>
between the layer-wise symmetric quantizer and the highly asymmetric activation<br>
distribution in SR models. This discrepancy leads to either a waste on the<br>
quantization levels or detail loss in reconstructed images. Therefore, we<br>
propose a novel activation quantizer, referred to as Dynamic Dual Trainable<br>
Bounds (DDTB), to accommodate the asymmetry of the activations. Specifically,<br>
DDTB innovates in: 1) A layer-wise quantizer with trainable upper and lower<br>
bounds to tackle the highly asymmetric activations. 2) A dynamic gate<br>
controller to adaptively adjust the upper and lower bounds at runtime to<br>
overcome the drastically varying activation ranges over different samples.To<br>
reduce the extra overhead, the dynamic gate controller is quantized to 2-bit<br>
and applied to only part of the SR networks according to the introduced dynamic<br>
intensity. Extensive experiments demonstrate that our DDTB exhibits significant<br>
performance improvements in ultra-low precision. For example, our DDTB achieves<br>
a 0.70dB PSNR increase on Urban100 benchmark when quantizing EDSR to 2-bit and<br>
scaling up output images to x4. Code is at<br>
\url{<a href="https://github.com/zysxmu/DDTB" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/zysxmu/DDTB&amp;source=gmail&amp;ust=1646963617856000&amp;usg=AOvVaw2P5ubRZHVaqXXj-EozrHpa" rel="noreferrer" target="_blank">https://github.com/<wbr>zysxmu/DDTB</a>}.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03844" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03844&amp;source=gmail&amp;ust=1646963617856000&amp;usg=AOvVaw0aFgaITtcrLms8RadngKBo" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03844</a> ,&nbsp; 1735kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03853 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 05:01:43 GMT&nbsp; &nbsp;(1417kb,D)<br>
<br>
Title: Where Does the Performance Improvement Come From? - A Reproducibility<br>
&nbsp; Concern about Image-Text Retrieval<br>
Authors: Jun Rao, Fei Wang, Liang Ding, Shuhan Qi, Yibing Zhan, Weifeng Liu,<br>
&nbsp; Dacheng Tao<br>
Categories: cs.IR cs.CL cs.CV<br>
Comments: submitted to SIGIR 2022 (reproducibility track)<br>
\\<br>
&nbsp; This paper seeks to provide the information retrieval community with some<br>
reflections on the current improvements of retrieval learning through the<br>
analysis of the reproducibility aspects of image-text retrieval models. For the<br>
latter part of the past decade, image-text retrieval has gradually become a<br>
major research direction in the field of information retrieval because of the<br>
growth of multi-modal data. Many researchers use benchmark datasets like<br>
MS-COCO and Flickr30k to train and assess the performance of image-text<br>
retrieval algorithms. Research in the past has mostly focused on performance,<br>
with several state-of-the-art methods being proposed in various ways. According<br>
to their claims, these approaches achieve better modal interactions and thus<br>
better multimodal representations with greater precision. In contrast to those<br>
previous works, we focus on the repeatability of the approaches and the overall<br>
examination of the elements that lead to improved performance by pretrained and<br>
nonpretrained models in retrieving images and text. To be more specific, we<br>
first examine the related reproducibility concerns and why the focus is on<br>
image-text retrieval tasks, and then we systematically summarize the current<br>
paradigm of image-text retrieval models and the stated contributions of those<br>
approaches. Second, we analyze various aspects of the reproduction of<br>
pretrained and nonpretrained retrieval models. Based on this, we conducted<br>
ablation experiments and obtained some influencing factors that affect<br>
retrieval recall more than the improvement claimed in the original paper.<br>
Finally, we also present some reflections and issues that should be considered<br>
by the retrieval community in the future. Our code is freely available at<br>
<a href="https://github.com/WangFei-2019/Image-text-Retrieval" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/WangFei-2019/Image-text-Retrieval&amp;source=gmail&amp;ust=1646963617857000&amp;usg=AOvVaw1mCEsAqYUR_CwiXHNjP18w" rel="noreferrer" target="_blank">https://github.com/WangFei-<wbr>2019/Image-text-Retrieval</a>.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03853" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03853&amp;source=gmail&amp;ust=1646963617857000&amp;usg=AOvVaw22Sl8N6KMT-Dt0YLWuXyT5" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03853</a> ,&nbsp; 1417kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04013 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 11:08:44 GMT&nbsp; &nbsp;(3456kb,D)<br>
<br>
Title: Mutual Contrastive Learning to Disentangle Whole Slide Image<br>
&nbsp; Representations for Glioma Grading<br>
Authors: Lipei Zhang, Yiran Wei, Ying Fu, Stephen Price, Carola-Bibiane<br>
&nbsp; Sch\"onlieb and Chao Li<br>
Categories: eess.IV cs.CV<br>
Comments: 11 pages, 4 figures, 2 tables<br>
\\<br>
&nbsp; Whole slide images (WSI) provide valuable phenotypic information for<br>
histological assessment and malignancy grading of tumors. The WSI-based<br>
computational pathology promises to provide rapid diagnostic support and<br>
facilitate digital health. The most commonly used WSI are derived from<br>
formalin-fixed paraffin-embedded (FFPE) and frozen sections. Currently, the<br>
majority of automatic tumor grading models are developed based on FFPE<br>
sections, which could be affected by the artifacts introduced by tissue<br>
processing. Here we propose a mutual contrastive learning scheme to integrate<br>
FFPE and frozen sections and disentangle cross-modality representations for<br>
glioma grading. We first design a mutual learning scheme to jointly optimize<br>
the model training based on FFPE and frozen sections. Further, we develop a<br>
multi-modality domain alignment mechanism to ensure semantic consistency in the<br>
backbone model training. We finally design a sphere normalized<br>
temperature-scaled cross-entropy loss (NT-Xent), which could promote<br>
cross-modality representation disentangling of FFPE and frozen sections. Our<br>
experiments show that the proposed scheme achieves better performance than the<br>
model trained based on each single modality or mixed modalities. The sphere<br>
NT-Xent loss outperforms other typical metrics loss functions.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04013" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04013&amp;source=gmail&amp;ust=1646963617857000&amp;usg=AOvVaw0Ptd-j92gixtVtqHWmOJ5L" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04013</a> ,&nbsp; 3456kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04042 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 12:22:31 GMT&nbsp; &nbsp;(47854kb,D)<br>
<br>
Title: Learning to Erase the Bayer-Filter to See in the Dark<br>
Authors: Xingbo Dong, Wanyan Xu, Zhihui Miao, Lan Ma, Chao Zhang, Jiewen Yang,<br>
&nbsp; Zhe Jin, Andrew Beng Jin Teoh, Jiajun Shen<br>
Categories: eess.IV cs.CV<br>
\\<br>
&nbsp; Low-light image enhancement - a pervasive but challenging problem, plays a<br>
central role in enhancing the visibility of an image captured in a poor<br>
illumination environment. Due to the fact that not all photons can pass the<br>
Bayer-Filter on the sensor of the color camera, in this work, we first present<br>
a De-Bayer-Filter simulator based on deep neural networks to generate a<br>
monochrome raw image from the colored raw image. Next, a fully convolutional<br>
network is proposed to achieve the low-light image enhancement by fusing<br>
colored raw data with synthesized monochrome raw data. Channel-wise attention<br>
is also introduced to the fusion process to establish a complementary<br>
interaction between features from colored and monochrome raw images. To train<br>
the convolutional networks, we propose a dataset with monochrome and color raw<br>
pairs named Mono-Colored Raw paired dataset (MCR) collected by using a<br>
monochrome camera without Bayer-Filter and a color camera with Bayer-Filter.<br>
The proposed pipeline take advantages of the fusion of the virtual monochrome<br>
and the color raw images and our extensive experiments indicate that<br>
significant improvement can be achieved by leveraging raw sensor data and<br>
data-driven learning.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04042" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04042&amp;source=gmail&amp;ust=1646963617857000&amp;usg=AOvVaw0-VxXgKJqyst6SV0uSnmvy" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04042</a> ,&nbsp; 47854kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04064 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 13:14:43 GMT&nbsp; &nbsp;(11509kb,D)<br>
<br>
Title: Analyzing General-Purpose Deep-Learning Detection and Segmentation<br>
&nbsp; Models with Images from a Lidar as a Camera Sensor<br>
Authors: Yu Xianjia, Sahar Salimpour, Jorge Pe\~na Queralta, Tomi Westerlund<br>
Categories: cs.RO cs.CV<br>
\\<br>
&nbsp; Over the last decade, robotic perception algorithms have significantly<br>
benefited from the rapid advances in deep learning (DL). Indeed, a significant<br>
amount of the autonomy stack of different commercial and research platforms<br>
relies on DL for situational awareness, especially vision sensors. This work<br>
explores the potential of general-purpose DL perception algorithms,<br>
specifically detection and segmentation neural networks, for processing<br>
image-like outputs of advanced lidar sensors. Rather than processing the<br>
three-dimensional point cloud data, this is, to the best of our knowledge, the<br>
first work to focus on low-resolution images with 360\textdegree field of view<br>
obtained with lidar sensors by encoding either depth, reflectivity, or<br>
near-infrared light in the image pixels. We show that with adequate<br>
preprocessing, general-purpose DL models can process these images, opening the<br>
door to their usage in environmental conditions where vision sensors present<br>
inherent limitations. We provide both a qualitative and quantitative analysis<br>
of the performance of a variety of neural network architectures. We believe<br>
that using DL models built for visual cameras offers significant advantages due<br>
to the much wider availability and maturity compared to point cloud-based<br>
perception.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04064" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04064&amp;source=gmail&amp;ust=1646963617857000&amp;usg=AOvVaw3SGLyxL8dbKrLmdCAUqPw_" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04064</a> ,&nbsp; 11509kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04093 (*cross-listing*)<br>
Date: Fri, 4 Mar 2022 04:03:41 GMT&nbsp; &nbsp;(447kb)<br>
<br>
Title: Exploration of Various Deep Learning Models for Increased Accuracy in<br>
&nbsp; Automatic Polyp Detection<br>
Authors: Ariel E. Isidro, Arnel C. Fajardo, Alexander A. Hernandez<br>
Categories: eess.IV cs.CV cs.IR<br>
Journal-ref: Volume 3 Issue 1, 36-48, 2019<br>
\\<br>
&nbsp; This paper is created to explore deep learning models and algorithms that<br>
results in highest accuracy in detecting polyp on colonoscopy images. Previous<br>
studies implemented deep learning using convolution neural network (CNN)<br>
algorithm in detecting polyp and non-polyp. Other studies used dropout, and<br>
data augmentation algorithm but mostly not checking the overfitting, thus,<br>
include more than four-layer modelss. Rulei Yu <a href="http://et.al/" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=http://et.al&amp;source=gmail&amp;ust=1646963617857000&amp;usg=AOvVaw2fo2x5YPljrk8d3SQh4Vut" rel="noreferrer" target="_blank">et.al</a> from the Institute of<br>
Software, Chinese Academy of Sciences said that transfer learning is better<br>
talking about performance or improving the previous used algorithm. Most<br>
especially in applying the transfer learning in feature extraction. Series of<br>
experiments were conducted with only a minimum of 4 CNN layers applying<br>
previous used models and identified the model that produce the highest<br>
percentage accuracy of 98% among the other models that apply transfer learning.<br>
Further studies could use different optimizer to a different CNN modelsto<br>
increase accuracy.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04093" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04093&amp;source=gmail&amp;ust=1646963617858000&amp;usg=AOvVaw2lbVETjxerbgD4K8_SaHdr" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04093</a> ,&nbsp; 447kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04099 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 14:08:47 GMT&nbsp; &nbsp;(1939kb,D)<br>
<br>
Title: VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer<br>
Authors: Juan F. Montesinos, Venkatesh S. Kadandale, Gloria Haro<br>
Categories: cs.SD cs.CV cs.LG eess.AS<br>
\\<br>
&nbsp; This paper presents an audio-visual approach for voice separation which<br>
outperforms state-of-the-art methods at a low latency in two scenarios: speech<br>
and singing voice. The model is based on a two-stage network. Motion cues are<br>
obtained with a lightweight graph convolutional network that processes face<br>
landmarks. Then, both audio and motion features are fed to an audio-visual<br>
transformer which produces a fairly good estimation of the isolated target<br>
source. In a second stage, the predominant voice is enhanced with an audio-only<br>
network. We present different ablation studies and comparison to<br>
state-of-the-art methods. Finally, we explore the transferability of models<br>
trained for speech separation in the task of singing voice separation. The<br>
demos, code, and weights will be made publicly available at<br>
<a href="https://ipcv.github.io/VoViT/" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://ipcv.github.io/VoViT/&amp;source=gmail&amp;ust=1646963617858000&amp;usg=AOvVaw3056Eyh_zVpwg0BqGEGvg4" rel="noreferrer" target="_blank">https://ipcv.github.io/VoViT/</a><br>
\\ ( <a href="https://arxiv.org/abs/2203.04099" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04099&amp;source=gmail&amp;ust=1646963617858000&amp;usg=AOvVaw09XhO90GndLVbFXfqYxKSf" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04099</a> ,&nbsp; 1939kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04114 (*cross-listing*)<br>
Date: Mon, 7 Mar 2022 07:29:55 GMT&nbsp; &nbsp;(100kb,D)<br>
<br>
Title: A study on joint modeling and data augmentation of multi-modalities for<br>
&nbsp; audio-visual scene classification<br>
Authors: Qing Wang, Jun Du, Siyuan Zheng, Yunqing Li, Yajian Wang, Yuzhong Wu,<br>
&nbsp; Hu Hu, Chao-Han Huck Yang, Sabato Marco Siniscalchi, Yannan Wang, Chin-Hui<br>
&nbsp; Lee<br>
Categories: cs.MM cs.CV cs.SD eess.AS<br>
Comments: 5 pages, 1 figure<br>
\\<br>
&nbsp; In this paper, we propose two techniques, namely joint modeling and data<br>
augmentation, to improve system performances for audio-visual scene<br>
classification (AVSC). We employ pre-trained networks trained only on image<br>
data sets to extract video embedding; whereas for audio embedding models, we<br>
decide to train them from scratch. We explore different neural network<br>
architectures for joint modeling to effectively combine the video and audio<br>
modalities. Moreover, data augmentation strategies are investigated to increase<br>
audio-visual training set size. For the video modality the effectiveness of<br>
several operations in RandAugment is verified. An audio-video joint mixup<br>
scheme is proposed to further improve AVSC performances. Evaluated on the<br>
development set of TAU Urban Audio Visual Scenes 2021, our final system can<br>
achieve the best accuracy of 94.2% among all single AVSC systems submitted to<br>
DCASE 2021 Task 1b.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04114" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04114&amp;source=gmail&amp;ust=1646963617858000&amp;usg=AOvVaw3kYosDjlNW6kEJgED0-X3q" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04114</a> ,&nbsp; 100kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04118 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 14:42:29 GMT&nbsp; &nbsp;(453kb)<br>
<br>
Title: An Efficient Polyp Segmentation Network<br>
Authors: Tugberk Erol and Duygu Sarikaya<br>
Categories: eess.IV cs.CV<br>
Comments: 4 pages, in Turkish language, 2 figures, 2 tables<br>
\\<br>
&nbsp; Cancer is a disease that occurs as a result of uncontrolled division and<br>
proliferation of cells. The number of cancer cases has been on the rise over<br>
the recent years.. Colon cancer is one of the most common types of cancer in<br>
the world. Polyps that can be seen in the large intestine can cause cancer if<br>
not removed with early intervention. Deep learning and image segmentation<br>
techniques are used to minimize the number of polyps that goes unnoticed by the<br>
experts during the diagnosis. Although these techniques give good results, they<br>
require too many parameters. We propose a new model to solve this problem. Our<br>
proposed model includes less parameters as well as outperforming the success of<br>
the state of the art models. In the proposed model, a partial decoder is used<br>
to reduce the number of parameters while maintaning success. EfficientNetB0,<br>
which gives successfull results as well as requiring few parameters, is used in<br>
the encoder part. Since polyps have variable aspect and aspect ratios, an<br>
asymetric convolution block was used instead of using classic convolution<br>
block. Kvasir and CVC-ClinicDB datasets were seperated as training, validation<br>
and testing, and CVC-ColonDB, ETIS and Endoscene datasets were used for<br>
testing. According to the dice metric, our model had the best results with<br>
%71.8 in the ColonDB test dataset, %89.3 in the EndoScene test dataset and<br>
%74.8 in the ETIS test dataset. Our model requires a total of 2.626.337<br>
parameters. When we compare it in the literature, according to similar studies,<br>
the model that requires the least parameters is U-Net++ with 9.042.177<br>
parameters.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04118" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04118&amp;source=gmail&amp;ust=1646963617858000&amp;usg=AOvVaw0lS1QRE06gbhj3I8AsOyjn" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04118</a> ,&nbsp; 453kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04179 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 16:09:54 GMT&nbsp; &nbsp;(186kb,D)<br>
<br>
Title: Understanding person identification via gait<br>
Authors: Simon Hanisch and Evelyn Muschter and Adamantini Chatzipanagioti and<br>
&nbsp; Shu-Chen Li and Thorsten Strufe<br>
Categories: cs.CR cs.CV<br>
\\<br>
&nbsp; Gait recognition is the process of identifying humans from their bipedal<br>
locomotion such as walking or running. As such gait data is privacy sensitive<br>
information and should be anonymized. With the rise of more and higher quality<br>
gait recording techniques, such as depth cameras or motion capture suits, an<br>
increasing amount of high-quality gait data becomes available which requires<br>
anonymization. As a first step towards developing anonymization techniques for<br>
high-quality gait data, we study different aspects of movement data to quantify<br>
their contribution to the gait recognition process. We first extract categories<br>
of features from the literature on human gait perception and then design<br>
computational experiments for each of the categories which we run against a<br>
gait recognition system. Our results show that gait anonymization is a<br>
challenging process as the data is highly redundant and interdependent.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04179" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04179&amp;source=gmail&amp;ust=1646963617858000&amp;usg=AOvVaw3saZBljZru2lEIFqGcgJ1t" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04179</a> ,&nbsp; 186kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04180 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 16:11:41 GMT&nbsp; &nbsp;(9215kb)<br>
<br>
Title: Tuning-free multi-coil compressed sensing MRI with Parallel Variable<br>
&nbsp; Density Approximate Message Passing (P-VDAMP)<br>
Authors: Charles Millard, Mark Chiew, Jared Tanner, Aaron T. Hess and Boris<br>
&nbsp; Mailhe<br>
Categories: math.NA cs.CV cs.NA eess.IV<br>
Comments: 24 pages, 10 figures. Submitted to Magnetic Resonance in Medicine on<br>
&nbsp; 8th March 2022<br>
\\<br>
&nbsp; Purpose: To develop a tuning-free method for multi-coil compressed sensing<br>
MRI that performs competitively with algorithms with an optimally tuned sparse<br>
parameter.<br>
&nbsp; Theory: The Parallel Variable Density Approximate Message Passing (P-VDAMP)<br>
algorithm is proposed. For Bernoulli random variable density sampling, P-VDAMP<br>
obeys a "state evolution", where the intermediate per-iteration image estimate<br>
is distributed according to the ground truth corrupted by a Gaussian vector<br>
with approximately known covariance. State evolution is leveraged to<br>
automatically tune sparse parameters on-the-fly with Stein's Unbiased Risk<br>
Estimate (SURE).<br>
&nbsp; Methods: P-VDAMP is evaluated on brain, knee and angiogram datasets at<br>
acceleration factors 5 and 10 and compared with four variants of the Fast<br>
Iterative Shrinkage-Thresholding algorithm (FISTA), including two tuning-free<br>
variants from the literature.<br>
&nbsp; Results: The proposed method is found to have a similar reconstruction<br>
quality and time to convergence as FISTA with an optimally tuned sparse<br>
weighting.<br>
&nbsp; Conclusions: P-VDAMP is an efficient, robust and principled method for<br>
on-the-fly parameter tuning that is competitive with optimally tuned FISTA and<br>
offers substantial robustness and reconstruction quality improvements over<br>
competing tuning-free methods.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04180" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04180&amp;source=gmail&amp;ust=1646963617859000&amp;usg=AOvVaw3U4kEBvRp30wZrp-DTdOH3" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04180</a> ,&nbsp; 9215kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03673 (*cross-listing*)<br>
Date: Mon, 7 Mar 2022 19:12:40 GMT&nbsp; &nbsp;(3121kb,D)<br>
<br>
Title: AgraSSt: Approximate Graph Stein Statistics for Interpretable Assessment<br>
&nbsp; of Implicit Graph Generators<br>
Authors: Wenkai Xu and Gesine Reinert<br>
Categories: stat.ML cs.LG<br>
\\<br>
&nbsp; We propose and analyse a novel statistical procedure, coined AgraSSt, to<br>
assess the quality of graph generators that may not be available in explicit<br>
form. In particular, AgraSSt can be used to determine whether a learnt graph<br>
generating process is capable of generating graphs that resemble a given input<br>
graph. Inspired by Stein operators for random graphs, the key idea of AgraSSt<br>
is the construction of a kernel discrepancy based on an operator obtained from<br>
the graph generator. AgraSSt can provide interpretable criticisms for a graph<br>
generator training procedure and help identify reliable sample batches for<br>
downstream tasks. Using Stein`s method we give theoretical guarantees for a<br>
broad class of random graph models. We provide empirical results on both<br>
synthetic input graphs with known graph generation procedures, and real-world<br>
input graphs that the state-of-the-art (deep) generative models for graphs are<br>
trained on.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03673" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03673&amp;source=gmail&amp;ust=1646963617859000&amp;usg=AOvVaw3aHuor1thtuWDW3xNDcw0l" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03673</a> ,&nbsp; 3121kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03706 (*cross-listing*)<br>
Date: Mon, 7 Mar 2022 21:13:54 GMT&nbsp; &nbsp;(162kb,D)<br>
<br>
Title: Detection of AI Synthesized Hindi Speech<br>
Authors: Karan Bhatia (1), Ansh Agrawal (1), Priyanka Singh (1) and Arun Kumar<br>
&nbsp; Singh (2) ((1) Dhirubhai Ambani Institute of Information and Communication<br>
&nbsp; Technology, (2) Indian Institute of Technology Jammu)<br>
Categories: cs.SD cs.LG eess.AS<br>
Comments: 5 Pages, 6 Figures, 4 Tables<br>
\\<br>
&nbsp; The recent advancements in generative artificial speech models have made<br>
possible the generation of highly realistic speech signals. At first, it seems<br>
exciting to obtain these artificially synthesized signals such as speech clones<br>
or deep fakes but if left unchecked, it may lead us to digital dystopia. One of<br>
the primary focus in audio forensics is validating the authenticity of a<br>
speech. Though some solutions are proposed for English speeches but the<br>
detection of synthetic Hindi speeches have not gained much attention. Here, we<br>
propose an approach for discrimination of AI synthesized Hindi speech from an<br>
actual human speech. We have exploited the Bicoherence Phase, Bicoherence<br>
Magnitude, Mel Frequency Cepstral Coefficient (MFCC), Delta Cepstral, and Delta<br>
Square Cepstral as the discriminating features for machine learning models.<br>
Also, we extend the study to using deep neural networks for extensive<br>
experiments, specifically VGG16 and homemade CNN as the architecture models. We<br>
obtained an accuracy of 99.83% with VGG16 and 99.99% with homemade CNN models.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03706" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03706&amp;source=gmail&amp;ust=1646963617859000&amp;usg=AOvVaw1AyFHaP5UpR1ghwTHHNqOi" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03706</a> ,&nbsp; 162kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03713 (*cross-listing*)<br>
Date: Wed, 16 Feb 2022 11:59:53 GMT&nbsp; &nbsp;(997kb)<br>
<br>
Title: A Predictive Model for Student Performance in Classrooms Using Student<br>
&nbsp; Interactions With an eTextbook<br>
Authors: Ahmed Abd Elrahman, Taysir Hassan A Soliman, Ahmed I. Taloba, and<br>
&nbsp; Mohammed F. Farghally<br>
Categories: cs.CY cs.LG<br>
Comments: 21 pages,11 figures<br>
\\<br>
&nbsp; With the rise of online eTextbooks and Massive Open Online Courses (MOOCs), a<br>
huge amount of data has been collected related to students' learning. With the<br>
careful analysis of this data, educators can gain useful insights into the<br>
performance of their students and their behavior in learning a particular<br>
topic. This paper proposes a new model for predicting student performance based<br>
on an analysis of how students interact with an interactive online eTextbook.<br>
By being able to predict students' performance early in the course, educators<br>
can easily identify students at risk and provide a suitable intervention. We<br>
considered two main issues the prediction of good/bad performance and the<br>
prediction of the final exam grade. To build the proposed model, we evaluated<br>
the most popular classification and regression algorithms on data from a data<br>
structures and algorithms course (CS2) offered in a large public research<br>
university. Random Forest Regression and Multiple Linear Regression have been<br>
applied in Regression. While Logistic Regression, decision tree, Random Forest<br>
Classifier, K Nearest Neighbors, and Support Vector Machine have been applied<br>
in classification.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03713" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03713&amp;source=gmail&amp;ust=1646963617859000&amp;usg=AOvVaw23TNo7gYjGYCfF2ymlDPn1" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03713</a> ,&nbsp; 997kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03716 (*cross-listing*)<br>
Date: Mon, 21 Feb 2022 02:33:34 GMT&nbsp; &nbsp;(5220kb,D)<br>
<br>
Title: Open-Ended Knowledge Tracing<br>
Authors: Naiming Liu, Zichao Wang, Richard G. Baraniuk, Andrew Lan<br>
Categories: cs.CY cs.LG<br>
\\<br>
&nbsp; Knowledge tracing refers to the problem of estimating each student's<br>
knowledge component/skill mastery level from their past responses to questions<br>
in educational applications. One direct benefit knowledge tracing methods<br>
provide is the ability to predict each student's performance on the future<br>
questions. However, one key limitation of most existing knowledge tracing<br>
methods is that they treat student responses to questions as binary-valued,<br>
i.e., whether the responses are correct or incorrect. Response correctness<br>
analysis/prediction is easy to navigate but loses important information,<br>
especially for open-ended questions: the exact student responses can<br>
potentially provide much more information about their knowledge states than<br>
only response correctness. In this paper, we present our first exploration into<br>
open-ended knowledge tracing, i.e., the analysis and prediction of students'<br>
open-ended responses to questions in the knowledge tracing setup. We first lay<br>
out a generic framework for open-ended knowledge tracing before detailing its<br>
application to the domain of computer science education with programming<br>
questions. We define a series of evaluation metrics in this domain and conduct<br>
a series of quantitative and qualitative experiments to test the boundaries of<br>
open-ended knowledge tracing methods on a real-world student code dataset.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03716" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03716&amp;source=gmail&amp;ust=1646963617859000&amp;usg=AOvVaw2RHgW54IgNFD4tZ3C0yRPx" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03716</a> ,&nbsp; 5220kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03722 (*cross-listing*)<br>
Date: Tue, 1 Mar 2022 03:53:19 GMT&nbsp; &nbsp;(932kb,D)<br>
<br>
Title: Cognitive Diagnosis with Explicit Student Vector Estimation and<br>
&nbsp; Unsupervised Question Matrix Learning<br>
Authors: Lu Dong, Zhenhua Ling, Qiang Ling and Zefeng Lai<br>
Categories: cs.CY cs.LG cs.SI<br>
Comments: 9 pages, 6 figures<br>
\\<br>
&nbsp; Cognitive diagnosis is an essential task in many educational applications.<br>
Many solutions have been designed in the literature. The deterministic input,<br>
noisy "and" gate (DINA) model is a classical cognitive diagnosis model and can<br>
provide interpretable cognitive parameters, e.g., student vectors. However, the<br>
assumption of the probabilistic part of DINA is too strong, because it assumes<br>
that the slip and guess rates of questions are student-independent. Besides,<br>
the question matrix (i.e., Q-matrix) recording the skill distribution of the<br>
questions in the cognitive diagnosis domain often requires precise labels given<br>
by domain experts. Thus, we propose an explicit student vector estimation<br>
(ESVE) method to estimate the student vectors of DINA with a local<br>
self-consistent test, which does not rely on any assumptions for the<br>
probabilistic part of DINA. Then, based on the estimated student vectors, the<br>
probabilistic part of DINA can be modified to a student dependent model that<br>
the slip and guess rates are related to student vectors. Furthermore, we<br>
propose an unsupervised method called heuristic bidirectional calibration<br>
algorithm (HBCA) to label the Q-matrix automatically, which connects the<br>
question difficulty relation and the answer results for initialization and uses<br>
the fault tolerance of ESVE-DINA for calibration. The experimental results on<br>
two real-world datasets show that ESVE-DINA outperforms the DINA model on<br>
accuracy and that the Q-matrix labeled automatically by HBCA can achieve<br>
performance comparable to that obtained with the manually labeled Q-matrix when<br>
using the same model structure.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03722" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03722&amp;source=gmail&amp;ust=1646963617859000&amp;usg=AOvVaw08-AFMcAJ4yGO5xOhTwlfs" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03722</a> ,&nbsp; 932kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03791 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 01:08:35 GMT&nbsp; &nbsp;(584kb,D)<br>
<br>
Title: Data adaptive RKHS Tikhonov regularization for learning kernels in<br>
&nbsp; operators<br>
Authors: Fei Lu, Quanjun Lang and Qingci An<br>
Categories: stat.ML cs.LG<br>
\\<br>
&nbsp; We present DARTR: a Data Adaptive RKHS Tikhonov Regularization method for the<br>
linear inverse problem of nonparametric learning of function parameters in<br>
operators. A key ingredient is a system intrinsic data-adaptive (SIDA) RKHS,<br>
whose norm restricts the learning to take place in the function space of<br>
identifiability. DARTR utilizes this norm and selects the regularization<br>
parameter by the L-curve method. We illustrate its performance in examples<br>
including integral operators, nonlinear operators and nonlocal operators with<br>
discrete synthetic data. Numerical results show that DARTR leads to an accurate<br>
estimator robust to both numerical error due to discrete data and noise in<br>
data, and the estimator converges at a consistent rate as the data mesh refines<br>
under different levels of noises, outperforming two baseline regularizers using<br>
$l^2$ and $L^2$ norms.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03791" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03791&amp;source=gmail&amp;ust=1646963617860000&amp;usg=AOvVaw3_B1pp1nzbuDCxEvEXrdwn" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03791</a> ,&nbsp; 584kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03797 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 01:36:48 GMT&nbsp; &nbsp;(9742kb,D)<br>
<br>
Title: Learning Sensorimotor Primitives of Sequential Manipulation Tasks from<br>
&nbsp; Visual Demonstrations<br>
Authors: Junchi Liang, Bowen Wen, Kostas Bekris and Abdeslam Boularias<br>
Categories: cs.RO cs.LG<br>
\\<br>
&nbsp; This work aims to learn how to perform complex robot manipulation tasks that<br>
are composed of several, consecutively executed low-level sub-tasks, given as<br>
input a few visual demonstrations of the tasks performed by a person. The<br>
sub-tasks consist of moving the robot's end-effector until it reaches a<br>
sub-goal region in the task space, performing an action, and triggering the<br>
next sub-task when a pre-condition is met. Most prior work in this domain has<br>
been concerned with learning only low-level tasks, such as hitting a ball or<br>
reaching an object and grasping it. This paper describes a new neural<br>
network-based framework for learning simultaneously low-level policies as well<br>
as high-level policies, such as deciding which object to pick next or where to<br>
place it relative to other objects in the scene. A key feature of the proposed<br>
approach is that the policies are learned directly from raw videos of task<br>
demonstrations, without any manual annotation or post-processing of the data.<br>
Empirical results on object manipulation tasks with a robotic arm show that the<br>
proposed network can efficiently learn from real visual demonstrations to<br>
perform the tasks, and outperforms popular imitation learning algorithms.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03797" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03797&amp;source=gmail&amp;ust=1646963617860000&amp;usg=AOvVaw1DPrDyx80MONCCBw1NxEuy" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03797</a> ,&nbsp; 9742kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03808 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 02:02:32 GMT&nbsp; &nbsp;(691kb,D)<br>
<br>
Title: A Fast Scale-Invariant Algorithm for Non-negative Least Squares with<br>
&nbsp; Non-negative Data<br>
Authors: Jelena Diakonikolas, Chenghui Li, Swati Padmanabhan, Chaobing Song<br>
Categories: math.OC cs.LG stat.ML<br>
\\<br>
&nbsp; Nonnegative (linear) least square problems are a fundamental class of<br>
problems that is well-studied in statistical learning and for which solvers<br>
have been implemented in many of the standard programming languages used within<br>
the machine learning community. The existing off-the-shelf solvers view the<br>
non-negativity constraint in these problems as an obstacle and, compared to<br>
unconstrained least squares, perform additional effort to address it. However,<br>
in many of the typical applications, the data itself is nonnegative as well,<br>
and we show that the nonnegativity in this case makes the problem easier. In<br>
particular, while the oracle complexity of unconstrained least squares problems<br>
necessarily scales with one of the data matrix constants (typically the<br>
spectral norm) and these problems are solved to additive error, we show that<br>
nonnegative least squares problems with nonnegative data are solvable to<br>
multiplicative error and with complexity that is independent of any matrix<br>
constants. The algorithm we introduce is accelerated and based on a primal-dual<br>
perspective. We further show how to provably obtain linear convergence using<br>
adaptive restart coupled with our method and demonstrate its effectiveness on<br>
large-scale data via numerical experiments.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03808" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03808&amp;source=gmail&amp;ust=1646963617860000&amp;usg=AOvVaw2Z_U-8pU_qFmP0aCTfyH1b" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03808</a> ,&nbsp; 691kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03828 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 03:26:08 GMT&nbsp; &nbsp;(2711kb,D)<br>
<br>
Title: Informative Planning for Worst-Case Error Minimisation in Sparse<br>
&nbsp; Gaussian Process Regression<br>
Authors: Jennifer Wakulicz, Ki Myung Brian Lee, Chanyeol Yoo, Teresa<br>
&nbsp; Vidal-Calleja, Robert Fitch<br>
Categories: cs.RO cs.LG<br>
Comments: 7 pages, 6 figures, accepted to Proc. of ICRA 2022<br>
\\<br>
&nbsp; We present a planning framework for minimising the deterministic worst-case<br>
error in sparse Gaussian process (GP) regression. We first derive a universal<br>
worst-case error bound for sparse GP regression with bounded noise using<br>
interpolation theory on reproducing kernel Hilbert spaces (RKHSs). By<br>
exploiting the conditional independence (CI) assumption central to sparse GP<br>
regression, we show that the worst-case error minimisation can be achieved by<br>
solving a posterior entropy minimisation problem. In turn, the posterior<br>
entropy minimisation problem is solved using a Gaussian belief space planning<br>
algorithm. We corroborate the proposed worst-case error bound in a simple 1D<br>
example, and test the planning framework in simulation for a 2D vehicle in a<br>
complex flow field. Our results demonstrate that the proposed posterior entropy<br>
minimisation approach is effective in minimising deterministic error, and<br>
outperforms the conventional measurement entropy maximisation formulation when<br>
the inducing points are fixed.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03828" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03828&amp;source=gmail&amp;ust=1646963617860000&amp;usg=AOvVaw1f-lcIGkEMHFJBFF0c0fXg" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03828</a> ,&nbsp; 2711kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03875 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 06:26:50 GMT&nbsp; &nbsp;(3247kb,D)<br>
<br>
Title: Occupancy Flow Fields for Motion Forecasting in Autonomous Driving<br>
Authors: Reza Mahjourian, Jinkyu Kim, Yuning Chai, Mingxing Tan, Ben Sapp,<br>
&nbsp; Dragomir Anguelov<br>
Categories: cs.RO cs.LG<br>
Journal-ref: IEEE Robotics and Automation Letters<br>
DOI: 10.1109/LRA.2022.3151613<br>
\\<br>
&nbsp; We propose Occupancy Flow Fields, a new representation for motion forecasting<br>
of multiple agents, an important task in autonomous driving. Our representation<br>
is a spatio-temporal grid with each grid cell containing both the probability<br>
of the cell being occupied by any agent, and a two-dimensional flow vector<br>
representing the direction and magnitude of the motion in that cell. Our method<br>
successfully mitigates shortcomings of the two most commonly-used<br>
representations for motion forecasting: trajectory sets and occupancy grids.<br>
Although occupancy grids efficiently represent the probabilistic location of<br>
many agents jointly, they do not capture agent motion and lose the agent<br>
identities. To this end, we propose a deep learning architecture that generates<br>
Occupancy Flow Fields with the help of a new flow trace loss that establishes<br>
consistency between the occupancy and flow predictions. We demonstrate the<br>
effectiveness of our approach using three metrics on occupancy prediction,<br>
motion estimation, and agent ID recovery. In addition, we introduce the problem<br>
of predicting speculative agents, which are currently-occluded agents that may<br>
appear in the future through dis-occlusion or by entering the field of view. We<br>
report experimental results on a large in-house autonomous driving dataset and<br>
the public INTERACTION dataset, and show that our model outperforms<br>
state-of-the-art models.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03875" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03875&amp;source=gmail&amp;ust=1646963617860000&amp;usg=AOvVaw3y3w9nQ6-nNSEzEGK1zYut" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03875</a> ,&nbsp; 3247kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03899 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 07:44:47 GMT&nbsp; &nbsp;(409kb,D)<br>
<br>
Title: Noisy Low-rank Matrix Optimization: Geometry of Local Minima and<br>
&nbsp; Convergence Rate<br>
Authors: Ziye Ma, Somayeh Sojoudi<br>
Categories: math.OC cs.LG stat.ML<br>
\\<br>
&nbsp; This paper is concerned with low-rank matrix optimization, which has found a<br>
wide range of applications in machine learning. This problem in the special<br>
case of matrix sense has been studied extensively through the notion of<br>
Restricted Isometry Property (RIP), leading to a wealth of results on the<br>
geometric landscape of the problem and the convergence rate of common<br>
algorithms. However, the existing results are not able to handle the problem<br>
with a general objective function subject to noisy data. In this paper, we<br>
address this problem by developing a mathematical framework that can deal with<br>
random corruptions to general objective functions, where the noise model is<br>
arbitrary. We prove that as long as the RIP constant of the noiseless objective<br>
is less than $1/3$, any spurious local solution of the noisy optimization<br>
problem must be close to the ground truth solution. By working through the<br>
strict saddle property, we also show that an approximate solution can be found<br>
in polynomial time. We characterize the geometry of the spurious local minima<br>
of the problem in a local region around the ground truth in the case when the<br>
RIP constant is greater than $1/3$. This paper offers the first set of results<br>
on the global and local optimization landscapes of general low-rank<br>
optimization problems under arbitrary random corruptions.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03899" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03899&amp;source=gmail&amp;ust=1646963617861000&amp;usg=AOvVaw0CoXMIP7HZ9ph5tKCd2tOH" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03899</a> ,&nbsp; 409kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03916 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 08:26:12 GMT&nbsp; &nbsp;(439kb)<br>
<br>
Title: Estimating the average causal effect of intervention in continuous<br>
&nbsp; variables using machine learning<br>
Authors: Yoshiaki Kitazawa<br>
Categories: stat.ML cs.LG<br>
\\<br>
&nbsp; The most widely discussed methods for estimating the Average Causal Effect /<br>
Average Treatment Effect are those for intervention in discrete binary<br>
variables whose value represents the intervention / non-intervention groups. On<br>
the other hand, methods for intervening in continuous variables independent of<br>
the data generating model has not been developed. In this study, we give a<br>
method for estimating the average causal effect for intervention in continuous<br>
variables that can be applied to data of any generating model as long as the<br>
causal effect is identifiable. The proposing method is independent of machine<br>
learning algorithms and preserves the identifiability of the data.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03916" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03916&amp;source=gmail&amp;ust=1646963617861000&amp;usg=AOvVaw3_aSc3xZNEgMx3n6PYmcON" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03916</a> ,&nbsp; 439kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03932 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 08:57:11 GMT&nbsp; &nbsp;(626kb)<br>
<br>
Title: Digital Speech Algorithms for Speaker De-Identification<br>
Authors: Stefano Marinozzi, Marcos Faundez-Zanuy<br>
Categories: cs.SD cs.LG eess.AS<br>
Comments: 4 pages<br>
Journal-ref: 2014 5th IEEE Conference on Cognitive Infocommunications<br>
&nbsp; (CogInfoCom), 2014, pp. 317-320<br>
DOI: 10.1109/CogInfoCom.2014.<wbr>7020470<br>
\\<br>
&nbsp; The present work is based on the COST Action IC1206 for De-identification in<br>
multimedia content. It was performed to test four algorithms of voice<br>
modifications on a speech gender recognizer to find the degree of modification<br>
of pitch when the speech recognizer have the probability of success equal to<br>
the probability of failure. The purpose of this analysis is to assess the<br>
intensity of the speech tone modification, the quality, the reversibility and<br>
not-reversibility of the changes made.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03932" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03932&amp;source=gmail&amp;ust=1646963617861000&amp;usg=AOvVaw3Dz49mmYyxJA7NFlQgk0Rt" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03932</a> ,&nbsp; 626kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03933 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 08:57:25 GMT&nbsp; &nbsp;(725kb)<br>
<br>
Title: A Preliminary Study on Aging Examining Online Handwriting<br>
Authors: Marcos Faundez-Zanuy, Enric Sesa-Nogueras, Josep Roure-Alcob\'e, Anna<br>
&nbsp; Esposito, Jiri Mekyska and Karmele L\'opez-de-Ipi\~na<br>
Categories: cs.HC cs.LG<br>
Comments: 4 pages<br>
Journal-ref: 2014 5th IEEE Conference on Cognitive Infocommunications<br>
&nbsp; (CogInfoCom), 2014, pp. 221-224<br>
DOI: 10.1109/CogInfoCom.2014.<wbr>7020449<br>
\\<br>
&nbsp; In order to develop infocommunications devices so that the capabilities of<br>
the human brain may interact with the capabilities of any artificially<br>
cognitive system a deeper knowledge of aging is necessary. Especially if<br>
society does not want to exclude elder people and wants to develop automatic<br>
systems able to help and improve the quality of life of this group of<br>
population, healthy individuals as well as those with cognitive decline or<br>
other pathologies. This paper tries to establish the variations in handwriting<br>
tasks with the goal to obtain a better knowledge about aging. We present the<br>
correlation results between several parameters extracted from online<br>
handwriting and the age of the writers. It is based on BIOSECURID database,<br>
which consists of 400 people that provided several biometric traits, including<br>
online handwriting. The main idea is to identify those parameters that are more<br>
stable and those more age dependent. One challenging topic for disease diagnose<br>
is the differentiation between healthy and pathological aging. For this<br>
purpose, it is necessary to be aware of handwriting parameters that are, in<br>
general, not affected by aging and those who experiment changes, increase or<br>
decrease their values, because of it. This paper contributes to this research<br>
line analyzing a selected set of online handwriting parameters provided by a<br>
healthy group of population aged from 18 to 70 years. Preliminary results show<br>
that these parameters are not affected by aging and therefore, changes in their<br>
values can only be attributed to motor or cognitive disorders.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03933" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03933&amp;source=gmail&amp;ust=1646963617861000&amp;usg=AOvVaw2kDpbrtWJuGWJIPRHpaq_p" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03933</a> ,&nbsp; 725kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03979 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 10:11:09 GMT&nbsp; &nbsp;(828kb,D)<br>
<br>
Title: Online Weak-form Sparse Identification of Partial Differential Equations<br>
Authors: Daniel A. Messenger, Emiliano Dall'Anese and David M. Bortz<br>
Categories: math.OC cs.LG stat.ML<br>
Comments: 21 pages, 4 figures<br>
MSC-class: 62J07, 68T05, 93-10<br>
\\<br>
&nbsp; This paper presents an online algorithm for identification of partial<br>
differential equations (PDEs) based on the weak-form sparse identification of<br>
nonlinear dynamics algorithm (WSINDy). The algorithm is online in a sense that<br>
if performs the identification task by processing solution snapshots that<br>
arrive sequentially. The core of the method combines a weak-form discretization<br>
of candidate PDEs with an online proximal gradient descent approach to the<br>
sparse regression problem. In particular, we do not regularize the<br>
$\ell_0$-pseudo-norm, instead finding that directly applying its proximal<br>
operator (which corresponds to a hard thresholding) leads to efficient online<br>
system identification from noisy data. We demonstrate the success of the method<br>
on the Kuramoto-Sivashinsky equation, the nonlinear wave equation with<br>
time-varying wavespeed, and the linear wave equation, in one, two, and three<br>
spatial dimensions, respectively. In particular, our examples show that the<br>
method is capable of identifying and tracking systems with coefficients that<br>
vary abruptly in time, and offers a streaming alternative to problems in higher<br>
dimensions.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03979" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03979&amp;source=gmail&amp;ust=1646963617861000&amp;usg=AOvVaw0j_F_yMtFdFSTlz2y4KTFv" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03979</a> ,&nbsp; 828kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04002 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 10:56:46 GMT&nbsp; &nbsp;(60kb)<br>
<br>
Title: Semi-Random Sparse Recovery in Nearly-Linear Time<br>
Authors: Jonathan A. Kelner, Jerry Li, Allen Liu, Aaron Sidford, Kevin Tian<br>
Categories: cs.DS cs.LG math.OC stat.ML<br>
Comments: 42 pages, comments welcome!<br>
\\<br>
&nbsp; Sparse recovery is one of the most fundamental and well-studied inverse<br>
problems. Standard statistical formulations of the problem are provably solved<br>
by general convex programming techniques and more practical, fast<br>
(nearly-linear time) iterative methods. However, these latter "fast algorithms"<br>
have previously been observed to be brittle in various real-world settings.<br>
&nbsp; We investigate the brittleness of fast sparse recovery algorithms to<br>
generative model changes through the lens of studying their robustness to a<br>
"helpful" semi-random adversary, a framework which tests whether an algorithm<br>
overfits to input assumptions. We consider the following basic model: let<br>
$\mathbf{A} \in \mathbb{R}^{n \times d}$ be a measurement matrix which contains<br>
an unknown subset of rows $\mathbf{G} \in \mathbb{R}^{m \times d}$ which are<br>
bounded and satisfy the restricted isometry property (RIP), but is otherwise<br>
arbitrary. Letting $x^\star \in \mathbb{R}^d$ be $s$-sparse, and given either<br>
exact measurements $b = \mathbf{A} x^\star$ or noisy measurements $b =<br>
\mathbf{A} x^\star + \xi$, we design algorithms recovering $x^\star$<br>
information-theoretically optimally in nearly-linear time. We extend our<br>
algorithm to hold for weaker generative models relaxing our planted RIP<br>
assumption to a natural weighted variant, and show that our method's guarantees<br>
naturally interpolate the quality of the measurement matrix to, in some<br>
parameter regimes, run in sublinear time.<br>
&nbsp; Our approach differs from prior fast iterative methods with provable<br>
guarantees under semi-random generative models: natural conditions on a<br>
submatrix which make sparse recovery tractable are NP-hard to verify. We design<br>
a new iterative method tailored to the geometry of sparse recovery which is<br>
provably robust to our semi-random model. We hope our approach opens the door<br>
to new robust, efficient algorithms for natural statistical inverse problems.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04002" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04002&amp;source=gmail&amp;ust=1646963617862000&amp;usg=AOvVaw0vAZiU8YRcHjrKeeXh2Ak7" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04002</a> ,&nbsp; 60kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04015 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 11:21:35 GMT&nbsp; &nbsp;(268kb,D)<br>
<br>
Title: A Compilation Flow for the Generation of CNN Inference Accelerators on<br>
&nbsp; FPGAs<br>
Authors: Seung-Hun Chung and Tarek S. Abdelrahman<br>
Categories: cs.DC cs.AR cs.LG<br>
Comments: 8 pages<br>
\\<br>
&nbsp; We present a compilation flow for the generation of CNN inference<br>
accelerators on FPGAs. The flow translates a frozen model into OpenCL kernels<br>
with the TVM compiler and uses the Intel OpenCL SDK to compile to an FPGA<br>
bitstream. We improve the quality of the generated hardware with optimizations<br>
applied to the base OpenCL kernels generated by TVM. These optimizations<br>
increase parallelism, reduce memory access latency, increase concurrency and<br>
save on-chip resources. We automate these optimizations in TVM and evaluate<br>
them by generating accelerators for LeNet-5, MobileNetV1 and ResNet-34 on an<br>
Intel Stratix~10SX. We show that the optimizations improve the performance of<br>
the generated accelerators by up to 846X over the base accelerators. The<br>
performance of the optimized accelerators is up to 4.57X better than TensorFlow<br>
on CPU, 3.83X better than single-threaded TVM and is only 0.34X compared to TVM<br>
with 56 threads. Our optimized kernels also outperform ones generated by a<br>
similar approach (that also uses high-level synthesis) while providing more<br>
functionality and flexibility. However, it underperforms an approach that<br>
utilizes hand-optimized designs. Thus, we view our approach as useful in<br>
pre-production environments that benefit from increased performance and fast<br>
prototyping, realizing the benefits of FPGAs without hardware design expertise.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04015" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04015&amp;source=gmail&amp;ust=1646963617862000&amp;usg=AOvVaw2p2rlnJLLjsN1v_K_afuG8" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04015</a> ,&nbsp; 268kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04075 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 13:39:44 GMT&nbsp; &nbsp;(3285kb,D)<br>
<br>
Title: Obstacle Aware Sampling for Path Planning<br>
Authors: Murad Tukan and Alaa Maalouf and Dan Feldman and Roi Poranne<br>
Categories: cs.RO cs.LG<br>
\\<br>
&nbsp; Many path planning algorithms are based on sampling the state space. While<br>
this approach is very simple, it can become costly when the obstacles are<br>
unknown, since samples hitting these obstacles are wasted. The goal of this<br>
paper is to efficiently identify obstacles in a map and remove them from the<br>
sampling space. To this end, we propose a pre-processing algorithm for space<br>
exploration that enables more efficient sampling. We show that it can boost the<br>
performance of other space sampling methods and path planners.<br>
&nbsp; Our approach is based on the fact that a convex obstacle can be approximated<br>
provably well by its minimum volume enclosing ellipsoid (MVEE), and a<br>
non-convex obstacle may be partitioned into convex shapes. Our main<br>
contribution is an algorithm that strategically finds a small sample, called<br>
the \emph{active-coreset}, that adaptively samples the space via<br>
membership-oracle such that the MVEE of the coreset approximates the MVEE of<br>
the obstacle. Experimental results confirm the effectiveness of our approach<br>
across multiple planners based on Rapidly-exploring random trees, showing<br>
significant improvement in terms of time and path length.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04075" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04075&amp;source=gmail&amp;ust=1646963617862000&amp;usg=AOvVaw1msC8OI5SDdpUvfApZMWgd" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04075</a> ,&nbsp; 3285kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04088 (*cross-listing*)<br>
Date: Sat, 26 Feb 2022 15:50:42 GMT&nbsp; &nbsp;(2689kb)<br>
<br>
Title: The role of alcohol outlet visits derived from mobile phone location<br>
&nbsp; data in enhancing domestic violence prediction at the neighborhood level<br>
Authors: Ting Chang, Yingjie Hu, Dane Taylor, Brian M. Quigley<br>
Categories: cs.CY cs.LG<br>
Comments: 35 pages<br>
MSC-class: 68T09<br>
Journal-ref: Health &amp; Place, 73, 102736 (2022)<br>
DOI: 10.1016/j.healthplace.2021.<wbr>102736<br>
\\<br>
&nbsp; Domestic violence (DV) is a serious public health issue, with 1 in 3 women<br>
and 1 in 4 men experiencing some form of partner-related violence every year.<br>
Existing research has shown a strong association between alcohol use and DV at<br>
the individual level. Accordingly, alcohol use could also be a predictor for DV<br>
at the neighborhood level, helping identify the neighborhoods where DV is more<br>
likely to happen. However, it is difficult and costly to collect data that can<br>
represent neighborhood-level alcohol use especially for a large geographic<br>
area. In this study, we propose to derive information about the alcohol outlet<br>
visits of the residents of different neighborhoods from anonymized mobile phone<br>
location data, and investigate whether the derived visits can help better<br>
predict DV at the neighborhood level. We use mobile phone data from the company<br>
SafeGraph, which is freely available to researchers and which contains<br>
information about how people visit various points-of-interest including alcohol<br>
outlets. In such data, a visit to an alcohol outlet is identified based on the<br>
GPS point location of the mobile phone and the building footprint (a polygon)<br>
of the alcohol outlet. We present our method for deriving neighborhood-level<br>
alcohol outlet visits, and experiment with four different statistical and<br>
machine learning models to investigate the role of the derived visits in<br>
enhancing DV prediction based on an empirical dataset about DV in Chicago. Our<br>
results reveal the effectiveness of the derived alcohol outlets visits in<br>
helping identify neighborhoods that are more likely to suffer from DV, and can<br>
inform policies related to DV intervention and alcohol outlet licensing.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04088" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04088&amp;source=gmail&amp;ust=1646963617862000&amp;usg=AOvVaw29hHIgDJ6o3_Il3SHJ7BO2" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04088</a> ,&nbsp; 2689kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04115 (*cross-listing*)<br>
Date: Wed, 2 Mar 2022 15:53:38 GMT&nbsp; &nbsp;(408kb,D)<br>
<br>
Title: Biological Sequence Design with GFlowNets<br>
Authors: Moksh Jain, Emmanuel Bengio, Alex-Hernandez Garcia, Jarrid<br>
&nbsp; Rector-Brooks, Bonaventure F. P. Dossou, Chanakya Ekbote, Jie Fu, Tianyu<br>
&nbsp; Zhang, Micheal Kilgour, Dinghuai Zhang, Lena Simine, Payel Das, Yoshua Bengio<br>
Categories: q-bio.BM cs.LG<br>
Comments: 15 pages, 3 figures. Code available at:<br>
&nbsp; <a href="https://github.com/MJ10/BioSeq-GFN-AL" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/MJ10/BioSeq-GFN-AL&amp;source=gmail&amp;ust=1646963617862000&amp;usg=AOvVaw2EVWL6g2khDN0UPlCm60Jy" rel="noreferrer" target="_blank">https://github.com/MJ10/<wbr>BioSeq-GFN-AL</a><br>
\\<br>
&nbsp; Design of de novo biological sequences with desired properties, like protein<br>
and DNA sequences, often involves an active loop with several rounds of<br>
molecule ideation and expensive wet-lab evaluations. These experiments can<br>
consist of multiple stages, with increasing levels of precision and cost of<br>
evaluation, where candidates are filtered. This makes the diversity of proposed<br>
candidates a key consideration in the ideation phase. In this work, we propose<br>
an active learning algorithm leveraging epistemic uncertainty estimation and<br>
the recently proposed GFlowNets as a generator of diverse candidate solutions,<br>
with the objective to obtain a diverse batch of useful (as defined by some<br>
utility function, for example, the predicted anti-microbial activity of a<br>
peptide) and informative candidates after each round. We also propose a scheme<br>
to incorporate existing labeled datasets of candidates, in addition to a reward<br>
function, to speed up learning in GFlowNets. We present empirical results on<br>
several biological sequence design tasks, and we find that our method generates<br>
more diverse and novel batches with high scoring candidates compared to<br>
existing approaches.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04115" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04115&amp;source=gmail&amp;ust=1646963617863000&amp;usg=AOvVaw3ZE6tUCE2HkeuMFvXNobYc" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04115</a> ,&nbsp; 408kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04176 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 16:06:37 GMT&nbsp; &nbsp;(5829kb,D)<br>
<br>
Title: Variational methods for simulation-based inference<br>
Authors: Manuel Gl\"ockler, Michael Deistler, Jakob H. Macke<br>
Categories: stat.ML cs.LG<br>
\\<br>
&nbsp; We present Sequential Neural Variational Inference (SNVI), an approach to<br>
perform Bayesian inference in models with intractable likelihoods. SNVI<br>
combines likelihood-estimation (or likelihood-ratio-estimation) with<br>
variational inference to achieve a scalable simulation-based inference<br>
approach. SNVI maintains the flexibility of likelihood(-ratio) estimation to<br>
allow arbitrary proposals for simulations, while simultaneously providing a<br>
functional estimate of the posterior distribution without requiring MCMC<br>
sampling. We present several variants of SNVI and demonstrate that they are<br>
substantially more computationally efficient than previous algorithms, without<br>
loss of accuracy on benchmark tasks. We apply SNVI to a neuroscience model of<br>
the pyloric network in the crab and demonstrate that it can infer the posterior<br>
distribution with one order of magnitude fewer simulations than previously<br>
reported. SNVI vastly reduces the computational cost of simulation-based<br>
inference while maintaining accuracy and flexibility, making it possible to<br>
tackle problems that were previously inaccessible.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04176" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04176&amp;source=gmail&amp;ust=1646963617863000&amp;usg=AOvVaw2m37WyawiS9v-KFqnX9GNm" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04176</a> ,&nbsp; 5829kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04201 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 17:03:19 GMT&nbsp; &nbsp;(704kb,D)<br>
<br>
Title: Follow the Water: Finding Water, Snow and Clouds on Terrestrial<br>
&nbsp; Exoplanets with Photometry and Machine Learning<br>
Authors: Dang Pham and Lisa Kaltenegger<br>
Categories: astro-ph.EP astro-ph.IM cs.LG<br>
Comments: 6 pages, 5 figures. Accepted for publications in MNRAS Letters<br>
\\<br>
&nbsp; All life on Earth needs water. NASA's quest to follow the water links water<br>
to the search for life in the cosmos. Telescopes like JWST and mission concepts<br>
like HabEx, LUVOIR and Origins are designed to characterise rocky exoplanets<br>
spectroscopically. However, spectroscopy remains time-intensive and therefore,<br>
initial characterisation is critical to prioritisation of targets.<br>
&nbsp; Here, we study machine learning as a tool to assess water's existence through<br>
broadband-filter reflected photometric flux on Earth-like exoplanets in three<br>
forms: seawater, water-clouds and snow; based on 53,130 spectra of cold,<br>
Earth-like planets with 6 major surfaces. XGBoost, a well-known machine<br>
learning algorithm, achieves over 90\% balanced accuracy in detecting the<br>
existence of snow or clouds for S/N$\gtrsim 20$, and 70\% for liquid seawater<br>
for S/N $\gtrsim 30$. Finally, we perform mock Bayesian analysis with<br>
Markov-chain Monte Carlo with five filters identified to derive exact surface<br>
compositions to test for retrieval feasibility.<br>
&nbsp; The results show that the use of machine learning to identify water on the<br>
surface of exoplanets from broadband-filter photometry provides a promising<br>
initial characterisation tool of water in different forms. Planned small and<br>
large telescope missions could use this to aid their prioritisation of targets<br>
for time-intense follow-up observations.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04201" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04201&amp;source=gmail&amp;ust=1646963617863000&amp;usg=AOvVaw2MHM2oOt_KE26e0G37Kd5F" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04201</a> ,&nbsp; 704kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04227 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 17:47:02 GMT&nbsp; &nbsp;(2196kb,D)<br>
<br>
Title: Learning based Age of Information Minimization in UAV-relayed IoT<br>
&nbsp; Networks<br>
Authors: Biplav Choudhury, Prasenjit Karmakar, Vijay K. Shah, Jeffrey H. Reed<br>
Categories: cs.IT cs.LG math.IT<br>
\\<br>
&nbsp; Unmanned Aerial Vehicles (UAVs) are used as aerial base-stations to relay<br>
time-sensitive packets from IoT devices to the nearby terrestrial base-station<br>
(TBS). Scheduling of packets in such UAV-relayed IoT-networks to ensure fresh<br>
(or up-to-date) IoT devices' packets at the TBS is a challenging problem as it<br>
involves two simultaneous steps of (i) sampling of packets generated at IoT<br>
devices by the UAVs [hop-1] and (ii) updating of sampled packets from UAVs to<br>
the TBS [hop-2]. To address this, we propose Age-of-Information (AoI)<br>
scheduling algorithms for two-hop UAV-relayed IoT-networks. First, we propose a<br>
low-complexity AoI scheduler, termed, MAF-MAD that employs Maximum AoI First<br>
(MAF) policy for sampling of IoT devices at UAV (hop-1) and Maximum AoI<br>
Difference (MAD) policy for updating sampled packets from UAV to the TBS<br>
(hop-2). We prove that MAF-MAD is the optimal AoI scheduler under ideal<br>
conditions (lossless wireless channels and generate-at-will traffic-generation<br>
at IoT devices). On the contrary, for general conditions (lossy channel<br>
conditions and varying periodic traffic-generation at IoT devices), a deep<br>
reinforcement learning algorithm, namely, Proximal Policy Optimization<br>
(PPO)-based scheduler is proposed. Simulation results show that the proposed<br>
PPO-based scheduler outperforms other schedulers like MAF-MAD, MAF, and<br>
round-robin in all considered general scenarios.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04227" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04227&amp;source=gmail&amp;ust=1646963617863000&amp;usg=AOvVaw0C1BKtBcV829oJGIM04Oht" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04227</a> ,&nbsp; 2196kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.04249 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 18:07:33 GMT&nbsp; &nbsp;(1413kb)<br>
<br>
Title: Second-life Lithium-ion batteries: A chemistry-agnostic and scalable<br>
&nbsp; health estimation algorithm<br>
Authors: Aki Takahashi, Anirudh Allam, Simona Onori<br>
Categories: eess.SY cs.LG cs.SY<br>
Comments: 22 pages<br>
\\<br>
&nbsp; Battery state of health is an essential metric for diagnosing battery<br>
degradation during testing and operation. While many unique measurements are<br>
possible in the design phase, for practical applications often only<br>
temperature, voltage and current sensing are accessible. This paper presents a<br>
novel combination of machine learning techniques to produce accurate<br>
predictions significantly faster than standard Gaussian processes. The<br>
data-driven approach uses feature generation with simple mathematics, feature<br>
filtering, and bagging, which is validated with publicly available aging<br>
datasets of more than 200 cells with slow and fast charging, across different<br>
cathode chemistries, and for various operating conditions. Based on multiple<br>
training-test partitions, average and median state of health prediction root<br>
mean square error (RMSE) is found to be less than 1.48% and 1.27%,<br>
respectively, with a limited amount of input data, showing the capability of<br>
the approach even when input data and time are limiting factors. The process<br>
developed in this paper has direct applicability to today's incumbent open<br>
challenge of assessing retired batteries on the basis of their residual health,<br>
and therefore nominal remaining useful life, to allow fast classification for<br>
second-life reutilization.<br>
\\ ( <a href="https://arxiv.org/abs/2203.04249" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.04249&amp;source=gmail&amp;ust=1646963617864000&amp;usg=AOvVaw1QQ_JettvHW_HTNv9ekXbZ" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>04249</a> ,&nbsp; 1413kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03850 (*cross-listing*)<br>
Date: Tue, 8 Mar 2022 04:48:07 GMT&nbsp; &nbsp;(404kb,D)<br>
<br>
Title: UniXcoder: Unified Cross-Modal Pre-training for Code Representation<br>
Authors: Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin<br>
Categories: cs.CL cs.PL cs.SE<br>
Comments: Published in ACL 2022<br>
\\<br>
&nbsp; Pre-trained models for programming languages have recently demonstrated great<br>
success on code intelligence. To support both code-related understanding and<br>
generation tasks, recent works attempt to pre-train unified encoder-decoder<br>
models. However, such encoder-decoder framework is sub-optimal for<br>
auto-regressive tasks, especially code completion that requires a decoder-only<br>
manner for efficient inference. In this paper, we present UniXcoder, a unified<br>
cross-modal pre-trained model for programming language. The model utilizes mask<br>
attention matrices with prefix adapters to control the behavior of the model<br>
and leverages cross-modal contents like AST and code comment to enhance code<br>
representation. To encode AST that is represented as a tree in parallel, we<br>
propose a one-to-one mapping method to transform AST in a sequence structure<br>
that retains all structural information from the tree. Furthermore, we propose<br>
to utilize multi-modal contents to learn representation of code fragment with<br>
contrastive learning, and then align representations among programming<br>
languages using a cross-modal generation task. We evaluate UniXcoder on five<br>
code-related tasks over nine datasets. To further evaluate the performance of<br>
code fragment representation, we also construct a dataset for a new task,<br>
called zero-shot code-to-code search. Results show that our model achieves<br>
state-of-the-art performance on most tasks and analysis reveals that comment<br>
and AST can both enhance UniXcoder.<br>
\\ ( <a href="https://arxiv.org/abs/2203.03850" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03850&amp;source=gmail&amp;ust=1646963617864000&amp;usg=AOvVaw0EyHOQGBYGWOoEKsd6veHY" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03850</a> ,&nbsp; 404kb)<br>
%%--%%--%%--%%--%%--%%--%%--%%<wbr>--%%--%%--%%--%%--%%--%%--%%--<wbr>%%--%%--%%--%%--%%<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2102.02959<br>
replaced with revised version Tue, 8 Mar 2022 02:13:44 GMT&nbsp; &nbsp;(1554kb)<br>
<br>
Title: Multi-Label Annotation of Chest Abdomen Pelvis Computed Tomography Text<br>
&nbsp; Reports Using Deep Learning<br>
Authors: Vincent M. D'Anniballe, Fakrul Islam Tushar, Khrystyna Faryna, Songyue<br>
&nbsp; Han, Maciej A. Mazurowski, Geoffrey D. Rubin, Joseph Y. Lo<br>
Categories: cs.AI cs.CL cs.LG<br>
\\ ( <a href="https://arxiv.org/abs/2102.02959" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2102.02959&amp;source=gmail&amp;ust=1646963617864000&amp;usg=AOvVaw1NNmaMXi-uUoaJR6n1Dnmv" rel="noreferrer" target="_blank">https://arxiv.org/abs/2102.<wbr>02959</a> ,&nbsp; 1554kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2202.13785<br>
replaced with revised version Tue, 8 Mar 2022 06:23:24 GMT&nbsp; &nbsp;(112kb,D)<br>
<br>
Title: CAKE: A Scalable Commonsense-Aware Framework For Multi-View Knowledge<br>
&nbsp; Graph Completion<br>
Authors: Guanglin Niu, Bo Li, Yongfei Zhang, Shiliang Pu<br>
Categories: cs.AI cs.CL<br>
Comments: The full version of a long paper accepted to ACL 2022 main conference<br>
\\ ( <a href="https://arxiv.org/abs/2202.13785" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2202.13785&amp;source=gmail&amp;ust=1646963617864000&amp;usg=AOvVaw3vNzpYkT6QTRpN6qg-UC7n" rel="noreferrer" target="_blank">https://arxiv.org/abs/2202.<wbr>13785</a> ,&nbsp; 112kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:1907.01696<br>
replaced with revised version Tue, 8 Mar 2022 06:53:31 GMT&nbsp; &nbsp;(0kb,I)<br>
<br>
Title: A Semi-Supervised Framework for Automatic Pixel-Wise Breast Cancer<br>
&nbsp; Grading of Histological Images<br>
Authors: Yanyuet Man, Xiangyun Ding, Xingcheng Yao, Han Bao<br>
Categories: cs.CV<br>
Comments: The author list and contents of this paper is not complete. Other<br>
&nbsp; authors request to withdraw this paper<br>
\\ ( <a href="https://arxiv.org/abs/1907.01696" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/1907.01696&amp;source=gmail&amp;ust=1646963617864000&amp;usg=AOvVaw3RaTFoTVyFwUU3LOwxmIa0" rel="noreferrer" target="_blank">https://arxiv.org/abs/1907.<wbr>01696</a> ,&nbsp; 0kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2011.15079<br>
replaced with revised version Tue, 8 Mar 2022 18:59:31 GMT&nbsp; &nbsp;(5059kb,D)<br>
<br>
Title: Forecasting Characteristic 3D Poses of Human Actions<br>
Authors: Christian Diller, Thomas Funkhouser, Angela Dai<br>
Categories: cs.CV cs.LG<br>
Comments: CVPR 2022; Project Page: <a href="https://charposes.christian-diller.de/" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://charposes.christian-diller.de/&amp;source=gmail&amp;ust=1646963617864000&amp;usg=AOvVaw1ekV6zUHcCGhbatY6CUj4f" rel="noreferrer" target="_blank">https://charposes.christian-<wbr>diller.de/</a>;<br>
&nbsp; Paper Video: <a href="https://youtu.be/kVhn8OWMgME" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://youtu.be/kVhn8OWMgME&amp;source=gmail&amp;ust=1646963617864000&amp;usg=AOvVaw3kOgp6gTO-y-GGKndRYzDt" rel="noreferrer" target="_blank">https://youtu.be/kVhn8OWMgME</a><br>
\\ ( <a href="https://arxiv.org/abs/2011.15079" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2011.15079&amp;source=gmail&amp;ust=1646963617864000&amp;usg=AOvVaw3cJzlwajoQdfePN8NC30ys" rel="noreferrer" target="_blank">https://arxiv.org/abs/2011.<wbr>15079</a> ,&nbsp; 5059kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2106.02320<br>
replaced with revised version Tue, 8 Mar 2022 00:20:03 GMT&nbsp; &nbsp;(764kb,D)<br>
<br>
Title: Few-Shot Segmentation via Cycle-Consistent Transformer<br>
Authors: Gengwei Zhang, Guoliang Kang, Yi Yang, Yunchao Wei<br>
Categories: cs.CV<br>
Comments: Advances in Neural Information Processing Systems (NeurIPS), 2021.<br>
&nbsp; Project: <a href="https://github.com/GengDavid/CyCTR" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/GengDavid/CyCTR&amp;source=gmail&amp;ust=1646963617865000&amp;usg=AOvVaw3EASqL3L3m9iJKajDvlApP" rel="noreferrer" target="_blank">https://github.com/GengDavid/<wbr>CyCTR</a><br>
\\ ( <a href="https://arxiv.org/abs/2106.02320" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2106.02320&amp;source=gmail&amp;ust=1646963617865000&amp;usg=AOvVaw1Rakg7jHgtwnQZU9866O73" rel="noreferrer" target="_blank">https://arxiv.org/abs/2106.<wbr>02320</a> ,&nbsp; 764kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2106.07617<br>
replaced with revised version Tue, 8 Mar 2022 03:09:40 GMT&nbsp; &nbsp;(2818kb,D)<br>
<br>
Title: Delving Deep into the Generalization of Vision Transformers under<br>
&nbsp; Distribution Shifts<br>
Authors: Chongzhi Zhang, Mingyuan Zhang, Shanghang Zhang, Daisheng Jin, Qiang<br>
&nbsp; Zhou, Zhongang Cai, Haiyu Zhao, Xianglong Liu, Ziwei Liu<br>
Categories: cs.CV<br>
Comments: Accepted by CVPR 2022<br>
\\ ( <a href="https://arxiv.org/abs/2106.07617" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2106.07617&amp;source=gmail&amp;ust=1646963617865000&amp;usg=AOvVaw3Lg1RsFWykohq4SkA6JjDD" rel="noreferrer" target="_blank">https://arxiv.org/abs/2106.<wbr>07617</a> ,&nbsp; 2818kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2108.12790<br>
replaced with revised version Tue, 8 Mar 2022 14:23:55 GMT&nbsp; &nbsp;(1521kb,D)<br>
<br>
Title: RPR-Net: A Point Cloud-based Rotation-aware Large Scale Place<br>
&nbsp; Recognition Network<br>
Authors: Zhaoxin Fan, Zhenbo Song, Wenping Zhang, Hongyan Liu, Jun He, and<br>
&nbsp; Xiaoyong Du<br>
Categories: cs.CV<br>
\\ ( <a href="https://arxiv.org/abs/2108.12790" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2108.12790&amp;source=gmail&amp;ust=1646963617865000&amp;usg=AOvVaw3yu5yY_lMo0W3Vv4DiT9V5" rel="noreferrer" target="_blank">https://arxiv.org/abs/2108.<wbr>12790</a> ,&nbsp; 1521kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2109.01359<br>
replaced with revised version Tue, 8 Mar 2022 13:15:06 GMT&nbsp; &nbsp;(2723kb,D)<br>
<br>
Title: CAM-loss: Towards Learning Spatially Discriminative Feature<br>
&nbsp; Representations<br>
Authors: Chaofei Wang, Jiayu Xiao, Yizeng Han, Qisen Yang, Shiji Song, Gao<br>
&nbsp; Huang<br>
Categories: cs.CV<br>
Comments: Accepted by ICCV2021<br>
\\ ( <a href="https://arxiv.org/abs/2109.01359" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2109.01359&amp;source=gmail&amp;ust=1646963617865000&amp;usg=AOvVaw1YRsmSqUCXxNi_ys8iBSMY" rel="noreferrer" target="_blank">https://arxiv.org/abs/2109.<wbr>01359</a> ,&nbsp; 2723kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2109.05205<br>
replaced with revised version Tue, 8 Mar 2022 05:41:45 GMT&nbsp; &nbsp;(511kb,D)<br>
<br>
Title: Contrastive Quantization with Code Memory for Unsupervised Image<br>
&nbsp; Retrieval<br>
Authors: Jinpeng Wang, Ziyun Zeng, Bin Chen, Tao Dai, Shu-Tao Xia<br>
Categories: cs.CV cs.AI cs.IR<br>
Comments: Accepted for AAAI'22 (Oral). 9 pages, 4 figures, 3 tables<br>
\\ ( <a href="https://arxiv.org/abs/2109.05205" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2109.05205&amp;source=gmail&amp;ust=1646963617865000&amp;usg=AOvVaw1WIXTK2dZXLfmfhHegI4hJ" rel="noreferrer" target="_blank">https://arxiv.org/abs/2109.<wbr>05205</a> ,&nbsp; 511kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2109.05565<br>
replaced with revised version Tue, 8 Mar 2022 17:12:22 GMT&nbsp; &nbsp;(3566kb,D)<br>
<br>
Title: SphereFace Revived: Unifying Hyperspherical Face Recognition<br>
Authors: Weiyang Liu, Yandong Wen, Bhiksha Raj, Rita Singh, Adrian Weller<br>
Categories: cs.CV cs.AI cs.LG<br>
Comments: Accepted to IEEE Transactions on Pattern Analysis and Machine<br>
&nbsp; Intelligence<br>
\\ ( <a href="https://arxiv.org/abs/2109.05565" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2109.05565&amp;source=gmail&amp;ust=1646963617865000&amp;usg=AOvVaw2cPQ57Mnp0ncVjPQPO0AYa" rel="noreferrer" target="_blank">https://arxiv.org/abs/2109.<wbr>05565</a> ,&nbsp; 3566kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2110.01775<br>
replaced with revised version Tue, 8 Mar 2022 14:50:25 GMT&nbsp; &nbsp;(8310kb,D)<br>
<br>
Title: Deep Instance Segmentation with High-Resolution Automotive Radar<br>
Authors: Jianan Liu, Weiyi Xiong, Liping Bai, Yuxuan Xia, Tao Huang, Wanli<br>
&nbsp; Ouyang, Bing Zhu<br>
Categories: cs.CV eess.SP<br>
\\ ( <a href="https://arxiv.org/abs/2110.01775" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2110.01775&amp;source=gmail&amp;ust=1646963617865000&amp;usg=AOvVaw22gyrqq22XB321Y-If6Kc3" rel="noreferrer" target="_blank">https://arxiv.org/abs/2110.<wbr>01775</a> ,&nbsp; 8310kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2110.05145<br>
replaced with revised version Tue, 8 Mar 2022 06:20:18 GMT&nbsp; &nbsp;(7633kb,D)<br>
<br>
Title: Sim2Air - Synthetic aerial dataset for UAV monitoring<br>
Authors: Antonella Barisic and Frano Petric and Stjepan Bogdan<br>
Categories: cs.CV cs.RO<br>
Journal-ref: IEEE Robotics and Automation Letters (vol. 7, no. 2, pp.<br>
&nbsp; 3757-3764, April 2022)<br>
DOI: 10.1109/LRA.2022.3147337<br>
\\ ( <a href="https://arxiv.org/abs/2110.05145" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2110.05145&amp;source=gmail&amp;ust=1646963617865000&amp;usg=AOvVaw1vpMP39N03RqzmCB_oTIg2" rel="noreferrer" target="_blank">https://arxiv.org/abs/2110.<wbr>05145</a> ,&nbsp; 7633kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2111.06021<br>
replaced with revised version Tue, 8 Mar 2022 10:00:37 GMT&nbsp; &nbsp;(8191kb,D)<br>
<br>
Title: Probabilistic Contrastive Learning for Domain Adaptation<br>
Authors: Junjie Li, Yixin Zhang, Zilei Wang, Keyu Tu<br>
Categories: cs.CV<br>
Comments: 12 pages,4 figures<br>
\\ ( <a href="https://arxiv.org/abs/2111.06021" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2111.06021&amp;source=gmail&amp;ust=1646963617865000&amp;usg=AOvVaw1g9MsfJ0GdIQaqhHKf-ewg" rel="noreferrer" target="_blank">https://arxiv.org/abs/2111.<wbr>06021</a> ,&nbsp; 8191kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2111.09136<br>
replaced with revised version Tue, 8 Mar 2022 02:46:03 GMT&nbsp; &nbsp;(1346kb,D)<br>
<br>
Title: IntraQ: Learning Synthetic Images with Intra-Class Heterogeneity for<br>
&nbsp; Zero-Shot Network Quantization<br>
Authors: Yunshan Zhong, Mingbao Lin, Gongrui Nan, Jianzhuang Liu, Baochang<br>
&nbsp; Zhang, Yonghong Tian, Rongrong Ji<br>
Categories: cs.CV<br>
Comments: CVPR2022<br>
\\ ( <a href="https://arxiv.org/abs/2111.09136" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2111.09136&amp;source=gmail&amp;ust=1646963617866000&amp;usg=AOvVaw2myXlz_KYyfiPr2S3viALJ" rel="noreferrer" target="_blank">https://arxiv.org/abs/2111.<wbr>09136</a> ,&nbsp; 1346kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2111.09452<br>
replaced with revised version Mon, 7 Mar 2022 21:13:29 GMT&nbsp; &nbsp;(2830kb,D)<br>
<br>
Title: Open Vocabulary Object Detection with Pseudo Bounding-Box Labels<br>
Authors: Mingfei Gao, Chen Xing, Juan Carlos Niebles, Junnan Li, Ran Xu, Wenhao<br>
&nbsp; Liu, Caiming Xiong<br>
Categories: cs.CV<br>
\\ ( <a href="https://arxiv.org/abs/2111.09452" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2111.09452&amp;source=gmail&amp;ust=1646963617866000&amp;usg=AOvVaw2D2dMiNOZ0_Vx_Je23FJ8J" rel="noreferrer" target="_blank">https://arxiv.org/abs/2111.<wbr>09452</a> ,&nbsp; 2830kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2111.13445<br>
replaced with revised version Mon, 7 Mar 2022 21:20:05 GMT&nbsp; &nbsp;(468kb,D)<br>
<br>
Title: How Well Do Sparse Imagenet Models Transfer?<br>
Authors: Eugenia Iofinova and Alexandra Peste and Mark Kurtz and Dan Alistarh<br>
Categories: cs.CV cs.AI cs.LG<br>
Comments: Accepted to CVPR'22 20 pages, 8 figures (including appendix)<br>
\\ ( <a href="https://arxiv.org/abs/2111.13445" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2111.13445&amp;source=gmail&amp;ust=1646963617866000&amp;usg=AOvVaw2MxarHC36_b9XySr6V302u" rel="noreferrer" target="_blank">https://arxiv.org/abs/2111.<wbr>13445</a> ,&nbsp; 468kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2112.01740<br>
replaced with revised version Tue, 8 Mar 2022 02:43:56 GMT&nbsp; &nbsp;(5657kb,D)<br>
<br>
Title: AirDet: Few-Shot Detection without Fine-tuning for Autonomous<br>
&nbsp; Exploration<br>
Authors: Bowen Li, Chen Wang, Pranay Reddy, Seungchan Kim, Sebastian Scherer<br>
Categories: cs.CV<br>
Comments: 23 pages, 9 figures<br>
\\ ( <a href="https://arxiv.org/abs/2112.01740" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2112.01740&amp;source=gmail&amp;ust=1646963617866000&amp;usg=AOvVaw0JucUXTcDes80vkoxTYajU" rel="noreferrer" target="_blank">https://arxiv.org/abs/2112.<wbr>01740</a> ,&nbsp; 5657kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2112.02891<br>
replaced with revised version Tue, 8 Mar 2022 18:43:08 GMT&nbsp; &nbsp;(4304kb,D)<br>
<br>
Title: Seeing BDD100K in dark: Single-Stage Night-time Object Detection via<br>
&nbsp; Continual Fourier Contrastive Learning<br>
Authors: Ujjal Kr Dutta<br>
Categories: cs.CV cs.AI cs.LG<br>
\\ ( <a href="https://arxiv.org/abs/2112.02891" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2112.02891&amp;source=gmail&amp;ust=1646963617866000&amp;usg=AOvVaw3iLVufYt1Jx1KXP-EK55eg" rel="noreferrer" target="_blank">https://arxiv.org/abs/2112.<wbr>02891</a> ,&nbsp; 4304kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2112.04585<br>
replaced with revised version Mon, 7 Mar 2022 20:37:37 GMT&nbsp; &nbsp;(1458kb,D)<br>
<br>
Title: MASTAF: A Model-Agnostic Spatio-Temporal Attention Fusion Network for<br>
&nbsp; Few-shot Video Classification<br>
Authors: Rex Liu, Huanle Zhang, Hamed Pirsiavash, Xin Liu<br>
Categories: cs.CV cs.LG<br>
\\ ( <a href="https://arxiv.org/abs/2112.04585" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2112.04585&amp;source=gmail&amp;ust=1646963617866000&amp;usg=AOvVaw3ZJrGKxboZUhYqw737FCse" rel="noreferrer" target="_blank">https://arxiv.org/abs/2112.<wbr>04585</a> ,&nbsp; 1458kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2112.10741<br>
replaced with revised version Tue, 8 Mar 2022 18:18:49 GMT&nbsp; &nbsp;(22942kb,D)<br>
<br>
Title: GLIDE: Towards Photorealistic Image Generation and Editing with<br>
&nbsp; Text-Guided Diffusion Models<br>
Authors: Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela<br>
&nbsp; Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen<br>
Categories: cs.CV cs.GR cs.LG<br>
Comments: 20 pages, 18 figures<br>
\\ ( <a href="https://arxiv.org/abs/2112.10741" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2112.10741&amp;source=gmail&amp;ust=1646963617866000&amp;usg=AOvVaw30GfGRlruUReorcGlceETs" rel="noreferrer" target="_blank">https://arxiv.org/abs/2112.<wbr>10741</a> ,&nbsp; 22942kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2201.04756<br>
replaced with revised version Tue, 8 Mar 2022 18:20:39 GMT&nbsp; &nbsp;(1293kb)<br>
<br>
Title: Roadside Lidar Vehicle Detection and Tracking Using Range And Intensity<br>
&nbsp; Background Subtraction<br>
Authors: Tianya Zhang and Peter J. Jin<br>
Categories: cs.CV eess.SP<br>
\\ ( <a href="https://arxiv.org/abs/2201.04756" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2201.04756&amp;source=gmail&amp;ust=1646963617866000&amp;usg=AOvVaw1SCviD9SDN8t4f_VA-zUFk" rel="noreferrer" target="_blank">https://arxiv.org/abs/2201.<wbr>04756</a> ,&nbsp; 1293kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2201.04788<br>
replaced with revised version Tue, 8 Mar 2022 12:23:23 GMT&nbsp; &nbsp;(3870kb,D)<br>
<br>
Title: AI Singapore Trusted Media Challenge Dataset<br>
Authors: Weiling Chen, Benjamin Chua, Stefan Winkler, See Kiong Ng<br>
Categories: cs.CV<br>
\\ ( <a href="https://arxiv.org/abs/2201.04788" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2201.04788&amp;source=gmail&amp;ust=1646963617866000&amp;usg=AOvVaw0h3UR-rizZOhXJfZTqJ4oO" rel="noreferrer" target="_blank">https://arxiv.org/abs/2201.<wbr>04788</a> ,&nbsp; 3870kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2201.04898<br>
replaced with revised version Tue, 8 Mar 2022 11:41:27 GMT&nbsp; &nbsp;(21998kb,D)<br>
<br>
Title: Flexible Style Image Super-Resolution using Conditional Objective<br>
Authors: Seung Ho Park, Young Su Moon and Nam Ik Cho<br>
Categories: cs.CV eess.IV<br>
Comments: Will be presented in IEEE ACCESS. Code and trained models will be<br>
&nbsp; available at <a href="https://github.com/seungho-snu/FxSR" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/seungho-snu/FxSR&amp;source=gmail&amp;ust=1646963617866000&amp;usg=AOvVaw2O5XqPaq4HIcKMj2F5zJZX" rel="noreferrer" target="_blank">https://github.com/seungho-<wbr>snu/FxSR</a><br>
\\ ( <a href="https://arxiv.org/abs/2201.04898" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2201.04898&amp;source=gmail&amp;ust=1646963617866000&amp;usg=AOvVaw0dw7LXDeKUXb94BizeEbfI" rel="noreferrer" target="_blank">https://arxiv.org/abs/2201.<wbr>04898</a> ,&nbsp; 21998kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2201.08845<br>
replaced with revised version Tue, 8 Mar 2022 09:19:35 GMT&nbsp; &nbsp;(32145kb,D)<br>
<br>
Title: Point-NeRF: Point-based Neural Radiance Fields<br>
Authors: Qiangeng Xu and Zexiang Xu and Julien Philip and Sai Bi and Zhixin Shu<br>
&nbsp; and Kalyan Sunkavalli and Ulrich Neumann<br>
Categories: cs.CV<br>
Comments: Accepted to CVPR 2022<br>
\\ ( <a href="https://arxiv.org/abs/2201.08845" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2201.08845&amp;source=gmail&amp;ust=1646963617867000&amp;usg=AOvVaw2LFbAlnyoBZlg7y9zjLbHb" rel="noreferrer" target="_blank">https://arxiv.org/abs/2201.<wbr>08845</a> ,&nbsp; 32145kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2201.08959<br>
replaced with revised version Tue, 8 Mar 2022 06:05:33 GMT&nbsp; &nbsp;(3082kb,D)<br>
<br>
Title: Few-shot Object Counting with Similarity-Aware Feature Enhancement<br>
Authors: Zhiyuan You, Yujun Shen, Kai Yang, Wenhan Luo, Xin Lu, Lei Cui, Xinyi<br>
&nbsp; Le<br>
Categories: cs.CV<br>
\\ ( <a href="https://arxiv.org/abs/2201.08959" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2201.08959&amp;source=gmail&amp;ust=1646963617867000&amp;usg=AOvVaw05_s8v4p0ohQ2bNXjG39nt" rel="noreferrer" target="_blank">https://arxiv.org/abs/2201.<wbr>08959</a> ,&nbsp; 3082kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2201.10665<br>
replaced with revised version Tue, 8 Mar 2022 00:06:51 GMT&nbsp; &nbsp;(6681kb,D)<br>
<br>
Title: Writer Recognition Using Off-line Handwritten Single Block Characters<br>
Authors: Adrian Leo Hagstr\"om, Rustam Stanikzai, Josef Bigun, Fernando<br>
&nbsp; Alonso-Fernandez<br>
Categories: cs.CV<br>
Comments: Accepted for publication at IEEE International Workshop on Biometrics<br>
&nbsp; and Forensics IWBF 2022<br>
\\ ( <a href="https://arxiv.org/abs/2201.10665" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2201.10665&amp;source=gmail&amp;ust=1646963617867000&amp;usg=AOvVaw3h2bWMqcJk9bWSB5Bcq_H_" rel="noreferrer" target="_blank">https://arxiv.org/abs/2201.<wbr>10665</a> ,&nbsp; 6681kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2201.10865<br>
replaced with revised version Tue, 8 Mar 2022 11:31:29 GMT&nbsp; &nbsp;(5178kb)<br>
<br>
Title: On the Issues of TrueDepth Sensor Data for Computer Vision Tasks Across<br>
&nbsp; Different iPad Generations<br>
Authors: Steffen Urban, Thomas Lindemeier, David Dobbelstein, Matthias Haenel<br>
Categories: cs.CV cs.RO<br>
Comments: 17 pages<br>
\\ ( <a href="https://arxiv.org/abs/2201.10865" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2201.10865&amp;source=gmail&amp;ust=1646963617867000&amp;usg=AOvVaw2nqyBMQEv2hjOOTzWJEFNE" rel="noreferrer" target="_blank">https://arxiv.org/abs/2201.<wbr>10865</a> ,&nbsp; 5178kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2201.12769<br>
replaced with revised version Tue, 8 Mar 2022 11:16:29 GMT&nbsp; &nbsp;(5134kb,D)<br>
<br>
Title: MVP-Net: Multiple View Pointwise Semantic Segmentation of Large-Scale<br>
&nbsp; Point Clouds<br>
Authors: Chuanyu Luo, Xiaohan Li, Nuo Cheng, Han Li, Shengguang Lei, Pu Li<br>
Categories: cs.CV cs.AI<br>
\\ ( <a href="https://arxiv.org/abs/2201.12769" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2201.12769&amp;source=gmail&amp;ust=1646963617867000&amp;usg=AOvVaw3XiRmsugv20kBKo7TjvYpk" rel="noreferrer" target="_blank">https://arxiv.org/abs/2201.<wbr>12769</a> ,&nbsp; 5134kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2202.02149<br>
replaced with revised version Tue, 8 Mar 2022 05:09:55 GMT&nbsp; &nbsp;(8061kb,D)<br>
<br>
Title: Edge-Selective Feature Weaving for Point Cloud Matching<br>
Authors: Rintaro Yanagi, Atsushi Hashimoto, Shusaku Sone, Naoya Chiba, Jiaxin<br>
&nbsp; Ma, and Yoshitaka Ushiku<br>
Categories: cs.CV<br>
\\ ( <a href="https://arxiv.org/abs/2202.02149" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2202.02149&amp;source=gmail&amp;ust=1646963617867000&amp;usg=AOvVaw1TJtCqLDP4ckIAgh7--2pj" rel="noreferrer" target="_blank">https://arxiv.org/abs/2202.<wbr>02149</a> ,&nbsp; 8061kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2202.09741<br>
replaced with revised version Tue, 8 Mar 2022 14:02:58 GMT&nbsp; &nbsp;(4040kb,D)<br>
<br>
Title: Visual Attention Network<br>
Authors: Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng and Shi-Min<br>
&nbsp; Hu<br>
Categories: cs.CV<br>
Comments: Code is available at <a href="https://github.com/Visual-Attention-Network" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/Visual-Attention-Network&amp;source=gmail&amp;ust=1646963617867000&amp;usg=AOvVaw1VIGkIVxivY3-5-zT4kOki" rel="noreferrer" target="_blank">https://github.com/Visual-<wbr>Attention-Network</a><br>
\\ ( <a href="https://arxiv.org/abs/2202.09741" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2202.09741&amp;source=gmail&amp;ust=1646963617867000&amp;usg=AOvVaw3bAEA13qZjDm0KxNsbJuv3" rel="noreferrer" target="_blank">https://arxiv.org/abs/2202.<wbr>09741</a> ,&nbsp; 4040kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2202.12403<br>
replaced with revised version Tue, 8 Mar 2022 15:18:26 GMT&nbsp; &nbsp;(22322kb,D)<br>
<br>
Title: Learning Transferable Reward for Query Object Localization with Policy<br>
&nbsp; Adaptation<br>
Authors: Tingfeng Li, Shaobo Han, Martin Renqiang Min, Dimitris N. Metaxas<br>
Categories: cs.CV cs.LG<br>
Comments: ICLR 2022<br>
\\ ( <a href="https://arxiv.org/abs/2202.12403" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2202.12403&amp;source=gmail&amp;ust=1646963617867000&amp;usg=AOvVaw0fvyK_ONTIOip1O8EyWGXG" rel="noreferrer" target="_blank">https://arxiv.org/abs/2202.<wbr>12403</a> ,&nbsp; 22322kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2202.12498<br>
replaced with revised version Tue, 8 Mar 2022 01:17:59 GMT&nbsp; &nbsp;(19785kb,D)<br>
<br>
Title: Diffeomorphic Image Registration with Neural Velocity Field<br>
Authors: Kun Han, Shanlin Sun, Hao Tang, Deying Kong, Xiangyi Yan, Xiaohui Xie<br>
Categories: cs.CV<br>
\\ ( <a href="https://arxiv.org/abs/2202.12498" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2202.12498&amp;source=gmail&amp;ust=1646963617867000&amp;usg=AOvVaw3L_BFTjCiy_B_eqiZN-NCa" rel="noreferrer" target="_blank">https://arxiv.org/abs/2202.<wbr>12498</a> ,&nbsp; 19785kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2202.12986<br>
replaced with revised version Tue, 8 Mar 2022 14:33:54 GMT&nbsp; &nbsp;(22kb)<br>
<br>
Title: Extracting Effective Subnetworks with Gumebel-Softmax<br>
Authors: Robin Dupont, Mohammed Amine Alaoui, Hichem Sahbi, Alice Lebois<br>
Categories: cs.CV<br>
Comments: 5 pages, 1 table<br>
\\ ( <a href="https://arxiv.org/abs/2202.12986" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2202.12986&amp;source=gmail&amp;ust=1646963617867000&amp;usg=AOvVaw2KBid8ydyXR7cXncZN3XGD" rel="noreferrer" target="_blank">https://arxiv.org/abs/2202.<wbr>12986</a> ,&nbsp; 22kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.01923<br>
replaced with revised version Tue, 8 Mar 2022 12:42:52 GMT&nbsp; &nbsp;(1363kb,D)<br>
<br>
Title: Recovering 3D Human Mesh from Monocular Images: A Survey<br>
Authors: Yating Tian, Hongwen Zhang, Yebin Liu, Limin Wang<br>
Categories: cs.CV cs.GR<br>
Comments: Survey paper on monocular 3D human mesh recovery, Project page:<br>
&nbsp; <a href="https://github.com/tinatiansjz/hmr-survey" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/tinatiansjz/hmr-survey&amp;source=gmail&amp;ust=1646963617868000&amp;usg=AOvVaw3LBxElpDwqMdnUIgzoNo1o" rel="noreferrer" target="_blank">https://github.com/<wbr>tinatiansjz/hmr-survey</a><br>
\\ ( <a href="https://arxiv.org/abs/2203.01923" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.01923&amp;source=gmail&amp;ust=1646963617868000&amp;usg=AOvVaw2PzF_tM5Ro6KKw1zNrYOiC" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>01923</a> ,&nbsp; 1363kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.02689<br>
replaced with revised version Tue, 8 Mar 2022 12:04:03 GMT&nbsp; &nbsp;(720kb,D)<br>
<br>
Title: Federated and Generalized Person Re-identification through Domain and<br>
&nbsp; Feature Hallucinating<br>
Authors: Fengxiang Yang, Zhun Zhong, Zhiming Luo, Shaozi Li, Nicu Sebe<br>
Categories: cs.CV<br>
Comments: Under Review<br>
\\ ( <a href="https://arxiv.org/abs/2203.02689" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.02689&amp;source=gmail&amp;ust=1646963617868000&amp;usg=AOvVaw2oxP6UWfHg-jXm5Y95-MnU" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>02689</a> ,&nbsp; 720kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.02925<br>
replaced with revised version Tue, 8 Mar 2022 03:06:25 GMT&nbsp; &nbsp;(1080kb,D)<br>
<br>
Title: Weakly Supervised Temporal Action Localization via Representative<br>
&nbsp; Snippet Knowledge Propagation<br>
Authors: Linjiang Huang, Liang Wang, Hongsheng Li<br>
Categories: cs.CV<br>
Comments: Accepted by CVPR 2022. Code is available at<br>
&nbsp; <a href="https://github.com/LeonHLJ/RSKP" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://github.com/LeonHLJ/RSKP&amp;source=gmail&amp;ust=1646963617868000&amp;usg=AOvVaw3Ns6YoALCzzgXjgkcLv1Wr" rel="noreferrer" target="_blank">https://github.com/LeonHLJ/<wbr>RSKP</a><br>
\\ ( <a href="https://arxiv.org/abs/2203.02925" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.02925&amp;source=gmail&amp;ust=1646963617868000&amp;usg=AOvVaw0Y_9NokoHpNBZ-uH_GkK33" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>02925</a> ,&nbsp; 1080kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03373<br>
replaced with revised version Tue, 8 Mar 2022 14:29:07 GMT&nbsp; &nbsp;(33827kb,D)<br>
<br>
Title: Adversarial Texture for Fooling Person Detectors in the Physical World<br>
Authors: Zhanhao Hu, Siyuan Huang, Xiaopei Zhu, Xiaolin Hu, Fuchun Sun, Bo<br>
&nbsp; Zhang<br>
Categories: cs.CV<br>
Comments: Accepted by CVPR 2022<br>
\\ ( <a href="https://arxiv.org/abs/2203.03373" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03373&amp;source=gmail&amp;ust=1646963617868000&amp;usg=AOvVaw3HDAZiIJip9VhIeSgSje8s" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03373</a> ,&nbsp; 33827kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2010.03172<br>
replaced with revised version Tue, 8 Mar 2022 05:32:41 GMT&nbsp; &nbsp;(16945kb,D)<br>
<br>
Title: Improving Sequential Latent Variable Models with Autoregressive Flows<br>
Authors: Joseph Marino, Lei Chen, Jiawei He, Stephan Mandt<br>
Categories: cs.LG<br>
Comments: Published at Machine Learning Journal<br>
Journal-ref: Mach Learn (2021)<br>
DOI: 10.1007/s10994-021-06092-6<br>
\\ ( <a href="https://arxiv.org/abs/2010.03172" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2010.03172&amp;source=gmail&amp;ust=1646963617868000&amp;usg=AOvVaw1qsELal2AmG0hlSzrwOGfc" rel="noreferrer" target="_blank">https://arxiv.org/abs/2010.<wbr>03172</a> ,&nbsp; 16945kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2010.07393<br>
replaced with revised version Tue, 8 Mar 2022 13:53:52 GMT&nbsp; &nbsp;(1021kb,D)<br>
<br>
Title: FAR: A General Framework for Attributional Robustness<br>
Authors: Adam Ivankay, Ivan Girardi, Chiara Marchiori and Pascal Frossard<br>
Categories: cs.LG<br>
Journal-ref: Ivankay, Adam, Ivan Girardi, Chiara Marchiori, and Pascal<br>
&nbsp; Frossard. "FAR: A General Framework for Attributional Robustness." (2021).<br>
&nbsp; BMVC 2021<br>
\\ ( <a href="https://arxiv.org/abs/2010.07393" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2010.07393&amp;source=gmail&amp;ust=1646963617868000&amp;usg=AOvVaw2jhRmtGKj_rza-1DnRsr8_" rel="noreferrer" target="_blank">https://arxiv.org/abs/2010.<wbr>07393</a> ,&nbsp; 1021kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2102.05375<br>
replaced with revised version Tue, 8 Mar 2022 09:00:56 GMT&nbsp; &nbsp;(3267kb,D)<br>
<br>
Title: Strength of Minibatch Noise in SGD<br>
Authors: Liu Ziyin, Kangqiao Liu, Takashi Mori, Masahito Ueda<br>
Categories: cs.LG stat.ML<br>
Comments: ICLR 2022 spotlight<br>
\\ ( <a href="https://arxiv.org/abs/2102.05375" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2102.05375&amp;source=gmail&amp;ust=1646963617868000&amp;usg=AOvVaw3WSBsfWTZzH3NBSY32qdO6" rel="noreferrer" target="_blank">https://arxiv.org/abs/2102.<wbr>05375</a> ,&nbsp; 3267kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2103.05134<br>
replaced with revised version Mon, 7 Mar 2022 23:15:09 GMT&nbsp; &nbsp;(627kb,D)<br>
<br>
Title: Constrained Learning with Non-Convex Losses<br>
Authors: Luiz F. O. Chamon and Santiago Paternain and Miguel Calvo-Fullana and<br>
&nbsp; Alejandro Ribeiro<br>
Categories: cs.LG math.ST stat.ML stat.TH<br>
\\ ( <a href="https://arxiv.org/abs/2103.05134" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2103.05134&amp;source=gmail&amp;ust=1646963617868000&amp;usg=AOvVaw2zWJtB4cE2-xobHAdJpEXu" rel="noreferrer" target="_blank">https://arxiv.org/abs/2103.<wbr>05134</a> ,&nbsp; 627kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2104.01714<br>
replaced with revised version Tue, 8 Mar 2022 12:03:53 GMT&nbsp; &nbsp;(1110kb)<br>
<br>
Title: Accounting for uncertainty of non-linear regression models by divisive<br>
&nbsp; data resorting<br>
Authors: Andrew Polar, Michael Poluektov<br>
Categories: cs.LG<br>
\\ ( <a href="https://arxiv.org/abs/2104.01714" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2104.01714&amp;source=gmail&amp;ust=1646963617868000&amp;usg=AOvVaw3bkMtUt0vPoib7daixRP_c" rel="noreferrer" target="_blank">https://arxiv.org/abs/2104.<wbr>01714</a> ,&nbsp; 1110kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2104.02395<br>
replaced with revised version Tue, 8 Mar 2022 04:44:41 GMT&nbsp; &nbsp;(188kb)<br>
<br>
Title: Ensemble deep learning: A review<br>
Authors: M.A. Ganaie and Minghui Hu and A.K. Malik and M. Tanveer and P.N.<br>
&nbsp; Suganthan<br>
Categories: cs.LG cs.AI cs.CV<br>
\\ ( <a href="https://arxiv.org/abs/2104.02395" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2104.02395&amp;source=gmail&amp;ust=1646963617868000&amp;usg=AOvVaw0rgCJarFYxD60bvUW5e5m4" rel="noreferrer" target="_blank">https://arxiv.org/abs/2104.<wbr>02395</a> ,&nbsp; 188kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2104.14332<br>
replaced with revised version Tue, 8 Mar 2022 06:38:05 GMT&nbsp; &nbsp;(2664kb,D)<br>
<br>
Title: Hypernetwork Dismantling via Deep Reinforcement Learning<br>
Authors: Dengcheng Yan, Wenxin Xie, Yiwen Zhang, Qiang He, Yun Yang<br>
Categories: cs.LG cs.SY eess.SY<br>
\\ ( <a href="https://arxiv.org/abs/2104.14332" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2104.14332&amp;source=gmail&amp;ust=1646963617868000&amp;usg=AOvVaw1K8eoug3JUzAdC7hwQfqbe" rel="noreferrer" target="_blank">https://arxiv.org/abs/2104.<wbr>14332</a> ,&nbsp; 2664kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2106.01216<br>
replaced with revised version Tue, 8 Mar 2022 11:52:34 GMT&nbsp; &nbsp;(1545kb,D)<br>
<br>
Title: Evidential Turing Processes<br>
Authors: Melih Kandemir, Abdullah Akg\"ul, Manuel Haussmann, Gozde Unal<br>
Categories: cs.LG<br>
Comments: accepted at ICLR2022; camera ready version<br>
\\ ( <a href="https://arxiv.org/abs/2106.01216" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2106.01216&amp;source=gmail&amp;ust=1646963617868000&amp;usg=AOvVaw1QuWh6CQV9a1rdUvlzoU6X" rel="noreferrer" target="_blank">https://arxiv.org/abs/2106.<wbr>01216</a> ,&nbsp; 1545kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2106.04590<br>
replaced with revised version Tue, 8 Mar 2022 18:15:04 GMT&nbsp; &nbsp;(2022kb,D)<br>
<br>
Title: PEARL: Data Synthesis via Private Embeddings and Adversarial<br>
&nbsp; Reconstruction Learning<br>
Authors: Seng Pei Liew, Tsubasa Takahashi, Michihiko Ueno<br>
Categories: cs.LG cs.CR<br>
Comments: 22 pages, 10 figures, accepted to ICLR 2022<br>
Journal-ref: The Tenth International Conference on Learning Representations<br>
&nbsp; (ICLR 2022)<br>
\\ ( <a href="https://arxiv.org/abs/2106.04590" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2106.04590&amp;source=gmail&amp;ust=1646963617868000&amp;usg=AOvVaw1cXG8obEq184zu1ZcJJbfJ" rel="noreferrer" target="_blank">https://arxiv.org/abs/2106.<wbr>04590</a> ,&nbsp; 2022kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2106.04823<br>
replaced with revised version Tue, 8 Mar 2022 00:22:37 GMT&nbsp; &nbsp;(1769kb,D)<br>
<br>
Title: Taxonomy of Machine Learning Safety: A Survey and Primer<br>
Authors: Sina Mohseni and Haotao Wang and Zhiding Yu and Chaowei Xiao and<br>
&nbsp; Zhangyang Wang and Jay Yadawa<br>
Categories: cs.LG cs.AI<br>
\\ ( <a href="https://arxiv.org/abs/2106.04823" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2106.04823&amp;source=gmail&amp;ust=1646963617869000&amp;usg=AOvVaw2DAOhtcUcuGHlYbiV3FIZj" rel="noreferrer" target="_blank">https://arxiv.org/abs/2106.<wbr>04823</a> ,&nbsp; 1769kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2106.12248<br>
replaced with revised version Tue, 8 Mar 2022 14:25:30 GMT&nbsp; &nbsp;(2187kb,D)<br>
<br>
Title: ADAVI: Automatic Dual Amortized Variational Inference Applied To<br>
&nbsp; Pyramidal Bayesian Models<br>
Authors: Louis Rouillard (PARIETAL, CEA), Demian Wassermann (PARIETAL, CEA)<br>
Categories: cs.LG cs.AI q-bio.NC stat.ML<br>
Journal-ref: ICLR 2022, Apr 2022, Online, France<br>
\\ ( <a href="https://arxiv.org/abs/2106.12248" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2106.12248&amp;source=gmail&amp;ust=1646963617869000&amp;usg=AOvVaw2Tp8P4w_39gE78I_LqvfdK" rel="noreferrer" target="_blank">https://arxiv.org/abs/2106.<wbr>12248</a> ,&nbsp; 2187kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2106.13264<br>
replaced with revised version Tue, 8 Mar 2022 17:13:40 GMT&nbsp; &nbsp;(610kb,D)<br>
<br>
Title: You are AllSet: A Multiset Function Framework for Hypergraph Neural<br>
&nbsp; Networks<br>
Authors: Eli Chien, Chao Pan, Jianhao Peng, Olgica Milenkovic<br>
Categories: cs.LG cs.AI<br>
Comments: ICLR 2022<br>
\\ ( <a href="https://arxiv.org/abs/2106.13264" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2106.13264&amp;source=gmail&amp;ust=1646963617869000&amp;usg=AOvVaw2Wyz0uwhouVUrcGSeUP7cp" rel="noreferrer" target="_blank">https://arxiv.org/abs/2106.<wbr>13264</a> ,&nbsp; 610kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2108.02741<br>
replaced with revised version Tue, 8 Mar 2022 00:30:52 GMT&nbsp; &nbsp;(5354kb,D)<br>
<br>
Title: GIFAIR-FL: A Framework for Group and Individual Fairness in Federated<br>
&nbsp; Learning<br>
Authors: Xubo Yue, Maher Nouiehed, Raed Al Kontar<br>
Categories: cs.LG cs.DC<br>
\\ ( <a href="https://arxiv.org/abs/2108.02741" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2108.02741&amp;source=gmail&amp;ust=1646963617869000&amp;usg=AOvVaw20IvSLPRBnD2Ink-WNmF7u" rel="noreferrer" target="_blank">https://arxiv.org/abs/2108.<wbr>02741</a> ,&nbsp; 5354kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2108.05828<br>
replaced with revised version Mon, 7 Mar 2022 19:44:02 GMT&nbsp; &nbsp;(526kb,D)<br>
<br>
Title: A general class of surrogate functions for stable and efficient<br>
&nbsp; reinforcement learning<br>
Authors: Sharan Vaswani, Olivier Bachem, Simone Totaro, Robert Mueller, Shivam<br>
&nbsp; Garg, Matthieu Geist, Marlos C. Machado, Pablo Samuel Castro, Nicolas Le Roux<br>
Categories: cs.LG cs.AI stat.ML<br>
Comments: AISTATS 2022<br>
\\ ( <a href="https://arxiv.org/abs/2108.05828" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2108.05828&amp;source=gmail&amp;ust=1646963617869000&amp;usg=AOvVaw204YQrMaAlDNnnzt4CjcaZ" rel="noreferrer" target="_blank">https://arxiv.org/abs/2108.<wbr>05828</a> ,&nbsp; 526kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2109.11200<br>
replaced with revised version Tue, 8 Mar 2022 10:47:15 GMT&nbsp; &nbsp;(721kb)<br>
<br>
Title: Secure PAC Bayesian Regression via Real Shamir Secret Sharing<br>
Authors: Jaron Skovsted Gundersen, Bulut Kuskonmaz, Rafael Wisniewski<br>
Categories: cs.LG cs.CR<br>
\\ ( <a href="https://arxiv.org/abs/2109.11200" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2109.11200&amp;source=gmail&amp;ust=1646963617869000&amp;usg=AOvVaw1OzkW7EsZ-bbWWA9cFhhza" rel="noreferrer" target="_blank">https://arxiv.org/abs/2109.<wbr>11200</a> ,&nbsp; 721kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2109.14960<br>
replaced with revised version Tue, 8 Mar 2022 09:04:13 GMT&nbsp; &nbsp;(252kb,D)<br>
<br>
Title: Prune Your Model Before Distill It<br>
Authors: Jinhyuk Park, Albert No<br>
Categories: cs.LG<br>
\\ ( <a href="https://arxiv.org/abs/2109.14960" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2109.14960&amp;source=gmail&amp;ust=1646963617869000&amp;usg=AOvVaw02OTX02kkxq9n-cHTld3at" rel="noreferrer" target="_blank">https://arxiv.org/abs/2109.<wbr>14960</a> ,&nbsp; 252kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2110.01515<br>
replaced with revised version Tue, 8 Mar 2022 11:45:53 GMT&nbsp; &nbsp;(9041kb,D)<br>
<br>
Title: A Review of the Gumbel-max Trick and its Extensions for Discrete<br>
&nbsp; Stochasticity in Machine Learning<br>
Authors: Iris A. M. Huijben, Wouter Kool, Max B. Paulus, Ruud J. G. van Sloun<br>
Categories: cs.LG stat.ML<br>
Comments: Accepted as a survey article in IEEE TPAMI<br>
\\ ( <a href="https://arxiv.org/abs/2110.01515" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2110.01515&amp;source=gmail&amp;ust=1646963617869000&amp;usg=AOvVaw0m13APunyAKIXk3dprbpsP" rel="noreferrer" target="_blank">https://arxiv.org/abs/2110.<wbr>01515</a> ,&nbsp; 9041kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2110.02792<br>
replaced with revised version Tue, 8 Mar 2022 17:20:16 GMT&nbsp; &nbsp;(558kb,D)<br>
<br>
Title: Hierarchical Potential-based Reward Shaping from Task Specifications<br>
Authors: Luigi Berducci, Edgar A. Aguilar, Dejan Ni\v{c}kovi\'c, Radu Grosu<br>
Categories: cs.LG cs.AI<br>
Comments: 7 pages main, 3 pages appendix - improved task specification language<br>
&nbsp; and experiments<br>
\\ ( <a href="https://arxiv.org/abs/2110.02792" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2110.02792&amp;source=gmail&amp;ust=1646963617869000&amp;usg=AOvVaw3-EZa1vWdNhDjL5_sLlafm" rel="noreferrer" target="_blank">https://arxiv.org/abs/2110.<wbr>02792</a> ,&nbsp; 558kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2110.05794<br>
replaced with revised version Mon, 7 Mar 2022 19:17:04 GMT&nbsp; &nbsp;(9479kb,D)<br>
<br>
Title: Information Theoretic Structured Generative Modeling<br>
Authors: Bo Hu, Shujian Yu, Jose C. Principe<br>
Categories: cs.LG cs.IT math.IT stat.ML<br>
\\ ( <a href="https://arxiv.org/abs/2110.05794" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2110.05794&amp;source=gmail&amp;ust=1646963617869000&amp;usg=AOvVaw0SlyUhCwc-BWd0f-QVd43Z" rel="noreferrer" target="_blank">https://arxiv.org/abs/2110.<wbr>05794</a> ,&nbsp; 9479kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2110.10380<br>
replaced with revised version Tue, 8 Mar 2022 09:24:43 GMT&nbsp; &nbsp;(19837kb)<br>
<br>
Title: Learning to Remember Patterns: Pattern Matching Memory Networks for<br>
&nbsp; Traffic Forecasting<br>
Authors: Hyunwook Lee, Seungmin Jin, Hyeshin Chu, Hongkyu Lim, Sungahn Ko<br>
Categories: cs.LG cs.NE<br>
Comments: 15 pages, Accepted as poster to ICLR 2022<br>
Journal-ref: International Conference on Learning Representations (ICLR 2022)<br>
\\ ( <a href="https://arxiv.org/abs/2110.10380" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2110.10380&amp;source=gmail&amp;ust=1646963617870000&amp;usg=AOvVaw0bdTPfpJ8CBeLqT7yYKUbK" rel="noreferrer" target="_blank">https://arxiv.org/abs/2110.<wbr>10380</a> ,&nbsp; 19837kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2110.14163<br>
replaced with revised version Tue, 8 Mar 2022 06:27:20 GMT&nbsp; &nbsp;(1396kb,D)<br>
<br>
Title: Does the Data Induce Capacity Control in Deep Learning?<br>
Authors: Rubing Yang, Jialin Mao, Pratik Chaudhari<br>
Categories: cs.LG stat.ML<br>
\\ ( <a href="https://arxiv.org/abs/2110.14163" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2110.14163&amp;source=gmail&amp;ust=1646963617870000&amp;usg=AOvVaw09D0d4F_HuinnYe0KorKUX" rel="noreferrer" target="_blank">https://arxiv.org/abs/2110.<wbr>14163</a> ,&nbsp; 1396kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2111.10144<br>
replaced with revised version Tue, 8 Mar 2022 14:03:15 GMT&nbsp; &nbsp;(2319kb,D)<br>
<br>
Title: Positional Encoder Graph Neural Networks for Geographic Data<br>
Authors: Konstantin Klemmer, Nathan Safir, Daniel B Neill<br>
Categories: cs.LG cs.CV<br>
\\ ( <a href="https://arxiv.org/abs/2111.10144" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2111.10144&amp;source=gmail&amp;ust=1646963617870000&amp;usg=AOvVaw2WVI2u9NAEHu_cU1_wIAkE" rel="noreferrer" target="_blank">https://arxiv.org/abs/2111.<wbr>10144</a> ,&nbsp; 2319kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2111.14833<br>
replaced with revised version Tue, 8 Mar 2022 01:24:01 GMT&nbsp; &nbsp;(50kb,D)<br>
<br>
Title: Adversarial Attacks in Cooperative AI<br>
Authors: Ted Fujimoto and Arthur Paul Pedersen<br>
Categories: cs.LG cs.AI cs.MA<br>
\\ ( <a href="https://arxiv.org/abs/2111.14833" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2111.14833&amp;source=gmail&amp;ust=1646963617870000&amp;usg=AOvVaw1XNiLvV-xnXDYUV_3f3glN" rel="noreferrer" target="_blank">https://arxiv.org/abs/2111.<wbr>14833</a> ,&nbsp; 50kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2201.06126<br>
replaced with revised version Mon, 7 Mar 2022 21:20:37 GMT&nbsp; &nbsp;(3373kb,D)<br>
<br>
Title: Solving Inventory Management Problems with Inventory-dynamics-informed<br>
&nbsp; Neural Networks<br>
Authors: Lucas B\"ottcher and Thomas Asikis and Ioannis Fragkos<br>
Categories: cs.LG math.OC<br>
Comments: 35 pages, 5 figures<br>
\\ ( <a href="https://arxiv.org/abs/2201.06126" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2201.06126&amp;source=gmail&amp;ust=1646963617870000&amp;usg=AOvVaw2YJhA6CU9VrETjqtvHtLr4" rel="noreferrer" target="_blank">https://arxiv.org/abs/2201.<wbr>06126</a> ,&nbsp; 3373kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2201.06532<br>
replaced with revised version Tue, 8 Mar 2022 18:26:17 GMT&nbsp; &nbsp;(24kb)<br>
<br>
Title: A New Look at Dynamic Regret for Non-Stationary Stochastic Bandits<br>
Authors: Yasin Abbasi-Yadkori, Andras Gyorgy, Nevena Lazic<br>
Categories: cs.LG stat.ML<br>
\\ ( <a href="https://arxiv.org/abs/2201.06532" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2201.06532&amp;source=gmail&amp;ust=1646963617870000&amp;usg=AOvVaw1-9BMG_m4UAxi-jBZmJouR" rel="noreferrer" target="_blank">https://arxiv.org/abs/2201.<wbr>06532</a> ,&nbsp; 24kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2201.07207<br>
replaced with revised version Tue, 8 Mar 2022 06:47:17 GMT&nbsp; &nbsp;(2325kb,D)<br>
<br>
Title: Language Models as Zero-Shot Planners: Extracting Actionable Knowledge<br>
&nbsp; for Embodied Agents<br>
Authors: Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch<br>
Categories: cs.LG cs.AI cs.CL cs.CV cs.RO<br>
Comments: Project website at <a href="https://huangwl18.github.io/language-planner" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://huangwl18.github.io/language-planner&amp;source=gmail&amp;ust=1646963617870000&amp;usg=AOvVaw0ZcceTMB3kacaGP30HmLkC" rel="noreferrer" target="_blank">https://huangwl18.github.io/<wbr>language-planner</a><br>
\\ ( <a href="https://arxiv.org/abs/2201.07207" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2201.07207&amp;source=gmail&amp;ust=1646963617870000&amp;usg=AOvVaw09xOoozgSLfU8StJWGmJbv" rel="noreferrer" target="_blank">https://arxiv.org/abs/2201.<wbr>07207</a> ,&nbsp; 2325kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2201.10460<br>
replaced with revised version Mon, 7 Mar 2022 21:37:18 GMT&nbsp; &nbsp;(125kb)<br>
<br>
Title: Conditional entropy minimization principle for learning domain invariant<br>
&nbsp; representation features<br>
Authors: Thuan Nguyen, Boyang Lyu, Prakash Ishwar, Matthias Scheutz, Shuchin<br>
&nbsp; Aeron<br>
Categories: cs.LG cs.AI<br>
Comments: 7 pages<br>
\\ ( <a href="https://arxiv.org/abs/2201.10460" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2201.10460&amp;source=gmail&amp;ust=1646963617870000&amp;usg=AOvVaw0ECi4eZ4BeprJaHfUkPjNN" rel="noreferrer" target="_blank">https://arxiv.org/abs/2201.<wbr>10460</a> ,&nbsp; 125kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2201.12414<br>
replaced with revised version Mon, 7 Mar 2022 21:10:26 GMT&nbsp; &nbsp;(2400kb,D)<br>
<br>
Title: Any Variational Autoencoder Can Do Arbitrary Conditioning<br>
Authors: Ryan R. Strauss, Junier B. Oliva<br>
Categories: cs.LG stat.ML<br>
\\ ( <a href="https://arxiv.org/abs/2201.12414" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2201.12414&amp;source=gmail&amp;ust=1646963617870000&amp;usg=AOvVaw224nd-jHlljAmizR-ZCKIk" rel="noreferrer" target="_blank">https://arxiv.org/abs/2201.<wbr>12414</a> ,&nbsp; 2400kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2202.01332<br>
replaced with revised version Tue, 8 Mar 2022 10:41:47 GMT&nbsp; &nbsp;(1709kb,D)<br>
<br>
Title: Training a Bidirectional GAN-based One-Class Classifier for Network<br>
&nbsp; Intrusion Detection<br>
Authors: Wen Xu, Julian Jang-Jaccard, Tong Liu, Fariza Sabrina<br>
Categories: cs.LG<br>
Comments: 16 pages, 8 figures<br>
\\ ( <a href="https://arxiv.org/abs/2202.01332" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2202.01332&amp;source=gmail&amp;ust=1646963617870000&amp;usg=AOvVaw0C7euEeYsvbL5ZTqR2FBpz" rel="noreferrer" target="_blank">https://arxiv.org/abs/2202.<wbr>01332</a> ,&nbsp; 1709kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2202.01694<br>
replaced with revised version Tue, 8 Mar 2022 02:33:54 GMT&nbsp; &nbsp;(2126kb,D)<br>
<br>
Title: Variational Nearest Neighbor Gaussian Processes<br>
Authors: Luhuan Wu, Geoff Pleiss, John Cunningham<br>
Categories: cs.LG stat.ML<br>
\\ ( <a href="https://arxiv.org/abs/2202.01694" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2202.01694&amp;source=gmail&amp;ust=1646963617870000&amp;usg=AOvVaw1oKpTGh8Zq02D0mVFsKj-6" rel="noreferrer" target="_blank">https://arxiv.org/abs/2202.<wbr>01694</a> ,&nbsp; 2126kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2202.04129<br>
replaced with revised version Tue, 8 Mar 2022 18:53:49 GMT&nbsp; &nbsp;(1436kb,D)<br>
<br>
Title: Independent Policy Gradient for Large-Scale Markov Potential Games:<br>
&nbsp; Sharper Rates, Function Approximation, and Game-Agnostic Convergence<br>
Authors: Dongsheng Ding and Chen-Yu Wei and Kaiqing Zhang and Mihailo R.<br>
&nbsp; Jovanovi\'c<br>
Categories: cs.LG cs.GT cs.MA math.OC<br>
Comments: 72 pages, 6 figures; Theorem 1 and Theorem 2 are improved by using<br>
&nbsp; $\tilde{\kappa}_\mu$, a minimax variant of distribution mismatch coefficient<br>
&nbsp; $\kappa_\mu$; the existence of an algorithm that enjoys finite-time<br>
&nbsp; best-iterate convergence in both competitive and cooperative Markov games is<br>
&nbsp; answered in Theorem 6<br>
\\ ( <a href="https://arxiv.org/abs/2202.04129" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2202.04129&amp;source=gmail&amp;ust=1646963617870000&amp;usg=AOvVaw0iP1hDNvsC95MLzr9N69xi" rel="noreferrer" target="_blank">https://arxiv.org/abs/2202.<wbr>04129</a> ,&nbsp; 1436kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2202.08549<br>
replaced with revised version Tue, 8 Mar 2022 07:49:14 GMT&nbsp; &nbsp;(45kb)<br>
<br>
Title: Oracle-Efficient Online Learning for Beyond Worst-Case Adversaries<br>
Authors: Nika Haghtalab, Yanjun Han, Abhishek Shetty, Kunhe Yang<br>
Categories: cs.LG cs.DS stat.ML<br>
Comments: Under submission<br>
\\ ( <a href="https://arxiv.org/abs/2202.08549" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2202.08549&amp;source=gmail&amp;ust=1646963617870000&amp;usg=AOvVaw1jgeE1hAj6aevVv4JoAFxG" rel="noreferrer" target="_blank">https://arxiv.org/abs/2202.<wbr>08549</a> ,&nbsp; 45kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2202.11684<br>
replaced with revised version Tue, 8 Mar 2022 12:30:03 GMT&nbsp; &nbsp;(310kb,D)<br>
<br>
Title: MuMiN: A Large-Scale Multilingual Multimodal Fact-Checked Misinformation<br>
&nbsp; Social Network Dataset<br>
Authors: Dan Saattrup Nielsen and Ryan McConville<br>
Categories: cs.LG cs.CL cs.CY cs.IR cs.SI<br>
Comments: 9+3 pages<br>
\\ ( <a href="https://arxiv.org/abs/2202.11684" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2202.11684&amp;source=gmail&amp;ust=1646963617870000&amp;usg=AOvVaw1e-A-1gBFHPLVUl5Snx1ld" rel="noreferrer" target="_blank">https://arxiv.org/abs/2202.<wbr>11684</a> ,&nbsp; 310kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.00829<br>
replaced with revised version Tue, 8 Mar 2022 02:51:54 GMT&nbsp; &nbsp;(10899kb,D)<br>
<br>
Title: Personalized Federated Learning With Structure<br>
Authors: Fengwen Chen, Guodong Long, Zonghan Wu, Tianyi Zhou and Jing Jiang<br>
Categories: cs.LG<br>
\\ ( <a href="https://arxiv.org/abs/2203.00829" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.00829&amp;source=gmail&amp;ust=1646963617870000&amp;usg=AOvVaw0vhoxyCygV2MI0OqTFdoo0" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>00829</a> ,&nbsp; 10899kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.01302<br>
replaced with revised version Tue, 8 Mar 2022 17:54:01 GMT&nbsp; &nbsp;(24827kb,D)<br>
<br>
Title: Evolving Curricula with Regret-Based Environment Design<br>
Authors: Jack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan,<br>
&nbsp; Jakob Foerster, Edward Grefenstette, Tim Rockt\"aschel<br>
Categories: cs.LG<br>
Comments: First two authors contributed equally<br>
\\ ( <a href="https://arxiv.org/abs/2203.01302" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.01302&amp;source=gmail&amp;ust=1646963617871000&amp;usg=AOvVaw0j6azHkTfdHhbO0CcuagPV" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>01302</a> ,&nbsp; 24827kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.02471<br>
replaced with revised version Mon, 7 Mar 2022 23:56:44 GMT&nbsp; &nbsp;(207kb,D)<br>
<br>
Title: Graph clustering with Boltzmann machines<br>
Authors: Pierre Miasnikof, Mohammad Bagherbeik, Ali Sheikholeslami<br>
Categories: cs.LG cs.DM math.OC stat.CO stat.ML<br>
\\ ( <a href="https://arxiv.org/abs/2203.02471" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.02471&amp;source=gmail&amp;ust=1646963617871000&amp;usg=AOvVaw2k7gtBp1J0effTmp3JnRwe" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>02471</a> ,&nbsp; 207kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03457<br>
replaced with revised version Tue, 8 Mar 2022 07:06:58 GMT&nbsp; &nbsp;(558kb,D)<br>
<br>
Title: Graph Neural Networks for Image Classification and Reinforcement<br>
&nbsp; Learning using Graph representations<br>
Authors: Naman Goyal, David Steiner<br>
Categories: cs.LG cs.CV<br>
Comments: The work was done as a project for Neural Networks and Deep Learning<br>
&nbsp; course, Fall 2021 offering by Prof. Richard Zemel at Columbia University<br>
\\ ( <a href="https://arxiv.org/abs/2203.03457" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03457&amp;source=gmail&amp;ust=1646963617871000&amp;usg=AOvVaw2EnwR57wcMDv-Jr90lTca0" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03457</a> ,&nbsp; 558kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03564<br>
replaced with revised version Tue, 8 Mar 2022 12:13:07 GMT&nbsp; &nbsp;(1835kb,D)<br>
<br>
Title: TIGGER: Scalable Generative Modelling for Temporal Interaction Graphs<br>
Authors: Shubham Gupta, Sahil Manchanda, Srikanta Bedathur and Sayan Ranu<br>
Categories: cs.LG cs.AI cs.IR cs.SI<br>
Comments: To be published in AAAI-2022, additionally contains technical<br>
&nbsp; appendices/supplementary material<br>
\\ ( <a href="https://arxiv.org/abs/2203.03564" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03564&amp;source=gmail&amp;ust=1646963617871000&amp;usg=AOvVaw3Yg1UeDsrudifK9dOm_gR_" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03564</a> ,&nbsp; 1835kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2201.10705<br>
replaced with revised version Tue, 8 Mar 2022 06:36:28 GMT&nbsp; &nbsp;(1118kb,D)<br>
<br>
Title: Learning to Recommend Method Names with Global Context<br>
Authors: Fang Liu, Ge Li, Zhiyi Fu, Shuai Lu, Yiyang Hao, Zhi Jin<br>
Categories: cs.SE<br>
Comments: Accepted for publication at ICSE 2022<br>
\\ ( <a href="https://arxiv.org/abs/2201.10705" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2201.10705&amp;source=gmail&amp;ust=1646963617871000&amp;usg=AOvVaw3gKjeaNykJGw-sLD0Nl3Ea" rel="noreferrer" target="_blank">https://arxiv.org/abs/2201.<wbr>10705</a> ,&nbsp; 1118kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2009.12517<br>
replaced with revised version Tue, 8 Mar 2022 12:05:02 GMT&nbsp; &nbsp;(930kb,D)<br>
<br>
Title: QuatRE: Relation-Aware Quaternions for Knowledge Graph Embeddings<br>
Authors: Dai Quoc Nguyen and Thanh Vu and Tu Dinh Nguyen and Dinh Phung<br>
Categories: cs.CL cs.AI cs.LG<br>
Comments: Accepted to The ACM Web Conference 2022 (WWW '22) (Poster and Demo<br>
&nbsp; Track)<br>
\\ ( <a href="https://arxiv.org/abs/2009.12517" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2009.12517&amp;source=gmail&amp;ust=1646963617871000&amp;usg=AOvVaw3fnK6lDgsT3EOaxZ0VWl5n" rel="noreferrer" target="_blank">https://arxiv.org/abs/2009.<wbr>12517</a> ,&nbsp; 930kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2106.12271<br>
replaced with revised version Tue, 8 Mar 2022 17:11:23 GMT&nbsp; &nbsp;(692kb,D)<br>
<br>
Title: Unsupervised Speech Enhancement using Dynamical Variational<br>
&nbsp; Auto-Encoders<br>
Authors: Xiaoyu Bie, Simon Leglaive, Xavier Alameda-Pineda, Laurent Girin<br>
Categories: cs.SD cs.AI cs.LG eess.AS<br>
\\ ( <a href="https://arxiv.org/abs/2106.12271" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2106.12271&amp;source=gmail&amp;ust=1646963617871000&amp;usg=AOvVaw2XcvYtTRMHJon2fEIGpWmT" rel="noreferrer" target="_blank">https://arxiv.org/abs/2106.<wbr>12271</a> ,&nbsp; 692kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2107.10648<br>
replaced with revised version Mon, 7 Mar 2022 20:51:01 GMT&nbsp; &nbsp;(4755kb,D)<br>
<br>
Title: DEAP-FAKED: Knowledge Graph based Approach for Fake News Detection<br>
Authors: Mohit Mayank, Shakshi Sharma, Rajesh Sharma<br>
Categories: cs.CL cs.AI<br>
Comments: This work has been submitted to the IEEE for possible publication.<br>
&nbsp; Copyright may be transferred without notice, after which this version may no<br>
&nbsp; longer be accessible<br>
\\ ( <a href="https://arxiv.org/abs/2107.10648" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2107.10648&amp;source=gmail&amp;ust=1646963617871000&amp;usg=AOvVaw2XrSRHgeOvmnFTYDaIA8av" rel="noreferrer" target="_blank">https://arxiv.org/abs/2107.<wbr>10648</a> ,&nbsp; 4755kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2110.08300<br>
replaced with revised version Tue, 8 Mar 2022 00:01:04 GMT&nbsp; &nbsp;(492kb,D)<br>
<br>
Title: When Combating Hype, Proceed with Caution<br>
Authors: Samuel R. Bowman<br>
Categories: cs.CL cs.AI<br>
Comments: Proceedings of ACL 2022<br>
\\ ( <a href="https://arxiv.org/abs/2110.08300" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2110.08300&amp;source=gmail&amp;ust=1646963617871000&amp;usg=AOvVaw0M5YIPrQEbhM_atRGdSdDp" rel="noreferrer" target="_blank">https://arxiv.org/abs/2110.<wbr>08300</a> ,&nbsp; 492kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2110.10572<br>
replaced with revised version Tue, 8 Mar 2022 07:34:08 GMT&nbsp; &nbsp;(987kb)<br>
<br>
Title: Estimation &amp; Recognition under Perspective of Random-Fuzzy Dual<br>
&nbsp; Interpretation of Unknown Quantity: with Demonstration of IMM Filter<br>
Authors: Wei Mei, Yunfeng Xu and Limin Liu<br>
Categories: eess.SY cs.AI cs.SY eess.SP math.PR stat.AP<br>
Comments: 15 pages, 11 figures, code available<br>
\\ ( <a href="https://arxiv.org/abs/2110.10572" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2110.10572&amp;source=gmail&amp;ust=1646963617871000&amp;usg=AOvVaw0pLAvrF-6eGiwuPs02NXva" rel="noreferrer" target="_blank">https://arxiv.org/abs/2110.<wbr>10572</a> ,&nbsp; 987kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2112.04906 (*cross-listing*)<br>
replaced with revised version Tue, 8 Mar 2022 02:40:45 GMT&nbsp; &nbsp;(548kb)<br>
<br>
Title: Enhancing Column Generation by a Machine-Learning-Based Pricing<br>
&nbsp; Heuristic for Graph Coloring<br>
Authors: Yunzhuang Shen, Yuan Sun, Xiaodong Li, Andrew Eberhard, Andreas Ernst<br>
Categories: math.OC cs.AI cs.LG<br>
Comments: Machine learning for column generation and branch-and-price; accepted<br>
&nbsp; to AAAI 2022<br>
\\ ( <a href="https://arxiv.org/abs/2112.04906" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2112.04906&amp;source=gmail&amp;ust=1646963617871000&amp;usg=AOvVaw1LeqklmwgLmhT843qmtRFg" rel="noreferrer" target="_blank">https://arxiv.org/abs/2112.<wbr>04906</a> ,&nbsp; 548kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2202.12154<br>
replaced with revised version Tue, 8 Mar 2022 17:59:59 GMT&nbsp; &nbsp;(13649kb,D)<br>
<br>
Title: Towards Effective and Robust Neural Trojan Defenses via Input Filtering<br>
Authors: Kien Do, Haripriya Harikumar, Hung Le, Dung Nguyen, Truyen Tran, Santu<br>
&nbsp; Rana, Dang Nguyen, Willy Susilo, Svetha Venkatesh<br>
Categories: cs.CR cs.AI cs.CV cs.LG<br>
\\ ( <a href="https://arxiv.org/abs/2202.12154" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2202.12154&amp;source=gmail&amp;ust=1646963617871000&amp;usg=AOvVaw3JV241mbUk0Ex3ob7ji1K2" rel="noreferrer" target="_blank">https://arxiv.org/abs/2202.<wbr>12154</a> ,&nbsp; 13649kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.00193<br>
replaced with revised version Tue, 8 Mar 2022 09:13:36 GMT&nbsp; &nbsp;(0kb,I)<br>
<br>
Title: EPPAC: Entity Pre-typing Relation Classification with Prompt<br>
&nbsp; AnswerCentralizing<br>
Authors: Jiejun Tan, Wenbin Hu, WeiWei Liu<br>
Categories: cs.CL cs.AI<br>
Comments: There are errors in experimental results<br>
\\ ( <a href="https://arxiv.org/abs/2203.00193" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.00193&amp;source=gmail&amp;ust=1646963617871000&amp;usg=AOvVaw0jqSkZDMrSn-5a5KbQgbA-" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>00193</a> ,&nbsp; 0kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.00812<br>
replaced with revised version Mon, 7 Mar 2022 23:25:36 GMT&nbsp; &nbsp;(304kb)<br>
<br>
Title: Efficient Dynamic Clustering: Capturing Patterns from Historical Cluster<br>
&nbsp; Evolution<br>
Authors: Binbin Gu, Saeed Kargar, Faisal Nawab<br>
Categories: cs.DB cs.AI<br>
\\ ( <a href="https://arxiv.org/abs/2203.00812" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.00812&amp;source=gmail&amp;ust=1646963617871000&amp;usg=AOvVaw3mhQo9F1A9vGf_fTTTnhrd" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>00812</a> ,&nbsp; 304kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.01426<br>
replaced with revised version Tue, 8 Mar 2022 05:50:44 GMT&nbsp; &nbsp;(3308kb,D)<br>
<br>
Title: SPICEprop: Backpropagating Errors Through Memristive Spiking Neural<br>
&nbsp; Networks<br>
Authors: Peng Zhou, Jason K. Eshraghian, Dong-Uk Choi, Sung-Mo Kang<br>
Categories: cs.NE cs.AI cs.ET<br>
\\ ( <a href="https://arxiv.org/abs/2203.01426" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.01426&amp;source=gmail&amp;ust=1646963617872000&amp;usg=AOvVaw0QDUNvhaGc5rW2H8_MpQVj" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>01426</a> ,&nbsp; 3308kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2109.05021 (*cross-listing*)<br>
replaced with revised version Tue, 8 Mar 2022 17:50:38 GMT&nbsp; &nbsp;(18561kb,D)<br>
<br>
Title: A Deep Learning-Based Unified Framework for Red Lesions Detection on<br>
&nbsp; Retinal Fundus Images<br>
Authors: Norah Asiri, Muhammad Hussain, Fadwa Al Adel, Hatim Aboalsamh<br>
Categories: eess.IV cs.CV<br>
\\ ( <a href="https://arxiv.org/abs/2109.05021" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2109.05021&amp;source=gmail&amp;ust=1646963617872000&amp;usg=AOvVaw2m_jqr7ZpdNptrGDDAWKP6" rel="noreferrer" target="_blank">https://arxiv.org/abs/2109.<wbr>05021</a> ,&nbsp; 18561kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.00911 (*cross-listing*)<br>
replaced with revised version Tue, 8 Mar 2022 00:19:47 GMT&nbsp; &nbsp;(15667kb,D)<br>
<br>
Title: Towards Bidirectional Arbitrary Image Rescaling: Joint Optimization and<br>
&nbsp; Cycle Idempotence<br>
Authors: Zhihong Pan, Baopu Li, Dongliang He, Mingde Yao, Wenhao Wu, Tianwei<br>
&nbsp; Lin, Xin Li, Errui Ding<br>
Categories: eess.IV cs.CV<br>
Comments: To appear at CVPR 2022<br>
\\ ( <a href="https://arxiv.org/abs/2203.00911" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.00911&amp;source=gmail&amp;ust=1646963617872000&amp;usg=AOvVaw08UORItAjJg5AAftEszW-c" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>00911</a> ,&nbsp; 15667kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:1907.04377 (*cross-listing*)<br>
replaced with revised version Tue, 8 Mar 2022 03:24:32 GMT&nbsp; &nbsp;(72kb)<br>
<br>
Title: Convergence Rates for Gaussian Mixtures of Experts<br>
Authors: Nhat Ho and Chiao-Yu Yang and Michael I. Jordan<br>
Categories: math.ST cs.LG stat.ML stat.TH<br>
Comments: 81 pages<br>
\\ ( <a href="https://arxiv.org/abs/1907.04377" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/1907.04377&amp;source=gmail&amp;ust=1646963617872000&amp;usg=AOvVaw2JP9YOTOyFOucZpgXQxOk6" rel="noreferrer" target="_blank">https://arxiv.org/abs/1907.<wbr>04377</a> ,&nbsp; 72kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2003.13153 (*cross-listing*)<br>
replaced with revised version Mon, 7 Mar 2022 21:09:05 GMT&nbsp; &nbsp;(468kb,D)<br>
<br>
Title: Nonconvex Matrix Completion with Linearly Parameterized Factors<br>
Authors: Ji Chen, Xiaodong Li, Zongming Ma<br>
Categories: math.ST cs.LG stat.ML stat.TH<br>
Comments: 34 pages, 2 figures<br>
\\ ( <a href="https://arxiv.org/abs/2003.13153" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2003.13153&amp;source=gmail&amp;ust=1646963617872000&amp;usg=AOvVaw2GzrRZJg9LsR-WWoXVYiVv" rel="noreferrer" target="_blank">https://arxiv.org/abs/2003.<wbr>13153</a> ,&nbsp; 468kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2103.00284 (*cross-listing*)<br>
replaced with revised version Tue, 8 Mar 2022 04:35:57 GMT&nbsp; &nbsp;(769kb,D)<br>
<br>
Title: On the Initialization for Convex-Concave Min-max Problems<br>
Authors: Mingrui Liu, Francesco Orabona<br>
Categories: math.OC cs.LG<br>
Comments: Accepted by ALT 2022<br>
\\ ( <a href="https://arxiv.org/abs/2103.00284" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2103.00284&amp;source=gmail&amp;ust=1646963617872000&amp;usg=AOvVaw1-BrKc891IWLk8tlGmS06c" rel="noreferrer" target="_blank">https://arxiv.org/abs/2103.<wbr>00284</a> ,&nbsp; 769kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2104.00165<br>
replaced with revised version Tue, 8 Mar 2022 00:16:31 GMT&nbsp; &nbsp;(3368kb,D)<br>
<br>
Title: Encoding Event-Based Data With a Hybrid SNN Guided Variational<br>
&nbsp; Auto-encoder in Neuromorphic Hardware<br>
Authors: Kenneth Stewart, Andreea Danielescu, Timothy Shea, Emre Neftci<br>
Categories: cs.NE cs.LG<br>
Comments: Published in NICE 2022<br>
DOI: 10.1145/3517343.3517372<br>
\\ ( <a href="https://arxiv.org/abs/2104.00165" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2104.00165&amp;source=gmail&amp;ust=1646963617872000&amp;usg=AOvVaw1bVOOxW7a_Z7CnEp2YTbYr" rel="noreferrer" target="_blank">https://arxiv.org/abs/2104.<wbr>00165</a> ,&nbsp; 3368kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2106.03579<br>
replaced with revised version Tue, 8 Mar 2022 15:51:55 GMT&nbsp; &nbsp;(3983kb,D)<br>
<br>
Title: Forward Looking Best-Response Multiplicative Weights Update Methods for<br>
&nbsp; Bilinear Zero-sum Games<br>
Authors: Michail Fasoulakis, Evangelos Markakis, Yannis Pantazis, Constantinos<br>
&nbsp; Varsos<br>
Categories: cs.GT cs.LG<br>
\\ ( <a href="https://arxiv.org/abs/2106.03579" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2106.03579&amp;source=gmail&amp;ust=1646963617872000&amp;usg=AOvVaw31n--vLqOfpqatD0LCXxFo" rel="noreferrer" target="_blank">https://arxiv.org/abs/2106.<wbr>03579</a> ,&nbsp; 3983kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2106.08217 (*cross-listing*)<br>
replaced with revised version Mon, 7 Mar 2022 19:08:57 GMT&nbsp; &nbsp;(211kb,D)<br>
<br>
Title: RFpredInterval: An R Package for Prediction Intervals with Random<br>
&nbsp; Forests and Boosted Forests<br>
Authors: Cansu Alakus, Denis Larocque, Aurelie Labbe<br>
Categories: stat.ML cs.LG<br>
Comments: 36 pages, 14 figures, 5 tables<br>
\\ ( <a href="https://arxiv.org/abs/2106.08217" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2106.08217&amp;source=gmail&amp;ust=1646963617872000&amp;usg=AOvVaw3-_cbvnaxUGKmDK55t30-Y" rel="noreferrer" target="_blank">https://arxiv.org/abs/2106.<wbr>08217</a> ,&nbsp; 211kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2106.09215 (*cross-listing*)<br>
replaced with revised version Tue, 8 Mar 2022 02:53:57 GMT&nbsp; &nbsp;(2333kb,D)<br>
<br>
Title: Optimum-statistical Collaboration Towards General and Efficient<br>
&nbsp; Black-box Optimization<br>
Authors: Wenjie Li, Chi-Hua Wang, Qifan Song, Guang Cheng<br>
Categories: stat.ML cs.LG<br>
\\ ( <a href="https://arxiv.org/abs/2106.09215" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2106.09215&amp;source=gmail&amp;ust=1646963617872000&amp;usg=AOvVaw0RmdJkbF8NjmAnLka_TprI" rel="noreferrer" target="_blank">https://arxiv.org/abs/2106.<wbr>09215</a> ,&nbsp; 2333kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2107.00363 (*cross-listing*)<br>
replaced with revised version Tue, 8 Mar 2022 16:00:34 GMT&nbsp; &nbsp;(1304kb,D)<br>
<br>
Title: Valid prediction intervals for regression problems<br>
Authors: Nicolas Dewolf, Bernard De Baets, Willem Waegeman<br>
Categories: stat.ML cs.LG<br>
Comments: submitted to AI Review<br>
\\ ( <a href="https://arxiv.org/abs/2107.00363" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2107.00363&amp;source=gmail&amp;ust=1646963617872000&amp;usg=AOvVaw1VizmRMmM5jed-lJAlRJcR" rel="noreferrer" target="_blank">https://arxiv.org/abs/2107.<wbr>00363</a> ,&nbsp; 1304kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2109.08549<br>
replaced with revised version Tue, 8 Mar 2022 07:50:07 GMT&nbsp; &nbsp;(1951kb,D)<br>
<br>
Title: Measuring Fairness under Unawareness of Sensitive Attributes: A<br>
&nbsp; Quantification-Based Approach<br>
Authors: Alessandro Fabris, Andrea Esuli, Alejandro Moreo, Fabrizio Sebastiani<br>
Categories: cs.CY cs.LG<br>
\\ ( <a href="https://arxiv.org/abs/2109.08549" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2109.08549&amp;source=gmail&amp;ust=1646963617872000&amp;usg=AOvVaw2SjxJqo-7L5y4WRB_WPw-J" rel="noreferrer" target="_blank">https://arxiv.org/abs/2109.<wbr>08549</a> ,&nbsp; 1951kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2110.02369<br>
replaced with revised version Mon, 7 Mar 2022 21:53:43 GMT&nbsp; &nbsp;(51kb)<br>
<br>
Title: EntQA: Entity Linking as Question Answering<br>
Authors: Wenzheng Zhang, Wenyue Hua, Karl Stratos<br>
Categories: cs.CL cs.LG<br>
Comments: ICLR 2022<br>
\\ ( <a href="https://arxiv.org/abs/2110.02369" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2110.02369&amp;source=gmail&amp;ust=1646963617872000&amp;usg=AOvVaw2Li_efJ-DJNyT5e4_Fkrnr" rel="noreferrer" target="_blank">https://arxiv.org/abs/2110.<wbr>02369</a> ,&nbsp; 51kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2110.04971<br>
replaced with revised version Mon, 7 Mar 2022 19:19:36 GMT&nbsp; &nbsp;(4984kb,D)<br>
<br>
Title: A Deep Generative Model for Reordering Adjacency Matrices<br>
Authors: Oh-Hyun Kwon, Chiun-How Kao, Chun-houh Chen, Kwan-Liu Ma<br>
Categories: cs.HC cs.LG cs.SI<br>
Comments: IEEE Transactions on Visualization and Computer Graphics<br>
DOI: 10.1109/TVCG.2022.3153838<br>
\\ ( <a href="https://arxiv.org/abs/2110.04971" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2110.04971&amp;source=gmail&amp;ust=1646963617872000&amp;usg=AOvVaw0k0sQRUynKedgKx72cHMUF" rel="noreferrer" target="_blank">https://arxiv.org/abs/2110.<wbr>04971</a> ,&nbsp; 4984kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2110.14434<br>
replaced with revised version Mon, 7 Mar 2022 20:41:20 GMT&nbsp; &nbsp;(21kb)<br>
<br>
Title: Nonnegative Tucker Decomposition with Beta-divergence for Music<br>
&nbsp; Structure Analysis of audio signals<br>
Authors: Axel Marmoret, Florian Voorwinden, Valentin Leplat, J\'er\'emy E.<br>
&nbsp; Cohen, Fr\'ed\'eric Bimbot<br>
Categories: cs.SD cs.LG cs.NA eess.AS math.NA<br>
Comments: 4 pages, 2 figures, 1 table, 1 algorithm. Rejected from ICASSP 2022,<br>
&nbsp; but the algorithm is available at<br>
&nbsp; <a href="https://gitlab.inria.fr/amarmore/nonnegative-factorization" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://gitlab.inria.fr/amarmore/nonnegative-factorization&amp;source=gmail&amp;ust=1646963617872000&amp;usg=AOvVaw0NQF6NNIguiD6PRGnuJHQE" rel="noreferrer" target="_blank">https://gitlab.inria.fr/<wbr>amarmore/nonnegative-<wbr>factorization</a><br>
MSC-class: 15-04<br>
ACM-class: G.1.6; H.5.5<br>
\\ ( <a href="https://arxiv.org/abs/2110.14434" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2110.14434&amp;source=gmail&amp;ust=1646963617873000&amp;usg=AOvVaw1yj7rqlOlLWvf1At5hSZVc" rel="noreferrer" target="_blank">https://arxiv.org/abs/2110.<wbr>14434</a> ,&nbsp; 21kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2110.14437<br>
replaced with revised version Mon, 7 Mar 2022 20:42:50 GMT&nbsp; &nbsp;(214kb,D)<br>
<br>
Title: Exploring single-song autoencoding schemes for audio-based music<br>
&nbsp; structure analysis<br>
Authors: Axel Marmoret, J\'er\'emy E. Cohen, Fr\'ed\'eric Bimbot<br>
Categories: cs.SD cs.LG eess.AS<br>
Comments: 4 pages, 4 figures, 2 tables. Rejected from ICASSP 2022, an extended<br>
&nbsp; version is available at arXiv:2202.04981<br>
ACM-class: H.5.5<br>
\\ ( <a href="https://arxiv.org/abs/2110.14437" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2110.14437&amp;source=gmail&amp;ust=1646963617873000&amp;usg=AOvVaw3_Ks0uQyIJy4rsGBi7lnRw" rel="noreferrer" target="_blank">https://arxiv.org/abs/2110.<wbr>14437</a> ,&nbsp; 214kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2111.02274<br>
replaced with revised version Tue, 8 Mar 2022 14:29:42 GMT&nbsp; &nbsp;(2512kb,D)<br>
<br>
Title: Manipulation of granular materials by learning particle interactions<br>
Authors: Neea Tuomainen, David Blanco-Mulero, Ville Kyrki<br>
Categories: cs.RO cs.LG<br>
Comments: 8 pages, 5 figures. Accepted to IEEE Robotics and Automation Letters<br>
\\ ( <a href="https://arxiv.org/abs/2111.02274" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2111.02274&amp;source=gmail&amp;ust=1646963617873000&amp;usg=AOvVaw1dC_m-KYQyjocq_YqCn00r" rel="noreferrer" target="_blank">https://arxiv.org/abs/2111.<wbr>02274</a> ,&nbsp; 2512kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2111.05987 (*cross-listing*)<br>
replaced with revised version Mon, 7 Mar 2022 22:31:17 GMT&nbsp; &nbsp;(170kb,D)<br>
<br>
Title: Tight bounds for minimum l1-norm interpolation of noisy data<br>
Authors: Guillaume Wang, Konstantin Donhauser, Fanny Yang<br>
Categories: math.ST cs.IT cs.LG math.IT stat.ML stat.TH<br>
Comments: 33 pages, 1 figure; accepted to AISTATS 2022<br>
\\ ( <a href="https://arxiv.org/abs/2111.05987" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2111.05987&amp;source=gmail&amp;ust=1646963617873000&amp;usg=AOvVaw2w3VjyOrgN8azf9DWRztns" rel="noreferrer" target="_blank">https://arxiv.org/abs/2111.<wbr>05987</a> ,&nbsp; 170kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2112.06101 (*cross-listing*)<br>
replaced with revised version Tue, 8 Mar 2022 16:49:43 GMT&nbsp; &nbsp;(9kb)<br>
<br>
Title: Confidence intervals for the random forest generalization error<br>
Authors: Paulo C. Marques F.<br>
Categories: stat.ML cs.LG<br>
Comments: 10 pages<br>
\\ ( <a href="https://arxiv.org/abs/2112.06101" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2112.06101&amp;source=gmail&amp;ust=1646963617873000&amp;usg=AOvVaw2-KjKvHYzVrxtfowjnCLn1" rel="noreferrer" target="_blank">https://arxiv.org/abs/2112.<wbr>06101</a> ,&nbsp; 9kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2112.15272<br>
replaced with revised version Tue, 8 Mar 2022 07:57:44 GMT&nbsp; &nbsp;(5234kb)<br>
<br>
Title: ViNMT: Neural Machine Translation Toolkit<br>
Authors: Nguyen Hoang Quan, Nguyen Thanh Dat, Nguyen Hoang Minh Cong, Nguyen<br>
&nbsp; Van Vinh, Ngo Thi Vinh, Nguyen Phuong Thai, and Tran Hong Viet<br>
Categories: cs.CL cs.LG<br>
\\ ( <a href="https://arxiv.org/abs/2112.15272" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2112.15272&amp;source=gmail&amp;ust=1646963617873000&amp;usg=AOvVaw3oW8xdakUKoCe-O1W7UFTO" rel="noreferrer" target="_blank">https://arxiv.org/abs/2112.<wbr>15272</a> ,&nbsp; 5234kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2202.01862<br>
replaced with revised version Tue, 8 Mar 2022 00:59:35 GMT&nbsp; &nbsp;(7019kb,D)<br>
<br>
Title: Practical Imitation Learning in the Real World via Task Consistency Loss<br>
Authors: Mohi Khansari and Daniel Ho and Yuqing Du and Armando Fuentes and<br>
&nbsp; Matthew Bennice and Nicolas Sievers and Sean Kirmani and Yunfei Bai and Eric<br>
&nbsp; Jang<br>
Categories: cs.RO cs.LG<br>
\\ ( <a href="https://arxiv.org/abs/2202.01862" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2202.01862&amp;source=gmail&amp;ust=1646963617873000&amp;usg=AOvVaw3phOD5IBwm3bXXperLmu2V" rel="noreferrer" target="_blank">https://arxiv.org/abs/2202.<wbr>01862</a> ,&nbsp; 7019kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.01063<br>
replaced with revised version Tue, 8 Mar 2022 09:27:31 GMT&nbsp; &nbsp;(6377kb,D)<br>
<br>
Title: Discontinuous Constituency and BERT: A Case Study of Dutch<br>
Authors: Konstantinos Kogkalidis and Gijs Wijnholds<br>
Categories: cs.CL cs.LG<br>
Comments: 8 pages plus references. To appear in Findings of the Association for<br>
&nbsp; Computational Linguistics 2022<br>
\\ ( <a href="https://arxiv.org/abs/2203.01063" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.01063&amp;source=gmail&amp;ust=1646963617873000&amp;usg=AOvVaw0vZUuSPc6IUqC-XX7pBtv6" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>01063</a> ,&nbsp; 6377kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.01786<br>
replaced with revised version Mon, 7 Mar 2022 20:10:20 GMT&nbsp; &nbsp;(1464kb,D)<br>
<br>
Title: Generative Modeling for Low Dimensional Speech Attributes with Neural<br>
&nbsp; Spline Flows<br>
Authors: Kevin J. Shih, Rafael Valle, Rohan Badlani, Jo\~ao Felipe Santos,<br>
&nbsp; Bryan Catanzaro<br>
Categories: cs.SD cs.LG eess.AS<br>
Comments: 22 pages, 11 figures, 3 tables<br>
\\ ( <a href="https://arxiv.org/abs/2203.01786" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.01786&amp;source=gmail&amp;ust=1646963617873000&amp;usg=AOvVaw0Gm4a1LHqAA2IIov40qhrr" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>01786</a> ,&nbsp; 1464kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.01870 (*cross-listing*)<br>
replaced with revised version Tue, 8 Mar 2022 18:47:41 GMT&nbsp; &nbsp;(2635kb,D)<br>
<br>
Title: KamNet: An Integrated Spatiotemporal Deep Neural Network for Rare Event<br>
&nbsp; Search in KamLAND-Zen<br>
Authors: A. Li, Z. Fu, L. Winslow, C. Grant, H. Song, H. Ozaki, I. Shimizu, A.<br>
&nbsp; Takeuchi<br>
Categories: physics.ins-det cs.LG<br>
Comments: 12 pages, dual submission with upcoming KamLAND-Zen 800 main result<br>
\\ ( <a href="https://arxiv.org/abs/2203.01870" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.01870&amp;source=gmail&amp;ust=1646963617873000&amp;usg=AOvVaw0nXybuqaxKo3VwkHIpKhKT" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>01870</a> ,&nbsp; 2635kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.02591 (*cross-listing*)<br>
replaced with revised version Tue, 8 Mar 2022 18:08:21 GMT&nbsp; &nbsp;(79kb)<br>
<br>
Title: A Small Gain Analysis of Single Timescale Actor Critic<br>
Authors: Alex Olshevsky, Bahman Gharesifard<br>
Categories: math.OC cs.LG cs.SY eess.SY<br>
\\ ( <a href="https://arxiv.org/abs/2203.02591" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.02591&amp;source=gmail&amp;ust=1646963617873000&amp;usg=AOvVaw1vNS1hj_pt4c68DSJu_jBu" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>02591</a> ,&nbsp; 79kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2203.03185 (*cross-listing*)<br>
replaced with revised version Tue, 8 Mar 2022 09:34:07 GMT&nbsp; &nbsp;(973kb,D)<br>
<br>
Title: Covariate-Balancing-Aware Interpretable Deep Learning models for<br>
&nbsp; Treatment Effect Estimation<br>
Authors: Kan Chen, Qishuo Yin, Qi Long<br>
Categories: stat.ML cs.LG<br>
Comments: 27 pages, 2 figures<br>
\\ ( <a href="https://arxiv.org/abs/2203.03185" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2203.03185&amp;source=gmail&amp;ust=1646963617873000&amp;usg=AOvVaw2hlD7gGIedyfhGHXpt_nkl" rel="noreferrer" target="_blank">https://arxiv.org/abs/2203.<wbr>03185</a> ,&nbsp; 973kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2103.06109<br>
replaced with revised version Tue, 8 Mar 2022 09:25:31 GMT&nbsp; &nbsp;(2093kb,D)<br>
<br>
Title: Session-based Social and Dependency-aware Software Recommendation<br>
Authors: Dengcheng Yan, Tianyi Tang, Wenxin Xie, Yiwen Zhang, Qiang He<br>
Categories: cs.IR cs.SE<br>
Journal-ref: Yan, D., Tang, T., Xie, W., Zhang, Y., &amp; He, Q. (2022).<br>
&nbsp; Session-based social and dependency-aware software recommendation. Applied<br>
&nbsp; Soft Computing, 118, 108463<br>
DOI: 10.1016/j.asoc.2022.108463<br>
\\ ( <a href="https://arxiv.org/abs/2103.06109" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2103.06109&amp;source=gmail&amp;ust=1646963617873000&amp;usg=AOvVaw3HdJOKxzO9FSbX4UKpItA0" rel="noreferrer" target="_blank">https://arxiv.org/abs/2103.<wbr>06109</a> ,&nbsp; 2093kb)<br>
------------------------------<wbr>------------------------------<wbr>------------------<br>
\\<br>
arXiv:2109.10294<br>
replaced with revised version Tue, 8 Mar 2022 10:14:23 GMT&nbsp; &nbsp;(1469kb,D)<br>
<br>
Title: DeepSTL - From English Requirements to Signal Temporal Logic<br>
Authors: Jie He, Ezio Bartocci, Dejan Ni\v{c}kovi\'c, Haris Isakovic and Radu<br>
&nbsp; Grosu<br>
Categories: cs.CL cs.SE<br>
Comments: 13 pages<br>
\\ ( <a href="https://arxiv.org/abs/2109.10294" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://arxiv.org/abs/2109.10294&amp;source=gmail&amp;ust=1646963617874000&amp;usg=AOvVaw2-rMcTRSeFegcC3DfMR264" rel="noreferrer" target="_blank">https://arxiv.org/abs/2109.<wbr>10294</a> ,&nbsp; 1469kb)<br>
%%%---%%%---%%%---%%%---%%%---<wbr>%%%---%%%---%%%---%%%---%%%---<wbr>%%%---%%%---%%%---<br>
</font></div></td></tr></tbody></table></td></tr></tbody></table></div></div></body></html>